40	ClarkCarter, D.	The account taken of statistical power in research published in the British Journal of Psychology	British Journal of Psychology	Since approximately 1925, researchers in psychology have evaluated their hypotheses against the probability of making a Type I error. Attempts to persuade researchers to augment this information, with details such as effect size and confidence intervals, and in particular to take the probability of a Type II error into account, have largely been ignored. The present paper reviews the degree to which statistical power has been explicitly considered in papers published in the British Journal of Psychology in 1993 and 1994. It summarizes the power for small, medium and large effect sizes for 54 papers, based on the sample sizes which were employed. The analysis confirms that the power of statistical tests is not taken into account by researchers and that, accordingly, they continue to run a high risk of rejecting their research hypotheses.	1997	88		71-83
41	Cuijpers, P.	Are all psychotherapies equally effective in the treatment of adult depression? The lack of statistical power of comparative outcome studies	Evidence-Based Mental Health	More than 100 comparative outcome trials, directly comparing 2 or more psychotherapies for adult depression, have been published. We first examined whether these comparative trials had sufficient statistical power to detect clinically relevant differences between therapies of d=0.24. In order to detect such an effect size, power calculations showed that a trial would need to include 548 patients. We selected 3 recent meta-analyses of psychotherapies for adult depression (cognitive behaviour therapy (CBT), interpersonal psychotherapy and non-directive counselling) and examined the number of patients included in the trials directly comparing other psychotherapies. The largest trial comparing CBT with another therapy included 178 patients, and had enough power to detect a differential effect size of only d-0.42. None of the trials in the 3 meta-analyses had enough power to detect effect sizes smaller than d=0.34, but some came close to the threshold for detecting a clinically relevant effect size of d=0.24. Meta-analyses may be able to solve the problem of the low power of individual trials. However, many of these studies have considerable risk of bias, and if we only focused on trials with low risk of bias, there would no longer be enough studies to detect clinically relevant effects. We conclude that individual trials are heavily underpowered and do not even come close to having sufficient power for detecting clinically relevant effect sizes. Despite this large number of trials, it is still not clear whether there are clinically relevant differences between these therapies.	2016	19	2	39-42
42	Woolley, Thomas W.	A comprehensive power-analytic investigation of research in medical education	Journal of Medical Education	Reviewed 230 articles in Volumes 55-57 of the Journal of Medical Education. Three statistical power determinations were made for each of 2,200 reported tests of significance, and the average power for detecting a range of possible treatment effects was calculated for 100 studies. 91% of the 100 articles analyzed had less than a 50% chance of detecting a small treatment effect. Guidelines for reporting the information necessary for the independent evaluation of published studies are provided. (11 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1983	58	9	710-715
43	Pereira, Tiago V., Patsopoulos, Nikolaos A., Salanti, Georgia and Ioannidis, John P.	Critical interpretation of Cochran's Q test depends on power and prior assumptions about heterogeneity	Research Synthesis Methods	We describe how an appropriate interpretation of the Q-test depends on its power to detect a given typical amount of between-study variance (tau2) as well as prior beliefs on heterogeneity. We illustrate these concepts in an evaluation of 1011 meta-analyses of clinical trials with >=4 studies and binary outcomes. These concepts can be seen as an application of the Bayes theorem. Across the 1011 meta-analyses, power to detect typical heterogeneity was low in most situations. Thus, usually a non-significant Q test did not change perceptibly prior convictions on heterogeneity. Conversely, significant results for the Q test typically augmented considerably the probability of heterogeneity. The posterior probability of heterogeneity depends on what tau2 we want to detect. With the same approach, one may also estimate the posterior probability for the presence of heterogeneity that is large enough to annul statistically significant summary effects; that is half the average within-study variance of the combined studies; and that is able to change the summary effect estimate of the meta-analysis by 20%. The discussed analyses are exploratory, and may depend heavily on prior assumptions when power for the Q-test is low. Statistical heterogeneity in meta-analyses should be cautiously interpreted considering the power to detect a specific tau2 and prior assumptions about the presence of heterogeneity. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2010	1	2	149-161
44	Sedlmeier, P. and Gigerenzer, G.	Do studies of statistical power have an effect on the power of studies?	Psychological Bulletin	The long-term impact of studies of statistical power is investigated using J. Cohen's (1962) pioneering work as an example. We argue that the impact is nil; the power of studies in the same journal that Cohen reviewed (now the Journal of Abnormal Psychology) has not increased over the past 24 years. In 1960 the median power (i.e., the probability that a significant result will be obtained if there is a true effect) was .46 for a medium size effect, whereas in 1984 it was only .37. The decline of power is a result of alpha-adjusted procedures. Low power seems to go unnoticed: only 2 out of 64 experiments mentioned power, and it was never estimated. Nonsignificance was generally interpreted as confirmation of the null hypothesis (if this was the research hypothesis), although the median power was as low as .25 in these cases. We discuss reasons for the ongoing neglect of power. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1989	105	2	309-316
45	Aguinis, H., Beaty, J. C., Boik, R. J. and Pierce, C. A.	Effect size and power in assessing moderating effects of categorical variables using multiple regression: A 30-year review	Journal of Applied Psychology	The authors conducted a 30-year review (1969-1998) of the size of moderating effects of categorical variables as assessed using multiple regression. The median observed effect size (f(2)) is only .002, but 72% of the moderator tests reviewed had power of .80 or greater to detect a targeted effect conventionally defined as small. Results suggest the need to minimize the influence of artifacts that produce a downward bias in the observed effect size and put into question the use of conventional definitions of moderating effect sizes. As long as an effect has a meaningful impact, the authors advise researchers to conduct a power analysis and plan future research designs on the basis of smaller and more realistic targeted effect sizes.	2005	90	1	94-107
46	Suzukawa, Yumi and Toyoda, Hideki	Effect sizes, statistical power and sample sizes in "The Japanese Journal of Psychology."	Japanese Journal of Psychology	This study analyzed the statistical power of research studies published in the "Japanese Journal of Psychology" in 2008 and 2009. Sample effect sizes and sample statistical powers were calculated for each statistical test and analyzed with respect to the analytical methods and the fields of the studies. The results show that in the fields like perception, cognition or learning, the effect sizes were relatively large, although the sample sizes were small At the same time, because of the small sample sizes, some meaningful effects could not be detected. In the other fields, because of the large sample sizes, meaningless effects could be detected This implies that researchers who could not get large enough effect sizes would use larger samples to obtain significant results. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2012	83	1	51-63
47	Spybrook, Jessaca K.	Examining the experimental designs and statistical power of group randomized trials funded by the institute of education sciences	Dissertation Abstracts International Section A: Humanities and Social Sciences	In federally sponsored education research, randomized trials, particularly those that randomize entire classrooms or schools, have been deemed as the most effective method for establishing strong evidence of the effectiveness of an intervention. Consequently, there has been a dramatic increase in the number of federally funded group randomized trials. However, the occurrence of a group randomized trial does not guarantee high-quality evidence. Low statistical power in group randomized trials may prevent studies from yielding conclusive evidence of the effectiveness of a program. In this study, I examine the experimental designs and power analyses for the group randomized trials funded by the National Center for Education Research (NCER) and the National Center for Education Evaluation and Regional Assistance (NCEE), two branches of the Institute of Education Sciences (IES). First, I categorize the studies by experimental design. Second, I calculate the minimum detectable effect size (MDES) for each study under plausible assumptions about intra-class correlations (ICC's), covariate-outcome correlations, and explanatory effects of blocking. Finally, I compare the MDES I calculate, denoted the recomputed MDES, to the MDES stated in the proposal. The blocked designs, that is the three-level multi-site cluster randomized trial and the four-level multi-site cluster randomized trial, constitute the most common experimental designs. The MDES for the NCER studies ranges between 0.20 and 1.0. However, the less precise NCER studies, those with an MDES ranging from 0.40 to 1.0, tended to be funded prior to 2005. The NCER studies funded in 2005 and 2006 were more precise, generally with an MDES ranging from 0.18 to 0.40. This is a positive finding and shows significant growth and improvement in the precision of NCER studies over time. The NCEE studies also tended to be more precise, with all of the MDES's falling below 0.40. The comparison between the stated MDES and the recomputed MDES for the complete sample reveals that in 56% of the studies, the two MDES's are in the same range. However, a subgroup analysis shows that there is greater agreement between the recomputed and stated MDES's for the NCEE studies than the NCER studies. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2008	68	10-A	4161
48	Cook, David A. and Hatala, Rose	Got power? A systematic review of sample size adequacy in health professions education research	Advances in Health Sciences Education	Many education research studies employ small samples, which in turn lowers statistical power. We re-analyzed the results of a meta-analysis of simulation-based education to determine study power across a range of effect sizes, and the smallest effect that could be plausibly excluded. We systematically searched multiple databases through May 2011, and included all studies evaluating simulation-based education for health professionals in comparison with no intervention or another simulation intervention. Reviewers working in duplicate abstracted information to calculate standardized mean differences (SMD's). We included 897 original research studies. Among the 627 no-intervention-comparison studies the median sample size was 25. Only two studies (0.3 %) had >=80 % power to detect a small difference (SMD > 0.2 standard deviations) and 136 (22 %) had power to detect a large difference (SMD > 0.8). 110 no-intervention-comparison studies failed to find a statistically significant difference, but none excluded a small difference and only 47 (43 %) excluded a large difference. Among 297 studies comparing alternate simulation approaches the median sample size was 30. Only one study (0.3 %) had >=80 % power to detect a small difference and 79 (27 %) had power to detect a large difference. Of the 128 studies that did not detect a statistically significant effect, 4 (3 %) excluded a small difference and 91 (71 %) excluded a large difference. In conclusion, most education research studies are powered only to detect effects of large magnitude. For most studies that do not reach statistical significance, the possibility of large and important differences still exists. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2015	20	1	73-83
49	Ward, Rose Marie	Highly significant findings in psychology: A power and effect size survey	Dissertation Abstracts International: Section B: The Sciences and Engineering	The quality of current psychological research has been questioned because of perceived flaws in the primary methods of inquiry. As early as the late 1930s researchers began criticizing the methods by which psychological research examines hypotheses. Over the last 50 years, researchers have suggested that the American Psychological Association require additional indices to augment findings of statistical significance. Among the indices suggested are statistical power and measures of magnitude of effect size. While researchers and the APA have suggested utilizing these and other methods to supplement NHST, it seems that researchers are not currently using statistical power and effect size measures to their fullest extent. The current study examined the articles in three psychology journals to assess the current status of statistical power and effect size measures. The results of the current study suggest that about 7% of studies estimate or discuss statistical power, and about 30% calculate effect size measures. These numbers are far below the desired level of mandatory reporting of these measures. Also, when statistical power was calculated for 157 articles (45 in the Journal of Personality and Social Psychology, 57 in the Journal of Consulting and Clinical Psychology, and 55 in the Journal of Abnormal Psychology ) for 2,747 statistical tests for a total of 27,705 power calculations (power was calculated for effects beyond the normal small, medium, and large), a slight increase (above the original 1962 study and the replication in 1990) in statistical power was noted. In terms of effect size measures, a medium effect size was discovered as the average effect size across studies, which confirms previous researchers speculations about the average effect size in psychological research. It would seem that though the average effect size in the current research is of medium size, current research designs do not have sufficient statistical power to detect such an effect size. The current research should also strive to improve current statistical power survey methods to incorporate more advanced statistical methods to gain a more representative evaluation of the average effect size in psychological research. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2002	63	5-B	2630
50	Schoonen, Rob	The internal validity of efficacy studies: Design and statistical power in studies of language therapy for aphasics	Brain and Language	Discusses 2 aspects (research design and statistical significance testing) of the internal validity of efficacy studies of language therapy for aphasic patients. Of the 35 studies reviewed, 3 are considered to have an adequate design to draw valid conclusions. It is estimated that none of the studies had enough statistical power to detect small therapy effects and only a few had enough power to detect medium and large effects. The internal validity problems are viewed as the major cause of the conflicting conclusions in the efficacy studies in the past 3 decades. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1991	41	3	446-464
51	Treiber, Bernhard	An investigation of interaction of teacher methods and student ability: Reanalysis of statistical power	Zeitschrift fur Entwicklungspsychologie und Padagogische Psychologie	To estimate the statistical power of 77 studies that tested the aptitude-treatment interaction (ATI) hypothesis, posteriori analyses were conducted using G. H. Bracht's (1969) ATI summary and following J. Cohen's (1969) computational procedures. Results suggest that there is no rational basis for assuming the nonexistence of smaller interaction effects. Suggestions are given for improving future ATI research. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1977	9	1	29-35
52	Hall, W. and Heather, N.	Issue of statistical power in comparative evaluations of minimal and intensive controlled drinking interventions	Addictive Behaviors	Conducted an analysis of 7 recent studies of minimal and intensive cognitive-behavioral treatments for problem drinking to determine whether a lack of statistical power explains the failure of the majority of studies to find a difference in outcome between these 2 types of treatment. The analysis suggests that the difference in outcome between one positive study and the majority of null results reflects some combination of differences in the type of clients who were treated, the therapists' experience, and the type of intensive therapy that was provided. The low power of these studies demonstrates the desirability of researchers calculating the sample size required to detect an effect before commencing an outcome study. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1991	16	1-2	83-87
53	Burback, D., Molnar, F. J., St John, P. and Man-Son-Hing, M.	Key methodological features of randomized controlled trials of Alzheimer's disease therapy - Minimal clinically important difference, sample size and trial duration	Dementia and Geriatric Cognitive Disorders	Background: The results of clinical trials are routinely presented in terms of statistical significance, which may or may not indicate clinical significance. Analysis of the minimal clinically important difference (MCID) of cognitive scales has received little attention to date. Objectives: By reviewing the key methodological features (sample size, duration, statistical and clinical significance) of clinical trials examining the efficacy of tacrine in the treatment of Alzheimer's disease (AD), we assessed their ability to detect clinically important changes in cognition. Design: The value for the MCID of the Mini-Mental State Examination (MMSE) was determined by surveying specialists in neurology and geriatric medicine. This value was then used to interpret the clinical significance of the results of published randomized controlled trials (RCTs) assessing the efficacy of tacrine in the treatment of AD and to retrospectively determine their optimal sample size and trial duration. Results: The mean survey MCID for the MMSE was 3.72 (95% confidence interval 3.50-3.95) points. Only 2 of 12 tacrine RCTs using the MMSE found a statistically significant difference in MMSE scores for patients taking tacrine compared with those taking placebo. These improvements were not clinically significant when compared with the survey MMSE MCID. For parallel trials of tacrine in AD, the smallest sample size and minimum trial duration required to demonstrate a clinically significant difference were calculated to be 53 subjects and 1 year, respectively. Five of the 7 parallel trials met the required sample size; however, none of them met the criteria for trial duration. Conclusions: When using the MMSE as an outcome measure, no tacrine trial reported results that were clinically significant as perceived by clinicians working with dementia patients. Application of a range of plausible MCIDs to the parallel design RCTs also demonstrated that 2 of 7 of these trials did not have sufficient sample size, and none had sufficient duration of treatment to reliably detect clinically meaningful changes in cognition. Future clinical trials in this area will need to incorporate the evolving knowledge of MCIDs in order to increase their chance of detecting clinically relevant results. Copyright (C) 1999 S. Karger AG, Basel.	1999	10	6	534-540
54	Cafri, Guy, Kromrey, Jeffrey D. and Brannick, Michael T.	A Meta-Meta-Analysis: Empirical Review of Statistical Power, Type I Error Rates, Effect Sizes, and Model Selection of Meta-Analyses Published in Psychology	Multivariate Behavioral Research	This article uses meta-analyses published in Psychological Bulletin from 1995 to 2005 to describe meta-analyses in psychology, including examination of statistical power, Type I errors resulting from multiple comparisons, and model choice. Retrospective power estimates indicated that univariate categorical and continuous moderators, individual moderators in multivariate analyses, and tests of residual variability within individual levels of categorical moderators had the lowest and most concerning levels of power. Using methods of calculating power prospectively for significance tests in meta-analysis, we illustrate how power varies as a function of the number of effect sizes, the average sample size per effect size, effect size magnitude, and level of heterogeneity of effect sizes. In most meta-analyses many significance tests were conducted, resulting in a sizable estimated probability of a Type I error, particularly for tests of means within levels of a moderator, univariate categorical moderators, and residual variability within individual levels of a moderator. Across all surveyed studies, the median effect size and the median difference between two levels of study level moderators were smaller than Cohen's (1988) conventions for a medium effect size for a correlation or difference between two correlations. The median Birge's (1932) ratio was larger than the convention of medium heterogeneity proposed by Hedges and Pigott (2001) and indicates that the typical meta-analysis shows variability in underlying effects well beyond that expected by sampling error alone. Fixed-effects models were used with greater frequency than random-effects models; however, random-effects models were used with increased frequency over time. Results related to model selection of this study are carefully compared with those from Schmidt, Oh, and Hayes (2009), who independently designed and produced a study similar to the one reported here. Recommendations for conducting future meta-analyses in light of the findings are provided.	2010	45	2	239-270
55	Moore, James Douglas, Jr.	Moving beyond point estimates in counseling psychology research: Implications for future research and meta-analysis inquiries	Dissertation Abstracts International: Section B: The Sciences and Engineering	This inquiry provides an assessment of correlation evidence provided in the five-year period of the Journal of Counseling Psychology beginning with the first issue of Volume 41 published in January, 1994. Three research objectives guided the inquiry. The first objective specified and described all 118 articles providing Pearson correlation evidence. The second objective identified and described characteristics for 7,971 Pearson correlation coefficients reported in the 118 articles. The third objective illustrated how to integrate correlations measuring common bivariate constructs. Only one of the 118 articles used probability sampling. While all 118 articles focused primarily on hypothesis testing, none explicitly discussed statistical power. None of the articles used confidence intervals to reflect sampling error, only a few (less than 10%) adequately addressed validity issues and 57 (48%) reported reliability indices for sample data. A total of 5,830 unique bivariate constructs were measured. The most frequent reoccurring bivariate constructs correlated each of Holland's occupational interest themes with one another (used 30 times). Sample sizes for the 7,791 total correlations ranged from a minimum of twelve to a maximum of 18,666. The median sample size was 139 and the average sample size was 271. The final chapter concludes with a detailed discussion of recommendations for future research and research synthesis efforts stemming from the research agenda begun in this inquiry. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2002	62	7-B	3385
56	McNeish, D.	Multilevel Mediation With Small Samples: A Cautionary Note on the Multilevel Structural Equation Modeling Framework	Structural Equation Modeling-a Multidisciplinary Journal	Multilevel structural equation modeling (ML-SEM) for multilevel mediation is noted for its flexibility over a system of multilevel models (MLMs). Sample size requirements are an overlooked limitation of ML-SEM (100 clusters is recommended). We find that 89% of ML-SEM studies have fewer than 100 clusters and the median number is 44. Furthermore, 75% of ML-SEM studies implement 2-1-1 or 1-1-1 models, which can be equivalently fit with MLMs. MLMs theoretically have lower sample size requirements, although studies have yet to assess small sample performance for multilevel mediation. We conduct a simulation to address this pervasive problem. We find that MLMs have more desirable small sample performance and can be trustworthy with 10 clusters. Importantly, many studies lack the sample size and model complexity to necessitate ML-SEM. Although ML-SEM is undeniably more flexible and uniquely positioned for difficult problems, small samples often can be more effectively and simply addressed with MLMs.	2017	34	4	609-625
57	Fraley, R. and Vazire, Simine	The N-pact Factor: Evaluating the quality of empirical journals with respect to sample size and statistical power	PLoS ONE Vol 9(10), 2014, ArtID e109019	The authors evaluate the quality of research reported in major journals in social-personality psychology by ranking those journals with respect to their N-pact Factors (NF)-the statistical power of the empirical studies they publish to detect typical effect sizes. Power is a particularly important attribute for evaluating research quality because, relative to studies that have low power, studies that have high power are more likely to (a) to provide accurate estimates of effects, (b) to produce literatures with low false positive rates, and (c) to lead to replicable findings. The authors show that the average sample size in social-personality research is 104 and that the power to detect the typical effect size in the field is approximately 50%. Moreover, they show that there is considerable variation among journals in sample sizes and power of the studies they publish, with some journals consistently publishing higher power studies than others. The authors hope that these rankings will be of use to authors who are choosing where to submit their best work, provide hiring and promotion committees with a superior way of quantifying journal quality, and encourage competition among journals to improve their NF rankings. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2014	9	10	
58	Medina, Jared and Cason, Samuel	No evidential value in samples of transcranial direct current stimulation (tDCS) studies of cognition and working memory in healthy populations	Cortex: A Journal Devoted to the Study of the Nervous System and Behavior	A substantial number of studies have been published over the last decade, claiming that transcranial direct current stimulation (tDCS) can influence performance on cognitive tasks. However, there is some skepticism regarding the efficacy of tDCS, and evidence from meta-analyses are mixed. One major weakness of these meta-analyses is that they only examine outcomes in published studies. Given biases towards publishing positive results in the scientific literature, there may be a substantial "file-drawer" of unpublished negative results in the tDCS literature. Furthermore, multiple researcher degrees of freedom can also inflate published p-values. Recently, Simonsohn, Nelson and Simmons (2014) created a novel meta-analytic tool that examines the distribution of significant p-values in a literature, and compares it to expected distributions with different effect sizes. Using this tool, one can assess whether the selected studies have evidential value. Therefore, we examined a random selection of studies that used tDCS to alter performance on cognitive tasks, and tDCS studies on working memory in a recently published meta-analysis (Mancuso et al., 2016). Using a p-curve analysis, we found no evidence that the tDCS studies had evidential value (33% power or greater), with the estimate of statistical power of these studies being approximately 14% for the cognitive studies, and 5% (what would be expected from randomly generated data) for the working memory studies. It is likely that previous tDCS studies are substantially underpowered, and we provide suggestions for future research to increase the evidential value of future tDCS studies. (PsycINFO Database Record (c) 2017 APA, all rights reserved)	2017	94		131-141
59	Mone, M. A., Mueller, G. C. and Mauland, W.	The perceptions and usage of statistical power in applied psychology and management research	Personnel Psychology	We first assess the current level of statistical power across articles in seven leading journals that represent a broad sample of applied psychology and management research. We next survey the authors of these articles to examine their perceptions and usage of statistical power analysis. Finally, we examine the perceptions and usage of power analysis in a survey of authors of regression-based research appearing in leading journals. Findings from the assessment of power and surveys of researchers indicate that power analyses are not typically conducted, researchers perceive little need for statistical power, and power in published research is low. We conclude by discussing implications of low power for the field and recommending avenues for improving researchers' awareness and usage of statistical power.	1996	49	1	103-120
60	Dilullo, Linda Kay	A post hoc power analysis of inferential research examining the relationship between mathematics anxiety and mathematics performance	Dissertation Abstracts International Section A: Humanities and Social Sciences	It is investigated in this dissertation whether or not low statistical power is the cause of the discrepancies existing in inferential quantitative research investigating the relationship between mathematics anxiety and mathematics performance. All journal articles investigating this relationship were selected from the annual listings of reported mathematics education research included in the Journal for Research in Mathematics Education for the years 1976-1995. The sample consisted of 81 statistical tests investigating the relationship between mathematics anxiety and mathematics performance within these articles. A post hoc statistical power analysis using Cohen's classifications of small and medium effect sizes, alpha2 =.05, and the sample sizes listed in the articles was performed on these data. To aide in the interpretation of the power results, the obtained effect size for each test was also computed. Overall power for the study was.81 at Cohen's classification of medium effect. However, 39% of these power values were less than Cohen's ideal power value of.80 and 41% were above the excessive power value of.95. Furthermore, 50% of the power values for nonsignificant results and 40% of the power values for significant results had serious power problems (their power values were less than.50 or greater than.95). When the average obtained effect size was computed for each statistical procedure, a medium effect size, just below a medium effect size, and a small effect size best represented the relationship between the two variables in the Pearson L tests, the ANOVA tests, and the multiple regression tests, respectively. Power was adequate in the Pearson r and ANOVA statistical procedures to detect a medium effect size, but it was insufficient to detect the the effect size in the multiple regression procedures. For nonsignificant results, the average obtained effect sizes were small for each statistical procedure and power was woefully inadequate to detect the small effects. Similar results surfaced for overall and nonsignificant results at the middle school, high school, college, and post graduate educational levels. The results suggested that low power is indeed a cause of the discrepancies which exist in the research investigating the relationship between mathematics anxiety and mathematics performance. The statistical tests for which power levels were less than.50 for their respective obtained effect size should be replicated with attention given to a priori statistical power to ensure sample sizes large enough to detect the appropriate effect size. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1998	58	7-A	2574
61	Jackson, Dan and Turner, Rebecca	Power analysis for random-effects meta-analysis	Research Synthesis Methods	One of the reasons for the popularity of meta-analysis is the notion that these analyses will possess more power to detect effects than individual studies. This is inevitably the case under a fixed-effect model. However, the inclusion of the between-study variance in the random-effects model, and the need to estimate this parameter, can have unfortunate implications for this power. We develop methods for assessing the power of random-effects meta-analyses, and the average power of the individual studies that contribute to meta-analyses, so that these powers can be compared. In addition to deriving new analytical results and methods, we apply our methods to 1991 meta-analyses taken from the Cochrane Database of Systematic Reviews to retrospectively calculate their powers. We find that, in practice, 5 or more studies are needed to reasonably consistently achieve powers from random-effects meta-analyses that are greater than the studies that contribute to them. Not only is statistical inference under the random-effects model challenging when there are very few studies but also less worthwhile in such cases. The assumption that meta-analysis will result in an increase in power is challenged by our findings. (PsycINFO Database Record (c) 2017 APA, all rights reserved)	2017			No Pagination Specified
62	Haase, Richard F.	Power analysis of research in counselor education	Counselor Education and Supervision	Discusses statistical power analysis as a necessary and desirable activity in counselor education research. Power of counseling research is discussed with respect to alpha levels, sample size, and size of effect. 4 yrs of counselor education research are reviewed and analyzed according to the criteria of statistical power. Recommendations for increasing the power of research in counselor education are made. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1974	14	2	124-132
63	Button, Katherine S., Ioannidis, John P., Mokrysz, Claire, Nosek, Brian A., Flint, Jonathan, Robinson, Emma S. and Munafo, Marcus R.	Power failure: Why small sample size undermines the reliability of neuroscience	Nature Reviews Neuroscience	[Correction Notice: An Erratum for this article was reported in Vol 14(6) of Nature Reviews Neuroscience (see record 2013-18272-020). In the original article, on page 366 of this article, the definition of R should have read: "R is the pre-study odds (that is, the odds that a probed effect is indeed non-null among the effects being probed)".] A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles. (PsycINFO Database Record (c) 2017 APA, all rights reserved)	2013	14	5	365-376
64	Pulver, Ann E., Bartko, John J. and McGrath, John A.	The power of analysis: Statistical perspectives: I	Psychiatry Research	Discusses how failure to consider statistical power when achieving apparently "negative" results prevents accurate interpretation of the results. It is noted that a nonsignificant result can be obtained when one includes an insufficient number of Ss or when one has an adequate number of Ss but a meaningful effect does not exist. The present authors examined 154 published nonsignificant t-test results and determined that when power is calculated with an effect size equal to standardized difference of unity, over 50% of the test were found to have inadequate power. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1988	23	3	295-299
65	Sindelar, Paul T., Allman, Carol, Monda, Lisa, Vail, Cynthia O., Wilson, Cynthia L. and Schloss, Patrick J.	The power of hypothesis testing in special education efficacy research	The Journal of Special Education	Investigated the efficacy of special class placement for mildly handicapped children. The meta-analysis of C. Carlberg and K. Kavale (1980) found that, in general, special class placement had proved to be no more effective than regular class placement on measures of achievement and social adjustment. However, the power with which the efficacy of special classes was tested is unknown. Thus, inadequate statistical power represents an alternative for the findings. The power of the statistical tests to detect small, medium, and large effect sizes was determined in 35 studies reviewed by Carlberg and Kavale. 21 studies had adequate power to detect a large effect size. The effect sizes for these 21 studies on academic and social measures were highly variable, and their averages were quite small. Findings provide further support that mildly handicapped students are no better off in special than regular classes. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1988	22	3	284-296
66	Gaskin, C. J. and Happell, B.	Power of mental health nursing research: A statistical analysis of studies in the International Journal of Mental Health Nursing	International Journal of Mental Health Nursing	Having sufficient power to detect effect sizes of an expected magnitude is a core consideration when designing studies in which inferential statistics will be used. The main aim of this study was to investigate the statistical power in studies published in the International Journal of Mental Health Nursing. From volumes 19 (2010) and 20 (2011) of the journal, studies were analysed for their power to detect small, medium, and large effect sizes, according to Cohen's guidelines. The power of the 23 studies included in this review to detect small, medium, and large effects was 0.34, 0.79, and 0.94, respectively. In 90% of papers, no adjustments for experiment-wise error were reported. With a median of nine inferential tests per paper, the mean experiment-wise error rate was 0.51. A priori power analyses were only reported in 17% of studies. Although effect sizes for correlations and regressions were routinely reported, effect sizes for other tests (?2-tests, t-tests, ANOVA/MANOVA) were largely absent from the papers. All types of effect sizes were infrequently interpreted. Researchers are strongly encouraged to conduct power analyses when designing studies, and to avoid scattergun approaches to data analysis (i.e. undertaking large numbers of tests in the hope of finding significant results). Because reviewing effect sizes is essential for determining the clinical significance of study findings, researchers would better serve the field of mental health nursing if they reported and interpreted effect sizes.	2013	22	1	69-75
67	Brown, J. and Hale, M. S.	The power of statistical studies in consultation-liaison psychiatry	Psychosomatics	Several authors recently have proclaimed the need for empirically based research articles in consultation-liaison psychiatry. The authors report that although the proportion of empirically based studies published in Psychosomatics increased 148% from 1979 to 1989, the power of statistical analyses and the deleterious effect of multiple tests were often neglected. A power analysis of empirical studies published in the 1989 volume year of Psychosomatics is reported, showing statistical power to be low for all but the most robust of effect sizes.	1992	33	4	437-443
68	Tomcho, Thomas J. and Foels, Rob	The power of teaching activities: Statistical and methodological recommendations	Teaching of Psychology	Researchers rarely mention statistical power in Teaching of Psychology teaching activity studies. Insufficiently powered tests promote uncertainty in the decision to accept or reject the tested null hypothesis and influence the interpretation of results. We analyzed the a priori power of statistical tests from 197 teaching activity effectiveness studies published from 1974 through 2006. We found that two thirds of the studies were powerful enough to detect only large effects. We compared observed sample sizes with expert recommendations and found that studies typically used sample sizes that were too small. We discuss limitations of underpowered statistical tests in evaluating teaching activity effectiveness and make design-related recommendations for improving power. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2009	36	2	90-95
69	Smith, Daniel R., Hardy, Ian C. and Gammell, Martin P.	Power rangers: No improvement in the statistical power of analyses published in Animal Behaviour	Animal Behaviour	Statistical hypothesis testing is widely employed to assess the probability (P) of an effect size estimated from the observed data occurring by sampling error alone, assuming the null hypothesis (H0) is true and the alternative hypothesis (H1) is false. In testing the statistical significance of an effect size, the researcher sets a significance threshold, or Type 1 error rate (a), to minimize the likelihood that a true H0 will be rejected. As a is lowered, for instance from 0.05 to 0.01, the probability of rejecting a true H0 is decreased but the probability of accepting a false H0, termed Type 2 error, is increased. If a researcher reports a nonsignificant (NS) test result. Here, we investigate the impact of this additional information to authors by comparing average power values of statistical tests from 1996 (the year immediately preceding recommendation), 2003 and 2009. There is clearly no difference in the median statistical power of tests published in Animal Behaviour between 1996, 2003 and 2009: we thus conclude that there has been no improvement. Median power values obtained in our study were all low and, as expected, greater for medium than for small standardized effect sizes. (PsycINFO Database Record (c) 2017 APA, all rights reserved)	2011	81	1	347-352
70	Kazdin, Alan E. and Bass, Debra	Power to detect differences between alternative treatments in comparative psychotherapy outcome research	Journal of Consulting and Clinical Psychology	Comparative studies of psychotherapy often find few or no differences in the outcomes that alternative treatments produce. Although these findings may reflect the comparability of alternative treatments, studies are often not sufficiently powerful to detect the sorts of effect sizes likely to be found when two or more treatments are contrasted. The present survey evaluated the power of psychotherapy outcome studies to detect differences for contrasts of two or more treatments and treatment vs no-treatment. 85 outcome studies were drawn from 9 journals over a 3-yr period (1984-1986). Data in each article were examined first to provide estimates of effect sizes and then to evaluate statistical power at posttreatment and follow-up. Findings indicate that the power of studies to detect differences between treatment and no treatment is quite adequate given the large effect sizes usually evident for this comparison. However, the power is relatively weak to detect the small-to-medium effect sizes likely to be evident when alternative treatments are contrasted. Thus, the equivalent outcomes that treatments produce may be due to the relatively weak power of the tests. Implications for interpreting outcome studies and for designing comparative studies are highlighted. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1989	57	1	138-147
71	Kazantzis, N.	Power to detect homework effects in psychotherapy outcome research	Journal of Consulting and Clinical Psychology	Studies of homework effects in psychotherapy outcome have produced inconsistent results. Although these findings may reflect the comparability of psychotherapy with and without homework assignments, many of these studies may not have been sensitive enough to detect the effects sizes (ESs) likely to be found when examining homework effects. The present study evaluated the power of homework research and showed that, on average, current power levels are relatively weak in controlled studies ranging from 0.58 for large ESs to 0.09 for small ESs. Thus, inconsistent findings between studies may very well be due to low statistical power.	2000	68	1	166-170
72	Chase, Lawrence J. and Tucker, Raymond K.	A power-analytic examination of contemporary communication research	Speech Monographs	An examination of articles published in 1973 in 9 communication journals indicates that published communication research was generally lacking in statistical power. (22 ref) (PsycINFO Database Record (c) 2017 APA, all rights reserved)	1975	42	1	29-41
73	Kim, Dae Shik	Power, effect size, and practical significance: How the reporting in Journal of Visual Impairment & Blindness articles has changed in the past 20 years	Journal of Visual Impairment & Blindness	This article examines how the the reporting of results in Journal of Visual Impairment & Blindness (JVIB) articles has changed in the past 20 years. The authors examined JVIB articles published in 1992-1993 and 2012-2013. In their analyses they included only quantitative studies that employed inferential statistical procedures solely because it is not possible to conduct power analyses for single-subject design and purely descriptive studies. The results showed that there was no mention of statistical power found in the 1992-1993 volumes of JVIB; even in the 2012-2013 volumes, only 12.5% of the articles performed power analyses to determine adequate sample size. The overall statistical power of the studies in JVIB increased noticeably in the past 20 years as a result of including more nonexperimental studies, but there was no increase in statistical power among the experimental design studies. The percentage of studies that referred to the obtained effect size when discussing practical significance of the findings remained low (25%) even in 2012-2013. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2015	109	3	214-218
74	Gaskin, Cadeyrn J. and Happell, Brenda	Power, effects, confidence, and significance: An investigation of statistical practices in nursing research	International Journal of Nursing Studies	Objectives: To (a) assess the statistical power of nursing research to detect small, medium, and large effect sizes; (b) estimate the experiment-wise Type I error rate in these studies; and (c) assess the extent to which (i) a priori power analyses, (ii) effect sizes (and interpretations thereof), and (iii) confidence intervals were reported. Design: Statistical review. Data sources: Papers published in the 2011 volumes of the 10 highest ranked nursing journals, based on their 5-year impact factors. Review methods: Papers were assessed for statistical power, control of experiment-wise Type I error, reporting of a priori power analyses, reporting and interpretation of effect sizes, and reporting of confidence intervals. The analyses were based on 333 papers, from which 10,337 inferential statistics were identified. Results: The median power to detect small, medium, and large effect sizes was .40 (interquartile range [IQR]=.24-.71), .98 (IQR =.85-1.00), and 1.00 (IQR =1.00-1.00), respectively. The median experiment-wise Type I error rate was .54 (IQR =.26-.80). A priori power analyses were reported in 28% of papers. Effect sizes were routinely reported for Spearman's rank correlations (100% of papers in which this test was used), Poisson regressions (100%), odds ratios (100%), Kendall's tau correlations (100%), Pearson's correlations (99%), logistic regressions (98%), structural equation modelling/confirmatory factor analyses/path analyses (97%), and linear regressions (83%), but were reported less often for two-proportion z tests (50%), analyses of variance/analyses of covariance/multivariate analyses of variance (18%), t tests (8%), Wilcoxon's tests (8%), Chi-squared tests (8%), and Fisher's exact tests (7%), and not reported for sign tests, Friedman's tests, McNemar's tests, multi-level models, and Kruskal-Wallis tests. Effect sizes were infrequently interpreted. Confidence intervals were reported in 28% of papers. Conclusion: The use, reporting, and interpretation of inferential statistics in nursing research need substantial improvement. Most importantly, researchers should abandon the misleading practice of interpreting the results from inferential tests based solely on whether they are statistically significant (or not) and, instead, focus on reporting and interpreting effect sizes, confidence intervals, and significance levels. Nursing researchers also need to conduct and report a priori power analyses, and to address the issue of Type I experiment-wise error inflation in their studies. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2014	51	5	795-806
75	Whittington, C. J., Podd, J. and Kan, M. M.	Recognition memory impairment in Parkinson's disease: Power and meta-analyses	Neuropsychology	Contrary findings notwithstanding, the prevailing notion is that recognition memory is little affected by Parkinson's disease (PD). Both a power analysis and a meta-analysis were conducted to help clarify the degree of recognition memory deficit associated with PD. The power analysis confirmed that, in general, memory studies of PD participants have been underpowered. This analysis indicated the need to pool study results in a subsequent meta-analysis, the main finding of which was that recognition memory deficits do occur with PD. The largest deficit occurs in PD participants with dementia. Nevertheless, deficits also occur in PD participants without dementia on medication, but nondopaminergic central nervous system abnormalities are more likely to underlie this deficit than PD medication itself. Future development of a theory of cognitive dysfunction in PD should take into account these recognition memory deficits, which may increase with disease progression.	2000	14	2	233-246
76	Tressoldi, P. E.	Replication unreliability in psychology: elusive phenomena or "elusive" statistical power?	Frontiers in Psychology	The focus of this paper is to analyze whether the unreliability of results related to certain controversial psychological phenomena may be a consequence of their low statistical power. Applying the Null Hypothesis Statistical Testing (NHST), still the widest used statistical approach, unreliability derives from the failure to refute the null hypothesis, in particular when exact or quasi-exact replications of experiments are carried out. Taking as example the results of meta-analyses related to four different controversial phenomena, subliminal semantic priming, incubation effect for problem solving, unconscious thought theory, and non-local perception, it was found that, except for semantic priming on categorization, the statistical power to detect the expected effect size (ES) of the typical study, is low or very low. The low power in most studies undermines the use of NHST to study phenomena with moderate or low ESs. We conclude by providing some suggestions on how to increase the statistical power or use different statistical approaches to help discriminate whether the results obtained may or may not be used to support or to refute the reality of a phenomenon with small ES.	2012	3		
77	Schweizer, G. and Furley, P.	Reproducible research in sport and exercise psychology: The role of sample sizes	Psychology of Sport and Exercise	Objectives: We aim to introduce the discussion on the crisis of confidence to sport and exercise psychology. We focus on an important aspect of this debate, the impact of sample sizes, by assessing sample sizes within sport and exercise psychology. Researchers have argued that publications in psychological research contain numerous false-positive findings and inflated effect sizes due to small sample sizes. Method: We analyse the four leading journals in sport and exercise psychology regarding sample sizes of all quantitative studies published in these journals between 2009 and 2013. Subsequently, we conduct power analyses. Results: A substantial proportion of published studies does not have sufficient power to detect effect sizes typical for psychological research. Sample sizes and power vary between research designs. Although many correlational studies have adequate sample sizes, experimental studies are often underpowered to detect small-to-medium effects. Conclusions: As sample sizes are small, research in sport and exercise psychology may suffer from false positive results and inflated effect sizes, while at the same time failing to detect meaningful small effects. Larger sample sizes are warranted, particularly in experimental studies. (C) 2015 Elsevier Ltd. All rights reserved.	2016	23		114-122
78	Holmes, C. B., Holmes, J. R. and Fanning, J. J.	Sample size in non-APA journals	Journal of Psychology	Ten non-APA journals for the year 1977 were examined to determine the typical sample size as reported in those journals. Descriptive statistics are presented on the total sample size per experiment and on individual group samples. The results are consistent with data presented on sample size as reported in APA journals.	1981	108	2	263-266
79	Marszalek, J. M., Barber, C., Kohlhart, J. and Holmes, C. B.	Sample size in psychological research over the past 30 years	Perceptual and Motor Skills	The American Psychological Association (APA) Task Force on Statistical Inference was formed in 1996 in response to a growing body of research demonstrating methodological issues that threatened the credibility of psychological research, and made recommendations to address them. One issue was the small, even dramatically inadequate, size of samples used in studies published by leading journals. The present study assessed the progress made since the Task Force's final report in 1999. Sample sizes reported in four leading APA journals in 1955, 1977, 1995, and 2006 were compared using nonparametric statistics, while data from the last two waves were fit to a hierarchical generalized linear growth model for more in-depth analysis. Overall, results indicate that the recommendations for increasing sample sizes have not been integrated in core psychological research, although results slightly vary by field. This and other implications are discussed in the context of current methodological critique and practice.	2011	112	2	331-348
80	Holmes, C. B.	Sample size in psychological-research	Perceptual and Motor Skills	This study was conducted to provide information on the typical sample size employed in psychological research, as it is reported in selected American Psychological Association journals. All the articles in those journals for the years 1955 and 1977 were read to ascertain both the total n for the study, and the size of each of the groups that made up that total n. The mean, SD, median, 25th percentile, 75th percentile, modes, and range were calculated for each year, both for total n and for individual group n. A comparison of the 1955 data and the 1977 data showed no significant changes in sample size between those years. Certain problems in determining sample size, as it was reported, are presented in this article.	1979	49	1	283-288
81	Salgado, J. F.	Sample size in validity studies of personnel selection	Journal of Occupational and Organizational Psychology	This report presents the results of several meta-analyses in which samples in the articles published in Journal of Applied Psychology, Journal of Occupational and Organizational Psychology and Personnel Psychology from 1983 to 1994 mere integrated. The findings show that in the last decade a small increase in sample size has been observed in relation to sample size of studies published two or three decades ago. However, the mean sample continues to be of a small size. This last finding suggests that sampling error continues to be the most important artifactual error for explaining the variability of validity coefficients. Some implications for the confidence intervals and power analysis are discussed.	1998	71		161-164
82	Nelson, Matthew S., Wooditch, Alese and Dario, Lisa M.	Sample size, effect size, and statistical power: A replication study of Weisburd's paradox	Journal of Experimental Criminology	Objectives: This study expands upon Weisburd's work (1993) by reexamining the relationship between sample size and statistical power in criminological experiments. This inquiry, now known as the Weisburd paradox, postulates that increasing the sample size of experiments does not always lead to increases in statistical power. The current research also begins to explore the potential sources of the Weisburd paradox.Methods: The effect sizes and statistical power are computed for the outcome measures (n = 402) of all experiments (n = 66) included in systematic reviews published by the Campbell Collaboration's Crime and Justice Coordinating Group. The design sensitivity of these experiments is reviewed by sample size, as well as other factors that may explain the variation in effect sizes and statistical power across studies.Results: Effect sizes decline as the sample size of the experiment increases, whereas statistical power is unrelated to sample size but strongly associated with effect size. Disclosure of fidelity issues and publication bias is unrelated to statistical power and treatment effects. Variability in the dependent variable and sample demographics are significantly related to statistical power, but not to effect size.Conclusions: The study finds support for the Weisburd paradox, as the ability to manipulate statistical power by increasing sample size is not as strong as statistical theory would suggest, and experiments with larger sample sizes generally produce smaller effects. It is believed that a relationship was not observed between sample size and statistical power because the sensitivity gained from increasing sample size is offset by effect size simultaneously decreasing. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2015	11	1	141-163
83	Thombs, B. D. and Rice, D. B.	Sample sizes and precision of estimates of sensitivity and specificity from primary studies on the diagnostic accuracy of depression screening tools: a survey of recently published studies	International Journal of Methods in Psychiatric Research	Depression screening tools are useful to the extent that they accurately discriminate between depressed and non-depressed patients. Studies without enough patients to generate precise estimates make it difficult to evaluate accuracy. We conducted a survey of recently published studies on depression screening tool accuracy to evaluate the percentage with sample size calculations; the percentage that provided confidence intervals; and precision, based on the width and lower bounds of 95% confidence intervals for sensitivity and specificity. We calculated 95% confidence intervals, if possible, when not provided. Only three of 89 studies (3%) described a viable sample size calculation. Only 30 studies (34%) provided reasonably accurate confidence intervals. Of 86 studies where 95% confidence intervals were provided or could be calculated, only seven (8%) had interval widths for sensitivity of 10%, whereas 53 (62%) had widths of 21%. Lower bounds of confidence intervals were < 80% for 84% of studies for sensitivity and 66% of studies for specificity. Overall, few studies on the diagnostic accuracy of depression screening tools reported sample size calculations, and the number of patients in most studies was too small to generate reasonably precise accuracy estimates. The failure to provide confidence intervals in published reports may obscure these shortcomings. Copyright (c) 2016 John Wiley & Sons, Ltd.	2016	25	2	145-152
84	Wing, R. R. and Jeffery, R. W.	Sample-size in clinical outcome research - the case of behavioral weight control	Behavior Therapy	This paper develops the thesis that the sample sizes which are commonly used in clinical outcome research are not sufficient to detect meaningful differences between treatments. Behavioral weight control is used to exemplify this problem. The sample sizes needed to statistically detect a difference between treatment conditions of 5, 10, and 15 pounds have been computed based on the attrition and the variability of treatment effects reported in the literature. It is demonstrated that sample sizes used in behavioral weight control studies are usually too small to detect any but the largest differences between conditions. With usual sample sizes, a 10-pound difference between conditions at the end of treatment and a 15-pound difference at follow-up (effect size of 1.2–1.3) would be required to assure statistical significance. Recommendations are made for (a) greater attention to sample size calculation in study design, (b) attempts to reduce between-subject variability, and (c) consideration of relaxing standard criteria for statistical significance in exploratory studies.

	1984	15	5	550-556
85	Shen, Winny, Kiger, Thomas B., Davies, Stacy E., Rasch, Rena L., Simon, Kara M. and Ones, Deniz S.	Samples in applied psychology: Over a decade of research in review	Journal of Applied Psychology	This study examines sample characteristics of articles published in Journal of Applied Psychology (JAP) from 1995 to 2008. At the individual level, the overall median sample size over the period examined was approximately 173, which is generally adequate for detecting the average magnitude of effects of primary interest to researchers who publish in JAP. Samples using higher units of analyses (e.g., teams, departments/work units, and organizations) had lower median sample sizes (Mdn = 65), yet were arguably robust given typical multilevel design choices of JAP authors despite the practical constraints of collecting data at higher units of analysis. A substantial proportion of studies used student samples (~40%); surprisingly, median sample sizes for student samples were smaller than working adult samples. Samples were more commonly occupationally homogeneous (~70%) than occupationally heterogeneous. U.S. and English-speaking participants made up the vast majority of samples, whereas Middle Eastern, African, and Latin American samples were largely unrepresented. On the basis of study results, recommendations are provided for authors, editors, and readers, which converge on 3 themes: (a) appropriateness and match between sample characteristics and research questions, (b) careful consideration of statistical power, and (c) the increased popularity of quantitative synthesis. Implications are discussed in terms of theory building, generalizability of research findings, and statistical power to detect effects. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2011	96	5	1055-1064
86	Chase, Lawrence J. and Chase, Richard B.	A statistical power analysis of applied psychological research	Journal of Applied Psychology	Studied the average statistical power exhibited in the 1974 Journal of Applied Psychology . 121 articles were power-analytically examined using 3 effect-size estimates and standardized (.05) alpha level, and assuming nondirectional nulls. The mean power figures for small, medium, and large population effect-size estimates were .25, .66, and .84, respectively. Interdisciplinary comparisons involving abnormal-social psychology, education, and communication indicate that applied psychology is relatively strong in terms of average statistical power. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1976	61	2	234-237
87	McKean, Kathleen E.	Statistical power analysis of doctoral dissertation research in educational psychology	Dissertation Abstracts International		1991	51	10-A	3367
88	Kosciulek, John F. and Szymanski, Edna M.	Statistical power analysis of rehabilitation counseling research	Rehabilitation Counseling Bulletin	Examined current rehabilitation counseling (RC) research from the perspective of statistical power analysis (PAN). A survey of RC professional journals yielded 150 articles, 32 of which contained statistical tests that could be power analyzed. References to PAN were found in only 1 study and only 3 authors discussed why a certain alpha or sample size was chosen. Awareness of and concern about statistical power seemed to be almost nonexistent. 100% of the studies did not have a 50-50 chance of detecting small effect sizes. Only 12 studies had a 1 in 2 chance of finding significant results assuming medium effects. Nine percent of the studies showed less than a 50-50 chance of detecting large effects. Three percent showed less than 3 in 10 chances. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1993	36	4	212-219
89	Sawyer, Alan G. and Ball, A.	Statistical power and effect size in marketing research	Journal of Marketing Research	The authors discuss how better attention to the factors of statistical power and effect size can improve the planning, execution, and reporting of marketing and consumer research. (3 p ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1981	18	3	275-290
90	Maddock, Jason Edward	Statistical power and effect size in the field of health psychology	Dissertation Abstracts International: Section B: The Sciences and Engineering	Statistical significance testing is one of the most pervasive techniques in psychology to examine treatment effects. Because of the ubiquitous use of these procedures, misuses have plagued the field of psychology (Harlow, 1997). In the first chapter, the significance test controversy is discussed in detail, and the general disregard for statistical power in most psychological research is discussed as a major contributor to this controversy (Cohen, 1988). This section ends by discussing four methods for improving significance testing: the use of confidence intervals, testing for probable upper bounds, meta-analysis, and power analyses. Each of these methods is explored because they offer simple ways in which significance testing can be improved without great resistance. In chapter two, power was calculated for 8,266 statistical tests in 187 journal articles published in the 1997 volumes of Health Psychology, Addictive Behaviors, and Journal of Studies on Alcohol. Power to detect small, medium and large effects was .34, .74, and .92 for Health Psychology, .34, .75, and .90 for Addictive Behaviors , and .41, .81, and .92 for the Journal of Studies on Alcohol . Mean power estimates are .36, .77, and .91, giving a good estimation for the field of health psychology. Comparison of these results to over 30 other power studies in fields indicate that health psychology journals rank among the highest in power. Results are encouraging for this field, although studies examining small effects are still very much underpowered. In chapter three, a meta-analysis of interventions to reduce college student drinking was conducted. Qualitative analyses examining these interventions have produced conflicting results. Twenty-one studies met criteria to be included in the meta-analyses. This criteria included use of random assignment or statistical control for baseline differences. Results indicated that the studies when examined as a group significantly reduced drinking in college students. However, cognitive-behavioral interventions (d = .53) produced significantly larger effects than traditional educational approaches (d = .17), indicating the superiority of this type of intervention. Power analysis of these articles revealed inadequate power, demonstrating the need to use higher powered studies to reduce controversial findings. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2000	60	9-B	4939
91	Bezeau, S. and Graves, R.	Statistical power and effect sizes of clinical neuropsychology research	Journal of Clinical and Experimental Neuropsychology	Cohen, in a now classic paper on statistical power, reviewed articles in the 1960 issue of one psychology journal and determined that the majority of studies had less than a 50-50 chance of detecting an effect that truly exists in the population, and thus of obtaining statistically significant results. Such low statistical power, Cohen concluded, was largely due to inadequate sample sizes. Subsequent reviews of research published in other experimental psychology journals found similar results. We provide a statistical power analysis of clinical neuropsychological research by reviewing a representative sample of 66 articles from the Journal of Clinical and Experimental Neuropsychology, the Journal of the International Neuropsychology Society, and Neuropsychology. The results show inadequate power, similar to that for experimental research, when Cohen's criterion for effect size is used. However, the results are encouraging in also showing that the field of clinical neuropsychology deals with larger effect sizes than are usually observed in experimental psychology and that the reviewed clinical neuropsychology research does have adequate power to detect these larger effect sizes. This review also reveals a prevailing failure to heed Cohen's recommendations that researchers should routinely report a priori power analyses, effect sizes and confidence intervals, and conduct fewer statistical tests.	2001	23	3	399-406
92	Okumura, Y. and Sakamoto, S.	Statistical power and effect sizes of depression research in Japan	Psychiatry and Clinical Neurosciences	Aims: Few studies have been conducted on the rationales for using interpretive guidelines for effect size, and most of the previous statistical power surveys have covered broad research domains. The present study aimed to estimate the statistical power and to obtain realistic target effect sizes of depression research in Japan. Methods: We systematically reviewed 18 leading journals of psychiatry and psychology in Japan and identified 974 depression studies that were mentioned in 935 articles published between 1990 and 2006. Results: In 392 studies, logistic regression analyses revealed that using clinical populations was independently associated with being a statistical power of <0.80 (odds ratio 5.9, 95% confidence interval 2.9-12.0) and of <0.50 (odds ratio 4.9, 95% confidence interval 2.3-10.5). Of the studies using clinical populations, 80% did not achieve a power of 0.80 or more, and 44% did not achieve a power of 0.50 or more to detect the medium population effect sizes. A predictive model for the proportion of variance explained was developed using a linear mixed-effects model. The model was then used to obtain realistic target effect sizes in defined study characteristics. Conclusion: In the face of a real difference or correlation in population, many depression researchers are less likely to give a valid result than simply tossing a coin. It is important to educate depression researchers in order to enable them to conduct an a priori power analysis.	2011	65	4	356-364
93	Ottenbacher, Kenneth	Statistical power and research in occupational therapy	Occupational Therapy Journal of Research	Conducted a post hoc power analysis of 205 statistical tests from 22 articles reporting occupational therapy research. Studies reporting small and medium effect sizes were associated with low power and were subject to a high probability of Type II experimental errors. It is suggested that occupational therapy researchers who employ power evaluations and pre-experimental planning will greatly reduce the number of Type II errors. (25 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1982	2	1	13-25
94	McQuitty, Shaun	Statistical power and structural equation models in business research	Journal of Business Research	Discusses the relevance of statistical power in the application of structural equation models in business research and its importance in measurement validation, and addresses the question of what level of power exists in models published in business journal articles. It has long been recognized that statistical power is important for structural equation models, but only recently has it become possible to estimate the power associated with the test of an entire model. Addressing this matter is essential, because statistical power directly affects the confidence with which test results can be interpreted. The issue is particularly appropriate in light of the increased use of structural equation models in business research. A survey of articles from leading business journals finds that power tends to be either very low, implying that too many false models will not be rejected (Type II error), or extremely high, causing overrejection of tenable models (Type I error). Recommendations that should improve the validity and application of structural equation modeling in business research are offered. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	2004	57	2	175-183
95	Acklin, Marvin W., McDowell, Claude J. and Orndoff, Steffani	Statistical power and the Rorschach: 1975-1991	Journal of Personality Assessment	Evaluated the average power of published Rorschach research between 1975 and 1991 to determine the extent to which artifactual controversy may plague the test's performance as a research tool. The authors also determined the extent to which the Comprehensive System, with its standardization of administration and scoring, has rectified the situation. Power was calculated for 2,300 statistical tests in 158 journal articles. Rorschach research was underpowered to detect the differences under investigation, and research conducted according to the Comprehensive System for the Rorschach was more powerful. Recommendations are offered for improving power and strengthening the design sensitivity of Rorschach research, including increasing sample sizes, use of parametric statistics, and reduction of error variance. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1992	59	2	366-379
96	Cashen, L. H. and Geiger, S. W.	Statistical power and the testing of null hypotheses: A review of contemporary management research and recommendations for future studies	Organizational Research Methods	The purpose of this study is to determine how well contemporary management research fares on the issue of statistical power with regard to studies specifically predicting null relationships between phenomena of interest. This power assessment differs from traditional power studies because it focuses solely on studies that offered and tested null hypotheses. A sample of studies containing hypothesized null relationships was taken from five mainstream management journals over the 1990 to 1999 time period. Results of the power assessment suggest that management researchers' abilities to affirm null hypotheses are low. On average, the power assessment revealed that for those studies that found nonsignificance of results and consequently affirmed their null hypotheses, the actual Type II error rate was nearly 15 times greater than what is advocated in the literature when failing to reject a false null hypothesis. Recommendations for researchers proposing and testing formal null hypotheses are also discussed.	2004	7	2	151-167
97	Orme, John G. and Combs-Orme, Terri D.	Statistical power and Type II errors in social work research	Social Work Research & Abstracts	Examined the statistical power of 79 articles published in Social Work Research and Abstracts from 1977 through 1984 to determine the extent to which social work research is designed for failure because of inadequate statistical power. Results indicate that almost half the reported studies could not detect a medium effect size with adequate statistical power. Ways to increase the effect size, possible publication biases, and selection of hypothesized effect sizes are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1986	22	3	3-10
98	Rothpearl, A. B., Mohs, R. C. and Davis, K. L.	Statistical power in biological psychiatry	Psychiatry Research	The use of statistical power and power analysis in both the design and evaluation of experiments in biological psychiatry is described. The possible copsequences of low power investigations are discussed, and guidelines are provided to facilitate the application of power analysis. Power curves are provided for sample sizes ranging from 10 to 100 for the Student’s t test, and for testing the significance of an obtained Pearson correlation coefficient at both the 0.01 and 0.05 alpha levels. Additionally, difference scales are provided for plasma cortisol and for several neurotransmitter metabolites that are frequently measured in cerebrospinal fluid (CSF). Power evaluation of selected CSF studies measuring neurotransmitter metabolites in depressives and controls suggests the majority had less than a 50% chance of detecting a medium size difference before the experiment was actually performed.	1981	5	3	257-266
99	Mazen, Abdelmagid M., Graf, Lee A., Kellogg, Calvin E. and Hemmasi, Masoud	Statistical power in contemporary management research	Academy of Management Journal	Attempts to revitalize interest in statistical power among management researchers by (1) briefly reviewing the main codeterminants of power; (2) assessing statistical power in a population of 84 management studies; and (3) discussing the meaning and implications of the findings of the present study, using examples from published research. The authors report that explicit consideration or calculation of power was almost nonexistent. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1987	30	2	369-380
100	Schmidt, Frank L., Hunter, John E. and Urry, Vern W.	Statistical power in criterion-related validation studies	Journal of Applied Psychology	Examined systematically the sample sizes necessary to provide adequate power in validation studies under various conditions of range restriction and criterion unreliability. For purposes of brevity, the examination was restricted to the validity parameter values (i.e., true validities) of .35 and .50. Results demonstrate that sample sizes required to produce adequate power in empirical validation studies are substantially larger than has typically been assumed. This finding leads to the conclusion that, from the viewpoint of sample-size requirements, criterion-related validity studies are "technically feasible" much less frequently than is commonly assumed. It is also shown that the "situational specificity" of employment test validities may be in large part a consequence of excessive faith in small-sample analyses, that is, belief in "the law of small numbers." (16 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1976	61	4	473-485
101	Valera Espin, Antonio, Sanchez Meca, Julio, Marin Martinez, Fulgencio and Velandrino Nicolas, Antonio P.	Statistical power in Revista Psicologia: General y Aplicada (1990-1992)	Revista de Psicologia General y Aplicada	Although the purpose of hypothesis testing is to reject the null hypothesis and detect relationships among variables, inadequate control of statistical power is very common in psychological research. Therefore, statistical power of papers published in Revista Psicologia: General y Aplicada (RPGA) from 1990-1992 was analyzed. The power for low, medium high and estimated effect sizes was computed. Values the authors found were .17, .57, l83 and .55, respectively. The distribution of effect magnitudes and sample sizes in journal papers was also analyzed. Results show that statistical power in RPGA is similar to other Spanish and international psychology journals. And finally, the importance of attending to statistical power, and working with sufficient power is discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1998	51	2	233-246
102	Acklin, Marvin W. and McDowell, Claude J., II	Statistical power in Rorschach research	Exner, John E Jr [Ed] (1995) Issues and methods in Rorschach research (pp 181-193) xiii, 324 pp Hillsdale, NJ, US: Lawrence Erlbaum Associates, Inc; US	the power of a statistical test is a function of the effect size (a standardized measure of the magnitude of the differences between means or the amount of variance accounted for), error variance, alpha criterion, sample size, and data analysis (the inherent power of the statistical tests to detect differences) / the Rorschach literature published between 1975 and 1990 was reviewed / the study focuses on all of the Rorschach research published in the "Journal of Personality Assessment" / the objective of this review was to evaluate average power of studies conducted with the goal of determining to what extent the controversy about the Rorschach's performance as a research tool may be based on "artifactual controversy," that is, on the failure of research designs to detect actual differences / a 2nd interest was to determine the extent to which the Comprehensive System, with its standardization of administration and scoring, has presumably rectified the situation (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1995			181-193
103	Cohen, Jacob	The statistical power of abnormal-social psychological research: A review	The Journal of Abnormal and Social Psychology		1962	65	3	145-153
104	Maddock, J. E. and Rossi, J. S.	Statistical power of articles published in three health psychology-related journals	Health Psychology	Power was calculated for 8,266 statistical rests in 187 journal articles published in the 1997 volumes of Health Psychology (HP), Addictive Behaviors (AB), and the Journal of Studies on Alcohol (JSA). Power to detect small, medium, and large effects was.34, .74, and .92 for HP; .34, .75, and.90 for AB; and .41, .81, and.92 for JSA. Mean power estimates are .36, .77, and.91, giving a good estimation for the field of health psychology. J. Cohen (1988) recommended that power to detect effects should be approximately .80. Using this criterion, the articles in these journals have adequate power to detect medium and large effects. Intervention studies have much less power to detect effects than nonintervention studies do. Results are encouraging for this field, although studies examining small effects are still very much underpowered. This issue is important, because most intervention effects in health psychology are small.	2001	20	1	76-78
105	Sugisawa, T.	Statistical power of educational psychology research in Japan	Japanese Journal of Educational Psychology	Statistical power was calculated for the articles published in the Japanese Journal of Educational Psychology between 1992 and 1996. The median power to detect small, medium, and large effects was .15,.68, and .96 respectively, and was higher than the results of previous power surveys, such as Cohen (1962). The power was different, depending on the research area and the research method. The power of tests of null hypotheses as research hypotheses was relatively high, but was not high enough to justify the use of nonsignificant results to support research hypotheses. Sample effect size was also calculated and used to examine Cohen's (1992) criteria of effect size. The correlation between sample effect size and N was negative, and the correlation between sample effect size and 1/N was stronger. Despite critical views on the limitations of power analysis, more attention should be paid to power as long as significance testing continues to be the major tool of data analysis.	1999	47	2	150-159
106	Rossi, Joseph S.	Statistical power of psychological research: What have we gained in 20 years?	Journal of Consulting and Clinical Psychology	Power was calculated for 6,155 statistical tests in 221 journal articles published in the 1982 volumes of the Journal of Abnormal Psychology, Journal of Consulting and Clinical Psychology, and Journal of Personality and Social Psychology. Power to detect small, medium, and large effects was .17, .57, and .83, respectively. 20 years after Cohen (1962) conducted the first power survey, the power of psychological research is still low. The implications of these results concerning the proliferation of Type I errors in the published literature, the failure of replication studies, and the interpretation of null (negative) results are emphasized. An example is given of the use of power analysis to help interpret null results by setting probable upper bounds on the magnitudes of effects. Limitations of statistical power analysis, suggestions for future research, sources of computational information, and recommendations for improving power are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1990	58	5	646-656
107	Woods, S. P., Rippeth, J. D., Conover, E., Carey, C. L., Parsons, T. D. and Troster, A. I.	Statistical power of studies examining the cognitive effects of subthalamic nucleus deep brain stimulation in Parkinson's disease	Clinical Neuropsychologist	It has been argued that neuropsychological studies generally possess adequate statistical power to detect large effect sizes. However, low statistical power is problematic in neuropsychological research involving clinical populations and novel interventions for which available sample sizes are often limited. One notable example of this problem is evident in the literature regarding the cognitive sequelae of deep brain stimulation (DBS) of the subthalamic nucleus (STN) in persons with Parkinson's disease (PD). In the current review, a post hoc estimate of the statistical power of 30 studies examining cognitive effects of STN DBS in PD revealed adequate power to detect substantial cognitive declines (i.e., very large effect sizes), but surprisingly low estimated power to detect cognitive changes associated with conventionally small, medium, and large effect sizes. Such wide spread Type II error risk in the STN DBS cognitive outcomes literature may affect the clinical decision-making process as concerns the possible risk of postsurgical cognitive morbidity, as well as conceptual inferences to be drawn regarding the role of the STN in higher-level cognitive functions. Statistical and methodological recommendations (e.g., meta-analysis) are offered to enhance the power of current and future studies examining the neuropsychological sequelae of STN DBS in PD.	2006	20	1	27-38
108	King, D. S.	Statistical power of the controlled research on wheat gluten and schizophrenia	Biological Psychiatry	Suggests that a consideration of statistical power is critical in interpreting the negative research findings for studies of wheat gluten effects on schizophrenia. It is indicated that practical considerations may have prevented larger sample sizes in the studies with low power. Future research should employ large sample sizes and/or identify probable responders. (12 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1985	20	7	785-787
109	Arvey, R. D., Cole, D. A., Hazucha, J. F. and Hartanto, F. M.	Statistical Power of Training Evaluation Designs	Personnel Psychology	Sample size requirements needed to achieve various levels of statistical power using posttest-only, gain-score, and analysis of covariance designs in evaluating training interventions have been developed. Results are presented which indicate that the power to detect true effects differs according to the type of design, the correlation between the pre- and posttest, and the size of the effect due to the training program. We show that the type of design and correlations between the pre- and posttest complexly determine the power curve. Finally, an estimate of typical sample sizes used in training evaluation design has been determined and reviewed to determine the power of the various designs to detect true effects, given this sample-size specification. Recommendations for type of design are provided based on sample size and projected correlations between pre- and posttest scores.	1985	38	3	493-507
110	Kosciulek, John F.	The statistical power of vocational evaluation research	Vocational Evaluation & Work Adjustment Bulletin	Discusses statistical power, the benefits of power analysis, and the history of statistical power studies. Results of a study of the statistical power of vocational evaluation (VE) research indicate that VE researchers had little chance of finding small relationships or differences that actually existed in the population of interest. Steps for performing pre-analysis power estimations are provided to aid researchers in improving the statistical power of their studies. (PsycINFO Database Record (c) 2016 APA, all rights reserved)	1993	26	4	142-145
111	Charter, R. A.	Study samples are too small to produce sufficiently precise reliability coefficients	Journal of General Psychology	In a survey of journal articles, test manuals, and test critique books, the author found that a mean sample size (N) of 260 participants had been used for reliability studies on 742 tests. The distribution was skewed because the median sample size for the total sample was only 90. The median sample sizes for the internal consistency, retest, and interjudge reliabilities were 182, 64, and 36, respectively. The author presented sample size statistics for the various internal consistency methods and types of tests. In general, the author found that the sample sizes that were used in the internal consistency studies were too small to produce sufficiently,precise reliability coefficients, which in turn could cause imprecise estimates of examinee true-score confidence intervals. The results also suggest that larger sample sizes have been used in the last decade compared with those that were used in earlier decades.	2003	130	2	117-129
112	Osborne, J. W.	Sweating the small stuff in educational psychology: how effect size and power reporting failed to change from 1969 to 1999, and what that means for the future of changing practices	Educational Psychology	Methodologists have written for years about the importance of attending to important details in quantitative research, yet there has been little research investigating methodological practice in the social sciences. This study assessed the extent to which innovations and practices are adopted by researchers voluntarily. In particular, I use the case of power analysis and effect size reporting as the primary example, but I also examine other reporting behaviours. Results show that while observed power and effect sizes in the educational psychology literature tend to be strong, researchers do not seem eager to adopt practices such as reporting effect sizes and power, and neither do they tend to report their testing assumptions or the quality of their measurement. There is room for much improvement in how we attend to the basics of quantitative research, and it does not appear that persuasion and professional communication are effective in changing practice.	2008	28	2	151-160
113	Vadillo, M. A., Konstantinidis, E. and Shanks, D. R.	Underpowered samples, false negatives, and unconscious learning	Psychonomic Bulletin & Review	The scientific community has witnessed growing concern about the high rate of false positives and unreliable results within the psychological literature, but the harmful impact of false negatives has been largely ignored. False negatives are particularly concerning in research areas where demonstrating the absence of an effect is crucial, such as studies of unconscious or implicit processing. Research on implicit processes seeks evidence of above-chance performance on some implicit behavioral measure at the same time as chance-level performance (that is, a null result) on an explicit measure of awareness. A systematic review of 73 studies of contextual cuing, a popular implicit learning paradigm, involving 181 statistical analyses of awareness tests, reveals how underpowered studies can lead to failure to reject a false null hypothesis. Among the studies that reported sufficient information, the meta-analytic effect size across awareness tests was d(z) = 0.31 (95 % CI 0.24-0.37), showing that participants' learning in these experiments was conscious. The unusually large number of positive results in this literature cannot be explained by selective publication. Instead, our analyses demonstrate that these tests are typically insensitive and underpowered to detect medium to small, but true, effects in awareness tests. These findings challenge a widespread and theoretically important claim about the extent of unconscious human cognition.	2016	23	1	87-102
