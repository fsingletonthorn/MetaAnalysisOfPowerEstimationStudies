,id,Author,Title,Jounral,Year,StudyName,exclude,SamplingStrategy,SampleSource,YearsStudied,TargetTest,SubfieldClassification,PowerEstimationTechnique,AmalgomationMethod,DistinguishedStatisticalTestsAppropriatly,NumberOfArticles,NumberOfTests,EffectSizeUsed,SmallEffectBenchmark,MediumEffectBenchmark,LargeEffectBenchmark,PowerAtSmallEffectMedian,FirstQuartilePowerAtSmall,ThirdQuartilePowerAtSmall,PowerAtMediumEffectMedian,FirstQuartilePowerAtMedium,ThirdQuartilePowerAtMedium,PowerAtLargeEffectMedian,FirstQuartilePowerAtLarge,ThirdQuartilePowerAtLarge,PowerAtSmallEffectMean,PowerAtMediumEffectMean,PowerAtLargeEffectMean,SDPowerAtSmall,SDPowerAtMedium,SDPowerAtLarge,SampleMedian,FirstQuartileSampleSize,ThirdQuartileSampleSize,SampleMean,SampleSizeSD,SDSmallAlgEstFromCDT,SDMedAlgEstFromCDT,SDLargeAlgEstFromCDT,Notes,Solutions,SampleMin,SampleMax,PowerSmallMin,PowerSmallMax,PowerMedMin,PowerMedMax,PowerLargeMin,PowerLargeMax
1,65,"Sindelar, Paul T., Allman, Carol, Monda, Lisa, Vail, Cynthia O., Wilson, Cynthia L. and Schloss, Patrick J.",The power of hypothesis testing in special education efficacy research,The Journal of Special Education,1988,Sindelar et al. (1988),NA,"""Carlberg and Kavale (1980) reviewed 48 studies of the efficacy of special class placement. An extensive library search yielded 44 of these 48 papers; each was examined to determine what statistical test had been used. . Those 35 studies in  which t test, analysis of variance, chi-square, or correlational analyses were used  were included in the present study. These studies are listed in Table 1, along with  the analysis, group size, and ESs for academic and social measures as reported by Carlberg and Kavale (1980).""","Applicable studies included in ""Carlberg and Kavale (1980)""",1932-1974,""" t test, analysis of variance, chi-square, or correlational analyses """,Education,"""The tables reported by Cohen (1969) were used to determine power for each measure in each study. In order to enter the tables, alpha was set at .05 and two tailed tests were assumed, regardless of the alpha level or directionality of the hypotheses used in the original study. Small, m e d i u m , and large ESs were set as recommended by Cohen at .20, .50, and .80 for t tests; .10, .25, and .40 for ANOVA; .05, .10, and .20 for chi-square; and .10, .30, and .50 for correlational analyses. With alpha and ESs set, three facts were required to enter the tables: the statistical test, the design of the study, and the group size or cell size in m o r e complex designs. For studies in which the groups had unequal A/s, an average group size was computed and used to enter the tables (Cohen, 1969).""","Power of the main test of either ""academic"" or ""social"" outcomes, if both were present, the mean power of both tests",yes,35,53,cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),0.12,0.08,0.13,0.46,0.295,0.585,0.85,0.635,0.945,0.119,0.468,0.786,0.432,0.194,0.197,60,37,97.5,69.686,35.09,NA,NA,NA,NA,NA,15,150,0.06,0.23,0.11,0.86,0.22,0.99
2,103,"Cohen, Jacob",The statistical power of abnormal-social psychological research: A review,The Journal of Abnormal and Social Psychology,1962,Cohen (1962),NA,"All applicable articles in the Journal of Abnormal and Social Psychology, 1960, 61",Journal of Abnormal and Social Psychology,1960,"""t (two means arc equal), Normal (two proportions), Normal, t two r's are equal,  t (r = 0), Sign test F (k means are equal), chi square (k proportions are equal, chi square (contingency test)""",General Psychology,"""Each article was read in turn, and the nature of each statistical test performed (or implied) in the article was noted. Generally, when sample sizes (and for F tests and x2, df) were added to the standard conditions, the power of the test for small, medium, and large effect size could be read directly from the appropriate prepared tables, or by interpolation between tabled values. The statistical tests given in Table 1 are not inclusive of all used in the volume, most noteworthily, nonparametric tests based on ranks could not be studied from the point of view of power due to unavailability of systematic studies of this issue in the literature. In the relatively few instances where such tests had been used, the power was determined for the analogous parametric test, e.g., the t test for means for the Mann-Whitney U test and for the Wilcoxon matched-pairs signed-ranks test, and the F test for the Kruskal-Wallis H test and for the Friedman test (Siegel, 19S6). Note that the effect of this substitution was to slightly overestimate the power of the tests on the usual assumption that the conditions required by the parametric tests obtained. Even if this assumption is questioned, it is quite unlikely that the substitution results in an underestimation of power. In general, in the few instances where statistical tests were so described as to leave a doubt about the exact details, the doubt was resolved in favor of higher power estimates. For example, if a group of n cases was divided into two subgroups for comparison, but the subgroup sizes were not given, it was assumed they were equal, which then leads to a maximum power estimate for that value of n""","Mean power of statistical tests in each article, parametric alternatives used for non-parametric tests",yes,70,2088,Cohen 1962 (d),0.25,0.5,1,NA,0.12,0.23,0.46,0.32,0.6,NA,0.73,0.94,NA,0.48,NA,0.08,0.2,0.16,NA,NA,NA,68,55,NA,NA,NA,NA,"""Since power is a direct monotonic function of sample size, it is recommended that investigators use larger sample sizes than they customarily do. It is further recommended that research plans be routinely subjected to power analysis, using as conventions the criteria of population effect size employed in this survey.""",NA,NA,NA,NA,NA,NA,NA,NA
3,118,"Levenson, J. R. L.",NA,NA,1980,Levenson (1980),NA,"Studies of attitudes in older people published in the Gerontologist ~ ""The study examined all issues of the Gerontologist (March, 1961 through December, 1977), and the Journal of Jounral of Gerontology (January, 1946 through November, 1977).2 Because the concept of attitude has been broadly defined in the psychological literature, all articles pertaining to values, beliefs, and ""attitudes"" were included in the sample""",Jounral of Gerontology,1946-1977,"t tests, Pearson correlations, differences between correlation coefficients, chisquare, and F tests",Clinical Psychology/Psychiatry,"""As in the manner of past investigations (Chase & Tucker, 1975; Chase & Chase, 1976), significance level was set at 0.05, and all tests were considered to be two-tailed (except F and chi-square tests). Formulae to compute the power of tests were adapted from Cohen (1977), and computations utilizing small, medium, and large effect sizes were performed""",Mean power of statistical tests in each article,yes,30,NA,Cohen 1977,Cohen 1977,Cohen 1977,Cohen 1977,0.29,NA,NA,0.99,NA,NA,0.99,NA,NA,0.37,0.91,0.98,0.25,0.12,0.02,NA,NA,NA,NA,NA,NA,NA,NA,NA,"perform a priori power analysis (gives a worked example, clearly implying but not directly stating this)",NA,NA,NA,NA,NA,NA,NA,NA
4,112,"Osborne, J. W.","Sweating the small stuff in educational psychology: how effect size and power reporting failed to change from 1969 to 1999, and what that means for the future of changing practices",Educational Psychology,2008,Osborne (2008),NA,"""we decided also to survey the volume three decades prior: 1969. This was seven years after the publication of Cohen’s seminal article on power, 30 years prior to the ‘current’ sample, and would allow a comparison of the long-term effects of this work (1998–1999) against the short-term effects (1969). As the Journal of Educational Psychology was the only one of the three journals that published during 1969, and at that time was the primary outlet for high-quality research in the field, it was the only journal surveyed for that time frame."" ""Articles containing no statistical analysis (e.g., qualitative, theory, or review articles) or containing statistical analyses for which there are no good methods or widely accepted methods of computing power or effect sizes (e.g., nonparametric tests, exploratory factor analysis) were excluded from this study (these were rare). All statistical tests within an article that related to central hypotheses being tested were recorded. Ancillary analyses, such as manipulation checks, and psychometric analyses of measures used, were not recorded""",Journal of Educational Psychology,1969,unclear,Education,"""Using methods described by Cohen (1988; see also Thompson, 2002b), all statistics were converted to effect sizes (d), and observed power was computed (as few authors report effect sizes and power, effect sizes were usually calculated from reported results). For tests not usually asso ciated with d for an effect size (e.g., correlation, multiple regression, chi-square), algebraic manipulations derived from Cohen (1988) were called upon to transform all other effect size indi ces to ds for comparability.4 For all tests, an a of .05 was assumed and, where appropriate, two tailed tests were assumed as well. The number of relevant effect sizes in individual articles ranged from 1 to 54. All effect sizes and power information were aggregated to the article level. Finally, to allow for comparison with other power surveys, power to detect small (d = .20), medium (d = 50), and large (d = .80) effects was calculated""","Mean power of statistical tests in each article, parametric alternatives used for non-parametric tests",unclear,55,NA,cohen 1988,cohen 1988,cohen 1988,cohen 1988,NA,NA,NA,NA,NA,NA,NA,NA,NA,0.27,0.71,0.89,0.17,0.27,0.19,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
5,118,"Levenson, J. R. L.","Statistical Power Analysis: Implications for Researchers, Planners, and Practitioners in Gerontology",The Gerontologist,1980,Levenson (1980) ,NA,"Studies of attitudes in older people published in the Gerontologist ~ ""The study examined all issues of the Gerontologist (March, 1961 through December, 1977), and the Journal of Gerontology (January, 1946 through November, 1977).2 Because the concept of attitude has been broadly defined in the psychological literature, all articles pertaining to values, beliefs, and ""attitudes"" were included in the sample""",Gerontologist,1961-1977,"t tests, Pearson correlations, differences between correlation coefficients, chisquare, and F tests",Clinical Psychology/Psychiatry,"""As in the manner of past investigations (Chase & Tucker, 1975; Chase & Chase, 1976), significance level was set at 0.05, and all tests were considered to be two-tailed (except F and chi-square tests). Formulae to compute the power of tests were adapted from Cohen (1977), and computations utilizing small, medium, and large effect sizes were performed""",Mean power of statistical tests in each article,yes,26,NA,Cohen 1977,Cohen 1977,Cohen 1977,Cohen 1977,0.34,NA,NA,0.99,NA,NA,0.99,NA,NA,0.36,0.85,0.94,0.29,0.24,0.1,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
6,62,"Haase, Richard F.",Power analysis of research in counselor education,Counselor Education and Supervision,1974,Haase (1974),NA,"""A survey of research articles published in Counselor Education and Supervision from 1968 to 1971 ·(Volumes 7, 8, 9, and 10) was conducted. Of the 234 articles, 60 contained a total of 206 statistical tests of hypotheses. (Multivariate studies, e.g., factor analysis, were not included in this survey.)""",Counselor Education and Supervision,1968-1971,"Chi square, t tests, f tests",Education,"""Each of the statistical tests was power analyzed using the method and tables provided by Cohen (1969). The power of each test was calculated assuming a ""small,"" ""medium,"" and ""large"" effect. Alpha levels of .05 were assumed for all tests. Although several statistical tests may have been performed in a single article, it was assumed that they were usually done on the same sample size. For each type of test employed (chi-square, t, F), the sample size was determined. In conjunction with alpha = .05, the power was read directly from Cohen's tabled values for the appropriate statistic. Power was read for large, medium, and small effect sizes. Results of the survey and descriptive statistics are presented in Table 1""",NA,yes,60,206,cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),0.095,NA,NA,0.365,NA,NA,0.742,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,0.2628393,0.3055613,0.2776435,NA,"""What can one do to improve the power of one's research and maximize one's chances of detecting a behavioral phenomenon, if indeed one does exist (small, medium, large)? Recalling the earlier discussion of the interrelationships between power, alpha levels, sample size, and effect size, an experimenter has two options for increasing power. It should be emphasized that the options described below should be exercised before the experiment is conducted""p. 130 ... ""The easiest and most obvious option is to increase the sample size employed, but not simply for the sake of large samples. (In this context, large is relative to the power desired in the experiment.) Once an acceptable power figure has been set, an alpha level chosen, and effect size estimated, the investigator can determine the sample size needed to achieve desired power at given levels of alpha and effect size. The beauty of this option is that it places still another unknown under the control of the investigator and makes the planning of experiments even more rational. As Cohen (1969) notes, this is the only rational basis for deciding on a sample size. Incidentally, the average sample size in the present survey was 28. Obtaining an additional 10 or 20 more subjects would prove highly rewarding for the additional time and effort invested. For example, the difference in power of the t-test with 28 subjects versus that with 50 subjects is a remarkable jump from 45 percent to 70 percent, assuming a medium effect size. A second option is to increase the alpha level. In light of our previous discussion of Type I errors and the basic conservatism of science, this is not a procedure to apply indiscriminantly, particularly when an increase in number will achieve the same result without running the additional risk of Type I errors. For enlightening discussions of this point, the reader is referred to Bakan (1967) and Winer (1962)""",NA,NA,NA,NA,NA,NA,NA,NA
7,122,"Penick, J. E. and Brewer, J. K.",The power of statistical tests in science teaching research,Journal of Research in Science Teaching,1974,Penick & Brewer (1974),NA,"""A survey of all the articles in volumes 6 and 7 (1969, 1970) of the JRST followed the decision to use Cohen’s tables and metric-free values for effect size. As each article was read, note was taken of the statistical test performed, the stated level of significance and the sample size.""",Journal of Research in Science Teaching,1969-1970,"t, F, chi square, r, other",Education,"""Cohen’s tables and metric-free values for effect size. As each article was read, note was taken of the statistical test performed, the stated level of significance and the sample size. When significance of a test was claimed but no level was indicated, it was assumed that the researchers would have rejected H, at the traditional level of 0.05. With this information, the power of the test with small, medium, and large effect sizes could be read directly from Cohen’s tables or determined by interpolation between tabled values. In those cases where detailsabout a test were lacking or unclear, the issue was resolved in favor of a higher power estimate. For instance, if a sample of size Nwas broken into two groups for comparison, it was assumed that the two subgroups were equal if no other data was provided. Such an assumption leads to a maximum power estimation for that N. To give additional benefit of doubt to the researchers in estimating the power of their tests, the final compilation involved only the 554 major statistical tests that were considered in the conclusions of the articles. The remaining 313 minor tests were typically exploratory or nonparametric, or tested the significance of correlations (see Table I). The mean power of the major tests in each article at each of the three levels of effect size was determined. This power calculation allowed each article to count equally in the summary of volumes 6 and 7""",Mean power of statistical tests in each article,yes,66,867,cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),NA,NA,NA,NA,NA,NA,NA,NA,NA,0.22,0.71,0.87,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"""1. Guarantee high power for the tests by considerablyincreasing the size of the samples. 2. Report in JRST the power of the test for several values of effect size. Then the readers would know whether to put any stock in the results, no matter how they came out. 3. Don’t be greedy when determining significance. If rejection at the .05 level is adequate protection, striving for significance at the .001 level does little more than reduce the power of the tests from 5080%. 4. If F tests are to be used, keep the number of comparison groups to a minimum since increasingthe treatment degrees of freedom reduces power substantially for fixed total sample size.""",NA,NA,NA,NA,NA,NA,NA,NA
8,113,"Brewer, James K",On the Power of Statistical Tests in the American Educational Research Journal,American Educational Research Journal,1972,Brewer (1972),NA,"""A survey of all articles in the AERJ from November 1969 to May 1971 was made. These eight issues contained eighty-five major articles (excluding reviews and comments) of which forty-seven contained at least one statistically significant test result for which power calculations could be made using Cohen's tables.""",American Educational Research Journal,1969-1971,"f tests, t tests, tests of correlations",Education,Unclear,statistical tests,unclear,85,373,cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),NA,NA,NA,NA,NA,NA,NA,NA,NA,0.14,0.58,0.78,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"""(1) Determine reasonably high (> .80) power a priori along with a and ES, then find the minimal sample size for the study (Cohen's tables are constructed to furnish these sizes for most tests). (2) If the sample size is fixed because of financial or other practical reasons, then fix a and calculate power for several levels of ES. If for all effect sizes that are of practical importance power is still below .80, then avoid conducting the statistical test at that a until the sample can be increased. (3) Avoid the practice of taking any sample, calculating p (the probability of the statistic under H0) and then equating this value to a. The sample necessary to conduct a test of H0 with&alpha;= .05 is altogether too small to conduct the test with&alpha;= .01 for power and ES fixed.""",NA,NA,NA,NA,NA,NA,NA,NA
9,115,"Jones, B. J. and Brewer, J. K.",An Analysis of the Power of Statistical Tests Reported in the Research Quarterly,Research Quarterly,1972,Jones & Brewer (1972),NA,"""survey of major articles dating from October 1969 to May 1971 was made in regard to statistical tests used, levels of significance, the sample size, and power. It was noted that statistical inferential testing was conducted in 136 of the 151 articles""",Research Quarterly,1969-1971,t and F tests,Sport and exercise psychology,"""As each article was read, note was taken of the statistical test performed, the stated level of significance, and the sample size. When significance of a test was claimed but no level was indicated, it was assumed that the researchers would have rejected H« at the traditional level of .05. With this information, the power of the test with small, medium, and large effect sizes could be read directly from Cohen's tables or determined by interpolation between tabled values. In those cases where details about a test were lacking or unclear, the issue was resolved in favor of a higher power estimate. For instance, if a sample size of n was broken into two groups for comparison, it was assumed that the two subgroups were equal if no other data were provided. Such an assumption leads to a maximum power estimation for that n""",Power of statistical tests,NA,106,263,cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),NA,NA,NA,NA,NA,NA,NA,NA,NA,0.1514,0.5587,0.8092,0.1015,0.295,0.2482,NA,NA,NA,NA,NA,NA,NA,NA,"Note, SDs had to be calculated manually from raw sds for the two types of tests (i.e., t tests and f tests)","""1. Guarantee higher power for the tests by increasing considerably the size of the samples for fixed a and d. 2. Report in the Research Quarterly the power of the test for several values of effect size. Then the readers would know whether to put any stock in the results - no matter how they came out. Support for this suggestion has been given by Tversky and Kahneman. An a priori calculation of power could result in the study not being conducted until large samples could be obtained. 3. Not be greedy when determining significance. If rejection at the .05 level is adequate protection, striving for significance at the .01 level does little more than reduce substantially the power of the test for fixed nand effect size. 4. If ANOVA tests are to be used, keep the number of comparison groups to a minimum, since increasing the treatment degrees of freedom reduces power considerably for fixed a, total n and effect size""",NA,NA,NA,NA,NA,NA,NA,NA
10,116,"Katzer, J. and Sodt, J.",An Analysis of the Use of Statistical Testing in Communication Research,Journal of Communication,1973,Katzer & Sodt (1973),NA,"""To answer the questions about OES and power, we analyzed all articles published in the 1971 and 1972 volumes of the Journal of Communication. Excluding book reviews, 54 articles were published in those years. Of these 31 (57%) used statistical testing.1° In each article WQ indentified the major statistical test.""",Jounral of Communication,1971-1972,t and F tests,Communication,Cohen's 1969 tables,Mean power of statistical tests in each article,yes,31,1671,cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),0.15,0.08,0.27,0.55,0.31,0.82,0.91,0.79,0.99,0.23,0.56,0.79,0.21,0.29,0.21,NA,NA,NA,NA,NA,NA,NA,NA,NA,"""we recommend that the power tables be used to determine sample size"" … ""A more useful rule-of-thumb would be that a sample size of 64 is needed in each group if the commonly used t test for independent means is to have a .80 a priori power.'"" Other statistical tests require other sample sizes, but all of them are probably higher than we were led to believe (or would like to believe)"" ... ""This information can be divided into two types, that which is decided before the data are collected, and that which depends upon the data. In the first category fall a, the power desired (or p ) , the minimal difference or effect size looked for, and the sample size needed (both total and by cell). The second category of information includes the sample size actually used (and the reason for any loss of subjects), the value of the obtained statistic (e.g. t, F, i'), the p value for that statistic, and the OES for all significant findings.""",NA,NA,NA,NA,NA,NA,NA,NA
11,98,"Rothpearl, A. B., Mohs, R. C. and Davis, K. L.",Statistical power in biological psychiatry,Psychiatry Research,1981,"Rothpearl, Mohs & Davis (1981)",NA,Unclear,Selected studies examining cerebro-spinal fluid neurotransmitter metabolites in people with depression v. control group,1966-1980,two tailed t test at alpha =.05,Clinical Psychology/Psychiatry,"""Power is for a two-tailed t test of the control group vs. the depressive group at alpha = 0.05.""",NA,NA,35,NA,d,0.25,0.5,NA,0.11,0.09,0.13,0.28,0.18,0.37,NA,NA,NA,0.11,0.29,NA,0.03,0.12,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
12,72,"Chase, Lawrence J. and Tucker, Raymond K.",A power-analytic examination of contemporary communication research,Speech Monographs,1975,"Chase, & Tucker (1975)",NA,"""In order to provide a contemporary assessment of the state of communication research, all studies in which significance tests were utilized in these journals [see ""sampled articles""] during 1973 were analyzed.""","""American Forensic Association Journal, Central States Speech Journal, Journal of Communication, The Quarterly Journal of Speech, Southern Speech Communication Journal, Speech Monographs, The Speech Teacher, Today's Speech, Western Speech""",1973,"t test, test of equal proportions, correlation, sign test, chi -square, F test",Clinical Psychology/Psychiatry,"""In order to provide a constant basis for comparison, both between disciplines and among the speech communication journals, certain criteria were standardized. The non-directional version of the null hypothesis was used uniformly in all power calculations. Thus, a two-sided test was used for normal, binominal, and t distributions, while the one-sided (high) value test for chi-square and F distributions was used. Although this procedure may provide slightly lower power figures in some cases, ""the more serious problem of inflated significance levels and the embarrassment of large effects in the nonpredicted direction"" was avoided.17 The five percent level of significance was also employed for each test, a practice which resulted in both underestimating and overestimating power in a number of cases. On three occasions, Scheffe's t was computed to effect post hoc comparisons. This is an appropriate test available to the experimenter interested in comparing all treatment and control conditions, regardless of whether the tests are planned or unplanned. The major difficulty with this procedure is its conservative nature with regard to Type I errors: the critical k is often too high to detect actual differences. Scheffe has suggested that the .05 alpha level be increased to .10 in such cases.18 Since the five percent level was used throughout this analysis, the cases involving the Scheffe test were attributed a slightly lower power figure. The underestimates were offset, however, bcause several investigators employed the one percent alpha level, resulting in higher power estimates than if the five percent level had been utilized""",Mean power of statistical tests in each article,yes,46,1298,cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),0.18,0.1,0.23,0.46,0.36,0.63,0.81,0.7,0.92,0.18,0.52,0.79,0.14,0.21,0.05,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,0.17,0.944,0.31,0.999
13,117,"Kroll, R. M. and Chase, L. J.",Communication disorders: A power analytic assessment of recent research,Journal of Communication Disorders,1975,Kroll & Chase (1975),NA,"""The present investigation assessed the statistical power of recently published research in the field of communicative disorders. The two journals selected for this analysis were the Journal of Communication Disorders and the Journal of Speech and Hearing Research. These journals provide a representative sample of contemporary research in speech pathology and audiology. Every 1973 issue of both journalsand those 1974 issues that had been published prior to the inauguration of this study, were included. All articles incorporating significance tests were examined. The average statistical power for the three effect sizes was computed for each study using the tables provided by Cohen ( 1969); appropriate measures of central tendency and dispersion were also calculated""",Journal of Communication Disorders and the Journal of Speech and Hearing Research,1973-1974,"t-tests, tests for normal proportions. tests for normal correlations, tests for significance of correlations, sign tests, chi-square tests, and F tests",Clinical Psychology/Psychiatry,"""In order to determine small, medium, and large effect size estimates, Cohen’s (1969) measures of experimental effect were utilized.""",Mean power of statistical tests in each article,yes,62,1037,cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),0.12,0.08,0.19,0.38,0.29,0.55,0.71,0.57,0.86,0.16,0.44,0.73,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"""Specifically, statistical power should be of major concern in both the planning and analysis phases of an empirical study. Increased awareness of the consequences of inadequate power will serve to improve the type and quality of experimental, clinical, and/or theoretical implications that may be derived from an empirical investigation. This, in turn, will aid in the establishment and maintenance of an effective, complementary relationship between the experimentalist and clinician.""",NA,NA,NA,NA,NA,NA,NA,NA
14,86,"Chase, Lawrence J. and Chase, Richard B.",A statistical power analysis of applied psychological research,Journal of Applied Psychology,1976,Chase & Chase (1976),NA,"""Empirical studies reported in the 1974 Journal of Applied Psychology (all six issues of Volume 59) were examined in this survey.2 The article served as the unit of analysis. The major significance tests were power-analyzed, while secondary tests such as manipulation checks and peripheral reliability estimates were omitted. This procedure""","""Journal of Applied Psychology (all six issues of Volume 59)""",1974,"""Of the 3,373 statistical tests examined in this sample, 1,736 were for correlations, 732 involved an F, 691 were simple t tests, and 218 used chisquare""",General Psychology,"""For each test of significance, three power determinations corresponding to the three levels of experimental effect (small, medium, and large) were made. When cell sizes were unequal, harmonic mean functions of the two «s were used (see Cohen, 1969). In order to remain consistent with previous power analyses, certain experimental criteria were standardized. The .05 level of significance was used uniformly as was the nondirectional version of the null hypothesis. Once these figures were obtained, the average statistical power for each article was calculated, resulting in mean estimates for small, medium, and large effect sizes.""",Mean power of statistical tests in each article,yes,121,3373,cohen 1988,cohen 1988,cohen 1988,cohen 1988,0.16,NA,NA,0.7,NA,NA,0.94,NA,NA,0.25,0.67,0.86,0.23,0.26,0.18,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
15,125,"Chase, Lawrence J and Baran, Stanley J",An Assessment of Quantitative Research in Mass Communication.,Journalism Quarterly,1976,Chase & Baran (1976),NA,"""In order to assess the level of statistical power in contemporary mass communication research, articles appearing in the 1974 volumes of JOURNALISM Q U A R T E R L Y and The Journal of Broadcasring were power-analytically examined. Overall power figures were obtained by I ) calculating the statistical power of the major significance tests within each article for each of the three postulated effect sizes (small, medium, and large); and 2) averagng the power figures according to level of experimental effect for the entire array of studies""","Journalism Quarterly, the Jounral of Broadcasting",1974,"t-test for means. F-test. significance of a .product-moment correlation coefficient, test for differences between correlations. test for differences between proportions, test that a proportion equals S O , the sign test and the chi-square test",Communication,"""The power determinations for each significance test were calculated using Cohen's handbook.""",Mean power of statistical tests in each article,yes,48,701,cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),0.225,NA,NA,0.88,NA,NA,0.99,NA,NA,0.34,0.76,0.91,0.25,0.23,0.16,NA,NA,NA,NA,NA,NA,NA,NA,NA,none,NA,NA,NA,NA,NA,NA,NA,NA
16,126,"Christensen, James E. and Christensen, c","Statistical Power Analysis of Health, Physical Education, and Recreation Research.","Research Quarterly. American Alliance for Health, Physical Education and Recreation",1977,Christensen & Christensen (1977),NA,"""Each study in the Research Quarterly, volume 46, 1975 that could be analysed""",Research Quarterly,1975,"unclear ~ context suggests t, F, Chi Square,r, other ""Cohen provided tables from which the power of most statistical test employed by social work researchers can be determined easily once the significance level, sample size, and ES have been specified.""",Management / IO Psychology,"""The power of each primary statistical test for small, medium, and large effect size was read directly from the appropriate table prepared by Cohen. The mean power of the major statistical tests was determined at the three levels of effect size for each research study.""",Mean power of statistical tests in each article,yes,43,NA,cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),cohen 1969 (equivilent to 1988),0.08333,0.06,0.12,0.31995,0.1733,0.5348,0.6933,0.3633,0.86737,0.17555,0.3877,0.62427,0.260384331,0.279821372,0.283019434,NA,NA,NA,NA,NA,NA,NA,NA,NA,"""More attention should focus on sample sizes and the concept of statistical power in research in the field of health, physical education, and recreation""",NA,NA,NA,NA,NA,NA,NA,NA
17,50,"Schoonen, Rob",The internal validity of efficacy studies: Design and statistical power in studies of language therapy for aphasics,Brain and Language,1991,Schoonen (1991),NA,"""we reviewed 35 efficacy studies."" examining ""the efficacy of language therapy""",Efficacy studes of language therapies - only articles using non-directional tests,1964-1989,"Only directional - f tests (analysis of variance and covariance), x2 tests, t tests for dependent and independent samples, Kruskall-Wallis tests, Mann-Whitney U tests, Wilcoxon tests, tests for Pearson’s correlation, sign tests, and Fisher’s exact tests.",Clinical Psychology/Psychiatry,"""Cohen’s tri-partition and estimated the statistical power of the tests for three hypothetical situations, assuming small treatment effects, medium effects, and large effects. Furthermore, we took the conventional 5% level as significance level. In Table 2, separate results are listed for directional tests, i.e., tests used for both one- and two-tailed testing (e.g., t test), and for nondirectional tests, i.e., tests generally not used for testing against a specified direction of the difference between H,, and H, (e.g., F test). For directional tests, we assumed one-tailed testing, which is more powerful than twotailed"" Cohen's 1977 power handbook was used",Mean power of statistical tests in each article,unclear,13,273,cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),0.15,0.12,0.186,0.52,0.39,0.55,0.796,0.69,0.913,0.182,0.502,0.76,0.109,0.196,0.184,NA,NA,NA,NA,NA,NA,NA,NA,NA,"""We are well aware that these research conditions are difficult to establish, but they are a prerequisite for unambiguous research. If one cannot establish these conditions, one should seriously consider refraining from further efficacy studies and investing energy and money in more theoretically oriented, small-scale research in which experimental effects are supposedly much larger than the treatment effects in efficacy studies,""",NA,NA,0.93,0.198,0.365,0.459,0.76,0.95
18,50,"Schoonen, Rob",NA,NA,1991,Schoonen (1991) ,NA,"""we reviewed 35 efficacy studies."" examining ""the efficacy of language therapy""",Efficacy studes of language therapies - only articles using directional tests,1964-1989,"only non-directional - F tests (analysis of variance and covariance), x2 tests, t tests for dependent and independent samples, Kruskall-Wallis tests, Mann-Whitney U tests, Wilcoxon tests, tests for Pearson’s correlation, sign tests, and Fisher’s exact tests.",Clinical Psychology/Psychiatry,"""Cohen’s tri-partition and estimated the statistical power of the tests for three hypothetical situations, assuming small treatment effects, medium effects, and large effects. Furthermore, we took the conventional 5% level as significance level. In Table 2, separate results are listed for directional tests, i.e., tests used for both one- and two-tailed testing (e.g., t test), and for nondirectional tests, i.e., tests generally not used for testing against a specified direction of the difference between H,, and H, (e.g., F test). For directional tests, we assumed one-tailed testing, which is more powerful than twotailed"" Cohen's 1977 power handbook was used",Mean power of statistical tests in each article,unclear,9,132,cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),0.0951,0.08175,0.17325,0.389,0.28075,0.646,0.718,0.634,0.8875,0.138,0.481,0.713,0.0936,0.298,0.223,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,0.06,0.15,0.32,0.369,0.99,0.99
19,131,"Woolley, Thomas W. and Dawson, George O.",A Follow-up Power Analysis of the Statistical Tests Used in the Journal of Research in Science Teaching.,Journal of Research in Science Teaching,1983,Woolley & Dawson (1983),NA,"""A total of 265 articlesin Volumes 14-17 (1977-1980) of the Journal of Research in Science Teaching (JRST)were reviewed for this survey. Seventy-three of these studies were omitted from consideration due to the fact that there were no statistical tests of significance included (or those tests reported were not applicable in this power survey), or the studies were uninterpretable, i.e., necessary information was missing. All statistical tests of significance,with the exception of secondary tests such as reliability estimates, etc., were power-analyzed""",Journal of Research in Science Teaching,1977-1980,"t, F, chi square, r",Education,"""1) only the most common statistical tests (r, F,x ,r) were power-analyzed; (2) alpha was held at a uniform value of 0.05 and a nondirectional alternative was assumed for all studies; (3) Cohen’s (1977) definitions of small, medium, and large effect sizes were adopted. Thus, three power determinations were made for each test of significance, and an average power for detecting small, medium, and large ESs was calculated for each study. Note that when unequal cell sizes were in evidence, the harmonic mean functions of the cell sizes were used (see Cohen, 1977).""",Mean power of statistical tests in each article,yes,73,3556,cohen 1977,cohen 1977,cohen 1977,cohen 1977,NA,NA,NA,NA,NA,NA,NA,NA,NA,0.29,0.63,0.85,NA,NA,NA,NA,NA,NA,NA,NA,0.2129196,0.2736723,0.1919208,NA,"""(1) increase sample sizes, (2) report the power of a test for several values of ES, (3) set a realistic alpha, and (4) keep the number of cells in analysis of variance designs to a minimum."" ""supply at least the minimal amoufit of information demanded for clear and independent evaluation. This would include not only information needed (and used?) for planning the data collection (a,desired power, hypothesized effect size, and necessary n), but also an adequate accounting of at least the following: (1) actual n used (total, per cell, per factor); (3) value of the calculated test statistic and its associated value; (4)all cell means and standard deviations; and (5) a complete description of experimental design(s) incorporated in the study"" ""Finally, a bold, yet meritorious, move should be made to amend the JRST editorial policies to require all such information relating to a priori design considerations and post hoc interpretation to be incorporated as a standard component of any research report submitted for publication. It is the sincere belief of the authors that such standards would contribute toward a substantial upgrading of the interpretability of research in science education.""",NA,NA,NA,NA,NA,NA,NA,NA
20,89,"Sawyer, Alan G. and Ball, A.",Statistical power and effect size in marketing research,Journal of Marketing Research,1981,Sawyer (1981),NA,"""To achieve the former goal, we examined with respect to power all empirical articles in the four 1979 issues of the Journal of Marketing Researc""",Journal of Marketing Research,1979,"""major tests""",Management / IO Psychology,"""is. As in past surveys of power in other disciplines including social psychology (Cohen 1962), education (Brewer 1972), communication (Chase and Tucker 1975), applied psychology (Chase and Chase 1976), and various other fields (Chase and Baran 1976; Katzer and Sodt 1973; Kroll and Chase 1975), a Type I error rate of 5% was uniformly employed and, unless the article specified a one-tailed test, all calculations were based on twotailed tests.""",Mean power of statistical tests in each article,yes,23,475,cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),NA,NA,NA,NA,NA,NA,NA,NA,NA,0.41,0.89,0.98,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
21,127,"Daly, John A. and Hexamer, Anne H",Statistical Power in Research in English Education.,Research in the Teaching of English,1983,Daly & Hexamer (1983),NA,"""Fifty-seven articles published between 1978 and 1980 in Research in the Teaching of English were examined in this inve""",NA,1978-1980,"""502 involved an F test, 334 used chi-square tests, 200 were t-tests, 108 were correlations, 79 tested the difference between proportions, six were regressions, and four assessed the difference between correlation coeffi""",Education,"""Cohen (1977)... tables""",Mean power of statistical tests in each article,yes,57,1233,cohen 1977,cohen 1977,cohen 1977,cohen 1977,0.146,NA,NA,0.616,NA,NA,0.933,NA,NA,0.218,0.632,0.864,0.187,0.244,0.165,NA,NA,NA,NA,NA,NA,NA,NA,NA,"""In short, we urge investigators to include i accurate and complete descriptions of procedur magnitude estimates. Not only will such reports meta-analyses but they will also allow readers t rectly interp""",NA,NA,NA,NA,NA,NA,NA,NA
22,97,"Orme, John G. and Combs-Orme, Terri D.",Statistical power and Type II errors in social work research,Social Work Research & Abstracts,1986,Orme & Combs-Orme (1986),NA,"""Articles from Social Work Research and Abstracts were selected because the journal publishes a broad array of articles on social work research. The article served as the primary unit of analysis, as in previous surveys of statistical power,"" With the exception of book reviews, essay reviews, Research Briefs, and Opinions from the Field, which were excluded because they do not present data analyses of findings from primary research, all articles in the 1977 volume (when the journalfirst began publishing empirical articles) through the 1984 volume were surveyed, including Research Notes. A total of 164 such articles were found, of which 143 (87 percent) were full-length articles and 21 (13 percent) were Research Notes. Of the 164 articles, 98 (60 percent) utilized statistical tests of hypotheses. However, of the 98 articles, 9 (9 percent) were excluded because they reported insufficient information to compute power, and 10(10 percent) were excluded because, to the authors' knowledge, procedures for the computation of power for the statistical tests used are not available. Therefore, power was computed for a total of 79 articles, 75 (95 percent) of which were full-length articles and 4 (5 percent) were Research Notes""",Social work research and abstracts,1977-1984,"t, F tests (regression, anova, ancova), chi square tests, correlations",Clinical Psychology/Psychiatry,"""The power of most statistical tests utilized by social work researchers can be determined easily (from the tables in Cohen's text) once the significance level, size of the sample, and ES have been specified."" The authors used Cohen's tables to determine the a priori power of the statistical tests examined.""",Mean power of statistical tests in each article,yes,79,3114,Cohen 1977,Cohen 1977,Cohen 1977,Cohen 1977,0.2,NA,NA,0.86,NA,NA,0.99,NA,NA,0.31,0.76,0.92,0.28,0.25,0.13,NA,NA,NA,NA,NA,NA,NA,NA,NA,"""Social work researchers should not passively accept low statistical power but, rather, should plan for adequate statistical power before conducting an investigation. Perhaps, what is most important is that the size of the sample that is necessary to address research questions with adequate statistical power should be carefully considered in any preinvestigation analysis. Researchers should plan samples that are large enough to test hypothesized interactions because statistical tests of interactions are even more likely to have inadequate power than are tests of main effects and the incorrect detection of interactions may result in the inappropriate interpretation of main effects."" p7, following all on P8 ""Increasing alpha"" ""p8 ""ather than merely increasing the size of a sample in an attempt to obtain adequate power to detectsmalland, in someinstances, inconsequential ESs, one should try, in many cases, to increase the ES as well."" ""editors and reviewers to insist that investigators who use tests of statistical significance report the explicit preinvestigation rationale for the size of the sample, the hypothesized ES,the degree of desired power, and perhaps the alpha value they selected."" p8",NA,NA,NA,NA,NA,NA,NA,NA
23,129,"Orme, John G. and Tolman, Richard M.",The Statistical Power of a Decade of Social Work Education Research.,Social Service Review,1986,Orme & Tolman (1986),NA,"""Articles from the Journal of Social Work Education were selected becau this is the leading social work education journal. The article serve the primary unit of analysis as in previous surveys of statistical power. Articles were selected from the 1976-spring 1985 volumes""",Social Work Education,1976-1985,"unclear ~ context suggests t, F, Chi Square,r, other ""Cohen provided tables from which the power of most statistical test employed by social work researchers can be determined easily once the significance level, sample size, and ES have been specified.""",Education,"""Cohen provided tables from which the power of most statistical test employed by social work researchers can be determined easily once the significance level, sample size, and ES have been specified.21 These tables were employed in the present study to determine the power of the statistical tests examined. For each article, the tests that directly addressed the major hypotheses or questions were identified. Secondary tests such as those concerned with manipulation checks, peripheral correlation coefficients, r estimates, or tests of statistical assumptions or clearly exploratory were omitted from the analyses.""",Mean power of statistical tests in each article,yes,64,NA,cohen 1977,cohen 1977,cohen 1977,cohen 1977,0.18,NA,NA,0.78,NA,NA,0.99,NA,NA,0.2,0.68,0.88,0.14,0.26,0.19,NA,NA,NA,NA,NA,NA,NA,NA,NA,"""A publication practice that might improve the quality of social work education research would be the routine inclusion by authors of their explicit preinvestigation rationale for the selection of the sample size employed, the hypothesized ES, the degree of desired power, and perhaps the alpha value selected.48 This practice might encourage careful consideration of these parameters, enhance the utility of statistical tests of hypotheses, and increase the validity of conclusions drawn from social work education research using such""",NA,NA,NA,NA,NA,NA,NA,NA
24,42,"Woolley, Thomas W.",A comprehensive power-analytic investigation of research in medical education,Journal of Medical Education,1983,Woolley (1983),NA,"""Major articles in volumes 55 through 57 (1980 - 1982) of the Journal of Medical Education were reviewed for this survey. Of these studies, 130 were omitted from consideration because there were no statsitical tests of significance included, tests reported were not appliacable in in this power survey, or the studies were uninterpretable, that is, necessesary information was missing""",Journal of Medical Education,1980-1982,"t, F, chi square and correlation coefficients",Education,"""Cohen's tables""",Mean power of statistical tests in each article,yes,100,2220,cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),cohen 1977 (equivilent to 1988),0.14,NA,NA,0.71,NA,NA,0.99,NA,NA,0.23,0.69,0.9,NA,NA,NA,112.5,NA,NA,199.7,NA,0.1958955,0.2584452,0.1611334,NA,"""consider statistical power … during the research design phase"" report all values necessary to evaluate power post hoc (n, alpha, test statistic values, all cell means and SDs, a complete description of the experimental designs)",NA,NA,NA,NA,NA,NA,NA,NA
25,106,"Rossi, Joseph S.",Statistical power of psychological research: What have we gained in 20 years?,Journal of Consulting and Clinical Psychology,1990,Rossi (1990),NA,"""All of the articles published in the Journal of Abnormal Psychology, 1982, Volume 91; the Journal of Consulting and Clinical Psychology, 1982, Volume 50; and the Journal of Personality and Social Psychology, 1982, Volume 42, were examined, and articles not reporting any statistical tests were eliminated from the study. In addition, some articles were excluded because they contained statistical methods for which power could not be determined""... ""As in previoussurveys, a distinction was made between major and peripheral statistical tests. Major tests were those that bore directly on the research hypotheses of the study, whereas peripheral tests did not. Peripheral tests, which were excluded from the survey, included all of the correlation coefficients of a factor analysis, unhypothesized higher order analysis of variance interactions, manipulation checks,interrater reliability coefficients, reliabilities of psychometric tests (internal consistency, test-retest), post hoc analysis of variance procedures (means comparisons tests, simple effects), and tests of statistical assumptions. For major statistical tests, power was determined for the following: / test, Pearson r, z test for the difference between two independent correlation coefficients, sign test, z test for the difference between two independent proportions, chi-square test, F test in the analysis of variance and covariance, and multiple regression F test.""","Journal of Abnormal psychology, Journal of Consulting and Clinical Psychology, and Journal of Personality and Social Psychology",1982,"F test (ANOVA and ANCOVA), pearson r, t test, chi square, F test, z test, sign test",General Psychology,"""Cohen's (1977) tables wereused to determine power for sign tests and chi-square tests. Tables and formulas in Rossi (1985b) were used to determine power for z tests for the difference between independent correlation coefficients and for z tests for the difference between independent proportions. The tabled valuesin thesesources are accurate to about one digit in the second decimal place whencompared with exact values. Computer programs were written by the author to expedite the determination of power for the remaining tests (t, r, and F). This was done because of the frequency of occurrence of these statistics and to avoid the interpolation errors inevitably encountered with the useof tables or charts. All programs were written in double precision IBM BASIC (version 3.10) to run on an IBM PC/AT microcomputer. The computer program to determine the power of the t test was based on the normal approximation to the noncentral I distribution given by Cohen (1977). Because this formula assumes that sample sizes are equal, it was modified slightly to permit unequal n power calculations. The determination of power for the Pearson correlation coefficient was based on the normal score approximation for r provided by the hyperbolic arctangent transformation, plus a correction factor for small sample sizes (Cohen, 1977). The cube root normal approximation of the noncentral F distribution was used for the analysis of variance power program (Laubscher. 1960). The accuracy of this formula has been found to be quite good, with errors appearing only in the 3rd or 4th decimal places for a = .05 (Cohen & Nee, 1987). Although more accurate approximations exist, the small gain in precision did not justify the additional computational complexity. For all computer programs, normal score approximations were converted to probability (power) values using formula 26.2.19 in Abramowitzand Stegun (1965, p. 932), who give the accuracy of the algorithm as±1.5 X 10~7. The resulting power values agreed to two decimal places with those obtained from Cohen's (1977) power tables and recently available computer program (Borenstein& Cohen, 1988).""",Mean power of statistical tests in each article,yes,221,6155,cohen 1988,cohen 1988,cohen 1988,cohen 1988,0.12,0.09,0.18,0.53,0.36,0.77,0.89,0.71,0.98,0.17,0.57,0.83,0.14,0.25,0.18,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
26,102,"Acklin, Marvin W. and McDowell, Claude J., II",Statistical power in Rorschach research,"Exner, John E Jr [Ed] (1995) Issues and methods in Rorschach research (pp 181-193) xiii, 324 pp Hillsdale, NJ, US: Lawrence Erlbaum Associates, Inc; US",1995,Acklin & McDowell (1995),NA,"""All of the articles published in the referenced journals between 1975 and 1991 were examined (N = 293), and articles not reporting statistics were eliminated from the study.""… ""a distinctionwas made between major and peripheral statistical tests. Major tests dealt directly with Rorschach variables, whereas peripheral tests did not. Peripheral tests that were excluded from the survey included, for example, all of the correlation coefficients of a factor analysis, cluster analysis, and multidimensional scaling; interrater, internal consistency, and temporal consistency reliability coefficients; tests of statistical assumptions; and statistical tests not bearing directing on Rorschach variables""","""Journal of Personality Assessment, Journal of Consulting and Clinical Psychology, Journal of Abnormal Psychology, Journal of Clinical Psychology, Journal of Personality, Psychological Bulletin, American Journal of Psychiatry, and Journal of Personality and Social Psychology""",1975-1991,"""t tests, Pearson's r, partial correlarions, chi square tests, and F test in the analysis of variance (ANOVA) and covariance(ANCOVA).Power coefficientswere computed for main effects only in factorial ANOVAs. In contrast to other power surveys, but following Cohen (1962), power analyses were calculated for nonparametric techniques, because these are overrepresented in the Rorschach research literature. In these cases, power was determined for the analogous parametric test, for example, the t test for means was substituted for the Mann-Whitney U test and for the Wilcoxon matched-pairs signed-rankstest, F test for the Kruskal-Wallis H test and for the Friedman test, and Pearson's r for Spearman's rho. """,Clinical Psychology/Psychiatry,"""Power coefficients were calculated using a computer program (Borenstein& Cohen, 1988).When the computer program was not helpful, we referred to power tables from Cohen (1988) and Lipsey (1990)""",Mean power of statistical tests in each article,yes,158,2300,cohen 1988,cohen 1988,cohen 1988,cohen 1988,0.105,0.08,0.15,0.57,0.348,0.745,0.945,0.738,0.99,0.131,0.561,0.85,0.104,0.238,0.167,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
27,119,"Mazen, A. M. M., Hemmasi, M. and Lewis, M. F",Assessment of statistical power in contemporary strategy research,Strategic Management Journal,1987,"Mazen, Hemmasi & Lewis (1987)",NA,"""For each test of significance, Cohen’s three conventional levels of effect size (small, medium, and large) were adopted. When cell sizes were unequal, harmonized mean functions were used. Also, when factorial and complex designs of analysis of variance were employed, or when interaction was considered, n for the respectiveResearch Notes and Communications 405 factor or interaction was determined according to Cohen's (1977) formulae. An a = 0.05 and the nondirectional version of the null hypothesis were used uniformly. Once power estimates for all tests in a study were computed, the average statistical power for each article was calculated for small, medium, and large effect size. By this conventional procedure, no matter how many tests were involved in a particular study, all articles counted equally in the description of the volumes examined. The mean power value of the studies at each of the three effect size levels were then distributed and their central tendency measures determined. The procedure described above is consistent with previous surveys of statistical power analysis""","Strategic Management Journal, Acadamy of Management Journal",1982-1984,"Regression (R^2 and Beta coefficients) [i.e., F and t tests], F tests, t tests, correlations, chi square tests, 211 tests of proportions",Management / IO Psychology,"""For each test of significance, Cohen’s three conventional levels of effect size (small, medium, and large) were adopted. When cell sizes were unequal, harmonized mean functions were used. Also, when factorial and complex designs of analysis of variance were employed, or when interaction was considered, n for the respectiveResearch Notes and Communications 405 factor or interaction was determined according to Cohen's (1977) formulae. An a = 0.05 and the nondirectional version of the null hypothesis were used uniformly. Once power estimates for all tests in a study were computed, the average statistical power for each article was calculated for small, medium, and large effect size. By this conventional procedure, no matter how many tests were involved in a particular study, all articles counted equally in the description of the volumes examined. The mean power value of the studies at each of the three effect size levels were then distributed and their central tendency measures determined. The procedure described above is consistent with previous surveys of statistical power analysis""",Mean power of statistical tests in each article,yes,44,3665,Cohen 1977,Cohen 1977,Cohen 1977,Cohen 1977,0.13,NA,NA,0.53,NA,NA,0.88,NA,NA,0.23,0.59,0.83,NA,NA,NA,NA,NA,NA,NA,NA,0.2328015,0.2717008,0.1996149,NA,"use ""repeated measures"", perform ""power planning"",",NA,NA,NA,NA,NA,NA,NA,NA
28,99,"Mazen, Abdelmagid M., Graf, Lee A., Kellogg, Calvin E. and Hemmasi, Masoud",Statistical power in contemporary management research,Academy of Management Journal,1987,Mazen et al. (1987),NA,"""This study includ empirical research in the 1984 issues of the Academy of Management Jour nal and the Journal of Management and in the 1984 Proceedings of the Midwest Division of the Academy of Management; we chose the last to represent published research on a regional level. In keeping with previous practice, studies served as the units of analysis, and we examined only major significance tests for which power tables are available (R2""","Academy of Management Journal, Journal of Management, and Proceedings of the Midwest Division of the Academy of Management",1984,"Multiple regression, regression coefficients, F tests, t tests, correlation coefficients, cchi square and sign tests",Management / IO Psychology,"""For each test of significance, we adopted Cohen's three conventional levels of effect size-small, medium, and large. When cell sizes were unequal, we used harmonized mean functions. Also, when factorial and complex designs of analysis of variance were employed or when interaction was considered, we determined the n for the factors or interactions """,Mean power of statistical tests in each article,yes,84,7215,cohen 1977,cohen 1977,cohen 1977,cohen 1977,0.25,NA,NA,0.89,NA,NA,0.99,NA,NA,0.31,0.77,0.91,NA,NA,NA,NA,NA,NA,NA,NA,0.2328015,0.2717008,0.1996149,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
29,44,"Sedlmeier, P. and Gigerenzer, G.",Do studies of statistical power have an effect on the power of studies?,Psychological Bulletin,1989,Sedlmeier & Gigerenzer (1989),NA,"""We decided to analyze all studies published in the 1984 volume of the Journal ofAbnormal Psychology""",Journal of Abnormal Psychology,1984,All included tests,Clinical Psychology/Psychiatry,"""J. Cohen (1962) calculated the power using articles as units, but 24 years later, articles often contain more than one experiment. We have treated different experiments within the same article as separate units ifthey used differentsamples ofsubjects.Powercalculationsweremade using two different units: articles, as in J. Cohen (1962), and experiments. For each unit, all tests were classifiedinto tests either of major or of minor hypotheses,followingJ. Cohen (1962). Then the powerof each test was calculated for small, medium, and large effect sizes; for most tests, J. Cohen's (1977) tables could be used. For all calculations, alpha was assumed to be .05 (two-tailedfor the unadjusted procedures; for the multiple comparison procedures the corresponding error rates [see, e.g., O'Neill & Wetherill, 1971] w e r e used). Nonparametric tests that occasionallyoccurred were treated like their corresponding parametric tests (e.g., Mann-Whitney U tests, Kruskal-Wallis tests, and Spearman correlations were treated like t tests, F tests, and Pearson correlations, respectively);this usuallyresults in a slightoverestimation of power(J. Cohen, 1965)""","Looked at power of major hypothesis test for each study (i.e., separate samples within an article), non-parametric tests treated as parametric alternatives",yes,54,64,cohen 1962,cohen 1962,cohen 1962,cohen 1962,0.14,0.1,0.22,0.44,0.28,0.7,0.9,0.76,0.99,0.21,0.5,0.84,0.19,0.27,0.18,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
30,60,"Dilullo, Linda Kay",A post hoc power analysis of inferential research examining the relationship between mathematics anxiety and mathematics performance,Dissertation Abstracts International Section A: Humanities and Social Sciences,1998,Dilullo (1998),NA,Articles listed in the July issues of Journal for Research in Mathematics Education 1976-1995,Journal for Research in Mathematics Education,1975-1994,"T tests for two means, preason product moment r, sign test, proportion is .5, differences between proportions, chi square, ANOVA, ANCOVA, multiple refression, MANOVA, MANCOVA",Education,Power tables from Cohen's (1988) Statistical Power Analysis for the Behavioural Sciences,Mean power of statistical tests in each article,yes,41,76,Cohen (nd),Cohen (nd),Cohen (nd),Cohen (nd),0.19,0.12,0.4,0.9,0.65,1,NA,NA,NA,0.3,0.81,NA,0.24,0.211,NA,111,60,283,355,116.84,NA,NA,NA,"Article found, manual calculation with their data was performed in order to obtain quantiles, sd, medians",NA,NA,NA,NA,NA,NA,NA,NA,NA
31,75,"Whittington, C. J., Podd, J. and Kan, M. M.",Recognition memory impairment in Parkinson's disease: Power and meta-analyses,Neuropsychology,2000,"Whittington, Podd, & Kan (2000)",NA,"""All studies selected for inclusion had to be published in a peer-reviewed, English-language journal with a publication date between 1978 and 1997. To be included in the power analysis, a study had to compare the performance of people with PD to healthy control participants on a task explicitly described as measuring memory.""",articles examining recognition memory performance in those with parkinson's disease,1978-1997,t and f tests,Clinical Psychology/Psychiatry,"""Statistical power was calculated using the GPOWER computer program developed by Erdfelder, Faul, and Buchner (1996). Power can be calculated for a range of univariate test statistics, including t and F. The program calculates power using Cohen's (1988) small, medium, and large ESs as the default. However, any ES value can be entered. For each power calculation, the appropriate values for alpha, ES, and N were entered and power was then calculated by the program for 1,360 test statistics. Where a study used a particular statistic more than once (almost always the case), the mean power level was calculated separately for small, medium, and large ESs for that study.""",Mean power of statistical tests in each article,yes,46,1360,cohen's d (1988),0.2,0.5,0.8,0.16,NA,NA,0.67,NA,NA,0.94,NA,NA,0.2,0.63,0.85,0.12,0.24,0.19,NA,NA,NA,NA,NA,NA,NA,NA,"minimum and maximum levels of power for small medium and large, [.06,55] [.15,1] [.32,1]",NA,NA,NA,NA,NA,NA,NA,NA,NA
32,45,"Aguinis, H., Beaty, J. C., Boik, R. J. and Pierce, C. A.",Effect size and power in assessing moderating effects of categorical variables using multiple regression: A 30-year review,Journal of Applied Psychology,2005,Aguinis et al. (2005),NA,"""The criteria for including studies in the review were as follows: (a) at least one MMR analysis was included as part of the study, (b) the MMR analysisincluded a continuous criterion, (c) the MMR analysis included a continuous predictor, and (d) the MMR analysis included a categorical moderator""","Journal of Applied Psychology, Personnel Psychology, and Academy of Management Journal",1977-1998,"""moderating effects of categorical variables as assessed using multiple regression""",Management / IO Psychology,"""Therefore, in the present study we used the Aguinis et al. (2001) theorem to compute power corresponding to a range of f 2 values.""",NA,yes,106,261,cohen 1988 f^2,0.02,0.15,0.35,0.84,NA,NA,0.98,NA,NA,1,NA,NA,0.84,0.98,0.99,0.21,0.1,0.02,158,NA,NA,378,548,NA,NA,NA,"Standardised effect size benchmarks are likely to be a vast overestimate of expectable effect sizes for moderation, from the article ""Our results show that the median effect size is .002. And, the effect size “bandwidth” is uniformly narrow and around .002 for the areas of personnel selection and work attitudes and for tests including the moderating effect of gender and ethnicity. Also, computations of the moderating effect size based on error-free measures increased the size of the median moderating effect by only .001.""",NA,NA,NA,NA,NA,NA,NA,NA,NA
33,71,"Kazantzis, N.",Power to detect homework effects in psychotherapy outcome research,Journal of Consulting and Clinical Psychology,2000,Kazantzis (2000),NA,"""Published articles that reported analysis of homework effects in psycho- therapy were located (a) by means of searches in the PsycLIT and PsyclNFO databases from 1980 though 1998 as well as (b) from the reference sections of relevantjournal articles and books. The terms behav- ioral practice, extratherapy, extratreatment, home practice, homework, and self-help assignments were used as text words in the searches of electronic databases. The rationale for not including studies published before 1980 was that they did not benefit from the work by Beck et al. (1979) on cognitive therapy of depression. The majority of outpatient treatment studies before 1980 omitted the assessment of client adherence with homework assignments (Kazantzis & Deane, 1998), a key emphasis of Beck et al.'s cognitive therapy.
As a result of the two searches described, a total of 719 homework- related studies were identified. The principal criterion for inclusion was whether the study had quantitatively assessed the relationship between homework and treatment outcome assessed at termination. Using this criterion, I excluded the majority of the selected studies, as they had only incorporated homework as a component of the treatment protocol but did not involve an examination of homework-related effects on outcome. Consequently, only 31 studies of the 719 studies identified in the searches were initially selected and considered for inclusion in the power analysis.""",studies examining the relationship between homework effects and clinical depression. Diverse Journals,1980-1998,t tests and ANOVAs,Clinical Psychology/Psychiatry,"""Statistical power was calculated based on samples sizes and Cohen's (1988) power tables and ES conventions, together with a computer pro- gram designed to calculate statistical power (Erdfelder, Faul, & Buchner, 1996). Statistical power was calculated separately for small, medium, and large ESs (Gillett, 1994). Alpha was set at .05 for all the power estimations. Power was not computed for those studies in which sufficient statistical information could not be determined (Kornblith et al., 1983; Michelson, Mavissakalian, Marchione, Dancu, & Greenwald, 1986) or in which non- parametric techniques (Fennell & Teasdale, 1987) or Tobit analyse~ (Per- sons et al., 1988) were used to examine homework effects. Thus, power was calculated for the remaining 27 articles examining the effects of homework assignments in psychotherapy.""",Mean power of statistical tests in each article,yes,27,32,cohen 1988,cohen 1988,cohen 1988,cohen 1988,0.09,0.075,0.13,0.37,0.225,0.61,0.76,0.515,0.945,0.11,0.44,0.71,0.07,0.28,0.27,34,21.5,55.5,48.56,44.45,NA,NA,NA,"explicitly performed to assess an underpowered area of research, Raw data extracted to caculate quantiles for sample size and effect sizes",NA,NA,NA,NA,NA,NA,NA,NA,NA
34,67,"Brown, J. and Hale, M. S.",The power of statistical studies in consultation-liaison psychiatry,Psychosomatics,1992,Brown & Hale (1992),NA,"""An examination of the empirical research published in Psychosomatics during 1989 was conducted to determine the mean statistical power of each investigation""","Psychonomics, 1989",1989,"""Statistical tests reported in these studies were classified as being either central to hypotheses being tested by the research or peripheral to these hypotheses. Since much of the research was exploratory in nature. the vast majority of statistical tests were classified as being central to the articles. The power of all reported analyses, excluding approximately 130, was calculated based upon the number of subjects in each comparison and assuming an alpha level of 0.05 and universal use of nondirectional tests.""",Clinical Psychology/Psychiatry,""" Finally, the mean power of all tests performed in each study was calculated for each of the three possible effect sizes.""",Mean power of statistical tests in each article,unclear,24,962,cohen 1977,cohen 1977,cohen 1977,cohen 1977,0.115,0.8,0.25,0.51,0.32,0.832,0.908,0.199,0.729,0.195,0.6,0.839,0.196,0.286,0.199,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
35,68,"Tomcho, Thomas J. and Foels, Rob",The power of teaching activities: Statistical and methodological recommendations,Teaching of Psychology,2009,"Tomcho, Thomas & Foels (2009)",NA,"""We identified ToP teaching activity and methods studies from 1974 through 2006 in which researchers (a) reported statistical significance test results, (b) reported sample size data, and (c) employed a commonly used significance test from which we could calculate power (see Cohen, 1988).""",Teaching of Psychology,1974-2006,"Chi square, t tests (independent and dependent samples), correlations",Education,"""We used Borenstein et al.’s (2001) Power and Precision software to calculate power estimates. We assumed a two tailed hypothesis test for all studies.""",Mean power of statistical tests in each article,yes,193,497,cohen 1988,cohen 1988,cohen 1988,cohen 1988,0.1,0.06,0.17,0.52,0.31,0.77,0.92,0.72,0.99,0.14,0.53,0.81,0.13,0.28,0.23,63,34,111,79.1,75.2,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
36,88,"Kosciulek, John F. and Szymanski, Edna M.",Statistical power analysis of rehabilitation counseling research,Rehabilitation Counseling Bulletin,1993,Kosciulek & Szymanski (1993),NA,"""A survey of research articles published in professional journals containing the majority of rehabilitation counseling research was conducted. The following volumes were examined: (a) Rehabilitation Counseling Bulletin, volume 34,1990-1991; (b) Rehabilitation Psychology, Volume 35,1990; (c) Journal of Applied Rehabilitation Counseling; and (e) Rehabilitation Education, Volume 4, 1990. Of the 150 articles examined, 32 contained statistical tests that could be power analyzed.""","Rehabilitation Counseling Bulletin, Rehabilitation Psychology, Journal of Applied Rehabilitation Counseling, Rehabilitation Education",1990-1991,"All two group comparisons, correlations, but not ""nonparametric and multivariate procedures, which had not been analyzed in the Cohen-type studies, were not included.""",Clinical Psychology/Psychiatry,"""Each of the statistical tests was power analyzed using Borenstein and Cohen's (1988), statistical power analysis computer program. The power of each test was calculated for small, medium, and large effects. Alpha levels of .05 were assumed for all tests. The mean power of the statistical tests were established at the three levels of effect size for each research study.""",Mean power of statistical tests in each article,yes,32,NA,cohen 1988,cohen 1988,cohen 1988,cohen 1988,0.13,0.11,0.19,0.59,0.53,0.82,0.97,0.89,0.99,0.15,0.63,0.9,0.07,0.19,0.15,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
37,59,"Mone, M. A., Mueller, G. C. and Mauland, W.",The perceptions and usage of statistical power in applied psychology and management research,Personnel Psychology,1996,"Mone, Mueller, & Mauland (1996)",NA,"""Article selection. To assess the current level of statistical power in management research, we examined empirical articles in seven leading management journals, including Academy of Management Journal, Administrative Science Quarterly, Journal of Applied Psychology, Journal of Management, OrganizationalBehavior and Human Decision Processes, Personnel Psychology, and Strategic Management Journal. These journals fairly equally represent the research in micro (e.g.,JAP,PP),macro (e.g., AS&, SMJ), and mid-range (e.g.,AMJ, JOM, OBHDP) areas of management. In addition, the leading status of these journals has been well documented in past research (e.g., Coe & Weinstock, 1984;Extejt & Smith, 1990;Johnson & Podsakoff, 1994). For each journal, we randomly selected 10 articles per volume from the most recent, complete 3 years (1992-1994). This resulted in a total of 210 articles selected (3 years x 10 articles x 7 journals). These articles represent approximately 18% of all articles published in the seven journals over these 3years, and include 26,471statistical tests. Like past power research (e.g., Mazen et al., 1987a), this number of tests includes all T Sin correlation matrices. The articles were screened to include only those containing statistical tests for which retrospective power analyses could be conducted. By a considerable margin, our selection of the 3 most recent years’ articles covers more years, more journals, and more statistical tests than any previously published assessment of power in management studies""","""Academy of Management Journal, Administrative Science Quarterly, Journal of Applied Psychology, Journal of Management, OrganizationalBehavior and Human Decision Processes, Personnel Psychology, and Strategic Management Journal",1992-1994,"correlation, multiple regression, ANOVA, t test, and chi-square",Management / IO Psychology,"""Cohen’s (1988) power analysis tables and formulae were used to compute statistical power"" ""we omitted secondary tests like manipulation checks and peripheral reliability estimates.""",Mean power of statistical tests in each article,Yes,210,26471,cohen 1988,cohen 1988,cohen 1988,cohen 1988,NA,NA,NA,NA,NA,NA,NA,NA,NA,0.27,0.74,0.92,NA,NA,NA,NA,NA,NA,NA,NA,0.2472399,0.2476749,0.1364398,NA,"""To remedy the lowpower that existsin management journals, we recommend several actions. First, journal editors, reviewers, and other gatekeepers could make more explicit the need to conduct and report power analyses. In instanceswhere statistically nonsignificant results appear, it should be stressed that post-hoc power analyses should reach or exceed the conventional .80 level. Without such power levels, researchers have not given their null hypotheses a fair chance of being rejected and, consequently, spuriously negative results are more likely to occur. From the few (14) articles in our sample of 210 reporting the use of power analyses, 5 used post-hoc power analyses to adjust alpha levels or to explain their nonsignificant findings (e.g., Collins, Hatcher, & Ross, 1993;Cortina, Doherty, Schmitt, Kaufman, & Smith, 1992). More research would seemingly benefit from such practice. Second, more emphases need to he placed by both editorial gatekeepers and researchers on the role of effect sizes, and less on the single criterion of statistical significance (a,or p ) , for judging the findings of results. A number of researchers, including Cohen (1988,1992), HunterM A R K A. MONE ET AL. 117 and Schmidt (1990), Lykken (1968), and Rosenthal (1991) have articulated the advantagesof reporting and evaluating effect sizesin research. In short, the reporting of effect sizes allows results to be evaluated in terms of their practical significance and increases the likelihood that effects (i-e., magnitude of a phenomenon in a population or impact of independent variables) can be compared across studies. The reporting of effect sizes therefore increases the replicability of research streams and the ability to aggregate effect sizes through meta-analyses. However, neither replication nor meta-analyses negate the need for individual researchers to fully utilize statistical power analyses. Indeed, statistically underpowered research may never overcome what Rosenthal(l979) described as the “file drawer problem,” meaning that due to a field’s obsession with statisticalsignificance, a study’snonsignificant findings may relegate it to terminal existencein a file drawer. Finally, researchers should realize that they have everything to gain, and little to lose, by examining the statistical power in their research more carefully. Perhaps by more clearly recognizing the consequences of low power, researchers will see the personal (e.g., economic, efficiency) gains and the potential impact on the advancement of knowledge. Likewise,greater sensitivityto the squandered time, effort, and financial resources devoted to unnecessarily large data collections should provide further incentive to investigate appropriate power before conducting research. Over a dozen microcomputer software packages now exist for computing power in both planning and retrospective analysis formats (for a review, see Goldstein, 1989). Cohen’s (1988) manual on power analysis, complete with straightforward tables for almost all statistical tests, is both comprehensive and easy to use. Other hands-on sources for power analysis, generally based on Cohen’s work, include Kraemer and Thiemann (1987), Lipsey (1990), and Rosenthal and Rosnow (1991)"" P 117-118. ""As another suggestion, more sensitive research designs can be employed (e.g., blocking, covariates, within-subject designs) to reduce unexplained error (Greenwald, 1976;Rosenthal & Rosnow, 1991). Finally, more powerful statistical techniques (e.g., factorial, repeated measures, combined predictors, non-parametric statistics) and adjustments to alpha can be used (Cascio & Zedeck, 1983;Collinset al., 1993; Sawyer & Ball, 1981). Cascio and Zedeck (1983), for example, provide detailed treatment of how to adjust alpha (Type I error risk) to reduce Type I1errors, identifymg circumstances (e.g., fixed sample, anticipated effect size identified) when it is appropriate to adjust alpha to maximize statistical power"" p 118",NA,NA,NA,NA,NA,NA,NA,NA
38,40,"ClarkCarter, D.",The account taken of statistical power in research published in the British Journal of Psychology,British Journal of Psychology,1997,Clark-Carter (1997),NA,"""Each paper in the 1993 and 1994 volumes of the British Journal of Psychology, the two latest complete volumes available at the time of the analysis, was read for a number of details. … First, it was evaluated to see whether it reported the results of original quantitative research, where the choice of sample size had an effect on the power of the tests employed""",British Journal of Psychology,1993-1994,"t tests, Pearson's r, partial correlarions, chi square tests, and F tests, parametric alternatives used",General Psychology,"""For each paper which was deemed to be amenable to power analysis, the analysis was conducted by identifying the design, the sample size, the statistical test employed and, where appropriate and given, whether the test was of a directional or non-directional hypothesis. The results of statistical tests (inferential statements) were classified according to whether they were relevant to the hypotheses being explored in the paper or were subsidiary results. Many papers did not give precise hypotheses, preferring to use terms such as ‘investigates the effects of’. An example of an inferential statement which was not considered to be directly relevant to an hypothesis would be an interaction in a multi-way ANOVA which had not been identified explicitly as relevant to the research. In addition, the area of psychology covered by each paper and the nature of the participants used in the research were noted. The relevant small, medium and large effect sizes suggested by Cohen (1988) for the given test were then noted and the power was calculated for each of these effect sizes for each inferential statement reported in each paper, given the sample size which was employed in the study. 1. Where exact figures were not given by Cohen, appropriate interpolation was used. 2. Where a within-subjects design was employed in t tests and ANOVA and where ANCOVA was used the correlation was assumed to be .5 as recommended by Cohen (1988) and Lipsey (1990). 3. Where a between-subjects design was employed and the individual subsamples were not given, the subsamples were treated as equal; this has the effect of assuming the maximum power for the given sample size. 4. In those instances where the authors did not specify whether they had employed a one- or a twotailed significance test, a two-tailed test was assumed. Few papers gave the direction of the test. 5. Where no alpha level was mentioned it was assumed to be p < .05. In fact, on the rare occasions when an alpha level was given, in all but one case it was at p < .05. In the one exception it was set at p < .01 to allow for the large number of statistical tests employed. 6. In the case of the Mann-Whitney U test an adjustment to the effect size, provided by Singer, Lovie & Lovie (1986), was made and Cohen’s tables were then utilized. 7. In the case of Spearman’s rho, power values provided by Kraemer & Thiemann (1987) were used where available, otherwise those for Pearson’s r were used; once again this inflates the power estimate. Only seven Spearman’s rhos were involved in this analysis. 8. Kruskal-Wallis tests were treated as one-way, between subjects parametric ANOVAs, which also has the effect of inflating the power estimates. Only three Kruskal-Wallis tests were analysed. 9. Meddis (1984) notes that Fisher’s Exact Probability Test should only be employed when the marginal totals for the frequency table are fixed and that when they are not power is low. One paper employed this test and did so with one set of marginal totals free. This test was not included in the analysis. 10. One paper reported the results of logistic regression. Power calculations were not made for this test. 11. Planned andpost hot analyses which followed an ANOVA, such as pairwise comparisons of means, were not included. As Sedlmeier & Gigerenzer (1989) note, the use ofpost hoc tests, which adjust alpha levels, reduces power and so their exclusion from this study once again inflates the power reported in this analysis.""",Statistical tests,yes (parametric alternatives used),54,1090,cohen 1988,cohen 1988,cohen 1988,cohen 1988,0.14,NA,NA,0.62,NA,NA,0.87,NA,NA,0.2,0.6,0.82,0.19,0.26,0.19,NA,NA,NA,NA,NA,NA,NA,NA,NA,"""Possible soltltions: It is likely that this state of affairs will continue unless journal editors, lecturers on research methods and the writers of statistics books for researchers adopt a different policy. Introductory texts on research design and statistics for behavioural scientistsStatistical power in research published in the BJP 81 routinely mention Type I1 errors. However, they often simply note the dangers and suggest rules of thumb for avoiding such an error, such as using a large sample. This approach does not appear to be having an effect on the majority of researchers. An absence of power analysis during the design stage of a piece of research can lead to the use either of low power or of unnecessarily large samples; one study reported some analyses based on over 9000 participants. The consequence of the latter situation is that given a sufficiently large sample, even exceedingly small effect sizes are likely to be shown as statistically significant."" p 80-81. ... ""he problem of the mathematical nature of power calculation could be solved if power tables in textbooks could be presented for each test rather than the use of general power tables. It would greatly facilitate the estimation of the size of sample which would be required to yield a given level of power. The estimation of sample size has been further simplifiedby Cohen’s having published a computer program for the purpose (Borenstein 8t Cohen, 1988). Journal editors could also help to change practice by stating that they expect authors to explain their choice of sample size in terms of the power of their tests and to report the actual power and effect size in their results.""",NA,NA,0.02,0.93,0.12,1,0.24,1
39,96,"Cashen, L. H. and Geiger, S. W.",Statistical power and the testing of null hypotheses: A review of contemporary management research and recommendations for future studies,Organizational Research Methods,2004,"Cashen, Geiger (2004)",NA,"""To assess the level of statistical power in studies proposing null hypotheses, we reviewed five major journals that published management research over the 10-year period of January 1990 to December 1999""","Academy of Management Journal, Administrative Science Quarterly, Strategic Management Journal, Journal of Management, and Journal of Management Studies",1990-1999,"All direct tests (i.e., not interactions effects)",Management / IO Psychology,"""To remain consistent with previous power assessments, we determined power for each null hypothesis statistical test reported in the articles providing that it was a major statistical test for which Cohen’s (1977) power analysis tables and formulae were available (i.e., R2, ß, F, t, r, and p). All studies in this sample used parametric tests; thus the use of equivalent parametric tests in lieu of nonparametric tests was not necessary as it has been in previous power assessment articles (e.g., Sedlmeier & Gigerenzer, 1989). In addition, following Mazen, Graf, et al. (1987) and Mone et al. (1996), we omitted secondary tests such as manipulation checks and peripheral reliability estimates.""",test level,yes,43,77,cohen 1977,cohen 1977,cohen 1977,cohen 1977,0.22,NA,NA,NA,NA,NA,NA,NA,NA,0.29,0.81,NA,NA,NA,NA,NA,NA,NA,NA,NA,0.3251309,NA,NA,States that using the unit of analysis of the article did not impact the outcome (say analysis = .293 vs. article = .290 at small benchmark),NA,NA,NA,NA,NA,NA,NA,NA,NA
40,124,"Borkowski, Susan C. Mary Jeanne Welsh and Zhang, Qinke",An Analysis of Statistical Power in Behavioral Accounting Research.,Behavioral Research in Accounting,2001,"Borkowski, Welsh & Zhang (2001)",NA,Each study reporting analysable statistics,Issues in Accounting Education,1993-1997,"The studies includes all regaular and supplemental issues of issues in accounting education, behavioural research in a ccounting and jounral of management accouting research",Education,"""cases were evaluated for statistical power using Cohen's 1988 standard power tables""",Mean power of statistical tests in each article,yes,96,1782,cohen 1992,cohen 1992,cohen 1992,cohen 1992,0.15,NA,NA,0.69,NA,NA,0.78,NA,NA,0.23,0.71,0.93,NA,NA,NA,NA,NA,NA,NA,NA,0.146394,0.2219286,0.1316439,NA,"""power analysis can be a useful tool in research design"" ""if power is unaccestably low for all effect sizes, then perhaps the study could be redsinged using a stronger statistical tests""",NA,NA,NA,NA,NA,NA,NA,NA
41,104,"Maddock, J. E. and Rossi, J. S.",Statistical power of articles published in three health psychology-related journals,Health Psychology,2001,Maddock & Rossi (2001),NA,"""All of the articles published in the 1997 Volume of the Journals HP, AB and JSA were examined, and only articles containing statistical tests in which power could be computed were selected""","Health psychology, Addictive Behaviours, the Journal of Studies on Alcohol",1997,"""Major tests are based directly on the research hypothesis of the study, whereas peripheral tests are not. Periferal tests were not included. Examination of major tests was limited to the tests included in Cohen's (1988) power handbook plus the multivariate analysis of variance""",Clinical Psychology/Psychiatry,"""Power was calculated from Cohen's 1988 tables and from computer programs designed to assess power (Rossi, 1990). Once the power was computed for each test the results were combined to assess the average power for each study. … When doing this the unit of analysis was the article, because the number of tests in each artucle was found to vary widely""",Mean power of statistical tests in each article,yes,187,8266,cohen 1988,cohen 1988,cohen 1988,cohen 1988,0.22,NA,NA,0.86,NA,NA,0.99,NA,NA,0.36,0.77,0.92,0.3,0.26,0.16,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
42,92,"Okumura, Y. and Sakamoto, S.",Statistical power and effect sizes of depression research in Japan,Psychiatry and Clinical Neurosciences,2011,Okumura & Sakamoto (2011),NA,"""Data for the present study were collected as a part of a systematic review conducted in 2007. The main theme of the systematic review was to evaluate the methods used for depression research in Japan. Details of this project have been reported elsewhere.20 We identified the leading journals of psychiatry and psychology in Japan (Table 2). We selected 18 leading journals that met the following criteria: (i) the journal title included one of the terms ‘psychiat*’, ‘psycho*’, or ‘counseling’; and (ii) more than 100 Japanese libraries subscribed to it in July 2007. Next, as Japanese reference databases were still underdeveloped, we manually searched all of the articles in the 18 journals published between January 1990 and December 2006. We identified 974 depression studies mentioned in 935 articles that met the following criteria: (i) words such as ‘depress*’, ‘dysphoria’, ‘mood disorder’, or ‘bipolar disorder’, were used in the title, abstract, or key words; or (ii) any statement on depression scales was written in the Methods section; and (iii) it was an Original Article or Short Report. We used a broader definition of ‘depression’ rather than the DSM-defined major depression because some studies have questioned the current categorical split of mood disorders into bipolar and depressive disorders,21 and the boundaries that separate depressive disorders from normality.22 In addition, we excluded experimental studies and case studies from the statistical power and sample effect sizes computations to maximize the interpretability of the results""","""Clinical Psychiatry, Japanese Journal of Child and Adolescent Psychiatry, Japanese Journal of Clinical Psychiatry, Japanese Journal of Counseling Science, Japanese Journal of Developmental Psychology, Japanese Journal of Educational Psychology, Japanese Journal of Experimental Social Psychology, Japanese Journal of Geriatric Psychiatry, Japanese Journal of Psychiatric Nursing, Japanese Journal of Psychiatric Treatment, Japanese Journal of Psycho-analysis, Japanese Journal of Psychology, Japanese Journal of Psychotherapy, Japanese Journal of Social Psychology, Japanese Society of Psychosomatic Medicine, Journal of Japanese Clinical Psychology, Psychiatria et Neurologia Japonica, Psychiatria et Neurologia Paediatrica Japonica""",1990-2006,"independent samples t-test, correlation, chi square-test, factorial ANOVA, and multiple regression analysis",Clinical Psychology/Psychiatry,"""To calculate the statistical power for independent t-test, correlation, c2-test, factorial ANOVA, and multiple regression analysis, we abstracted the types of statistical tests, sample sizes, and degrees of freedom. To estimate the statistical power, we followed the procedures in previous statistical power surveys.15,19,34 First, Cohen’s definitions of small, medium, and large population effect sizes were adopted (Table 1).5 Second, a two-tailed significance level (a) was held at a uniform value of 0.05 for all statistical tests. Third, the reported sample size for each statistical test was used. Then, the statistical power for each statistical test was calculated with the above specifications on what was deemed to be the non-redundant statistical test. To estimate the statistical power, we used the computation procedures described by Cohen.4 Once the statistical power was computed for each statistical test, the average statistical power values for detecting small, medium, and large population effect sizes were calculated for each study""","Looked at mean power of study (i.e., separate samples within an article)",yes,311,311,cohen 1988,cohen 1988,cohen 1988,cohen 1988,NA,NA,NA,NA,NA,NA,NA,NA,NA,0.27,0.71,0.89,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
43,91,"Bezeau, S. and Graves, R.",Statistical power and effect sizes of clinical neuropsychology research,Journal of Clinical and Experimental Neuropsychology,2001,Bezeau & Graves (2001),NA,"""The approach of this study was to review a representative sample of published articles in the field of clinical neuropsychology that reported comparisons of a clinical group to either another clinical group or to a normal comparison group. Articles from the Journal of Clinical and Experimental Neuropsychology, the Journal of the International Neuropsychology Society, and Neuropsychology were reviewed to determine: the observed statistical power for three different effect sizes, the actual effect size, the statistical power given the actual effect size, and the sample size. The number of statistical tests completed, the use of any alpha corrections, and whether the article mentioned power and effect sizes were also tabulated"" ""Thirty-three articles were reviewed from each of the 1998 and the 1999 volumes for a total of sixtysix articles. The first article in the first issue for each journal was the starting point and articles were reviewed in sequence until 11 articles were completed for each particular journal and year. No articles were excluded unless by necessity, namely because of the lack of any statistical analyses in nonexperimental studies, the use of only descriptive statistics, the use of non-clinical groups, a predicted non-significant result, our inability to determine STATISTICAL POWER AND NEUROPSYCHOLOGY 401 Downloaded by [The University Of Melbourne Libraries] at 06:17 05 January 2018which single statistical test corresponded to the major hypothesis (or to determine what the major hypothesis might be), or a study that did not ultimately relate a two-group comparison to the primary hypothesis. A few articles used ANCOVA when the covariate was significantly different between the preexisting groups, an inappropriate application (Evans & Anastasio, 1968), and these studies were also excluded. An estimated population effect size (using the delta parameter) was calculated using the formulae given by Richardson (1996) from data provided in the original articles. Power for the effect size implied by the data was then recalculated using GPOWER""","Clinical studies published in Journal of Clinical and Experimental Neuropsychology, the Journal of the International Neuropsychology Society, and Neuropsychology",1998-1999,"""Calculations of statistical power were made on what was deemed to be the primary hypothesis of the study. Post hoc power analyses were conducted using the GPOWER program (Faul & Erdfelder, 1992) with the .05 value of alpha assumed. In some studies, alpha correction procedures were used (e.g., Bonferroni) and these were noted, but were not incorporated into our calculations.""",Clinical Psychology/Psychiatry,"""Calculations of statistical power were made on what was deemed to be the primary hypothesis of the study. Post hoc power analyses were conducted using the GPOWER program (Faul & Erdfelder, 1992) with the .05 value of alpha assumed. In some studies, alpha correction procedures were used (e.g., Bonferroni) and these were noted, but were not incorporated into our calculations.""","""major hypothesis"" test",yes,66,66,cohen 1988,NA,cohen 1988,cohen 1988,NA,NA,NA,0.451,NA,NA,0.785,NA,NA,NA,0.5,0.768,NA,0.201,0.189,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
44,112,"Osborne, J. W.",NA,NA,2008,Osborne (2008) ,NA,"""In order to assess the current state of educational psychology in a representative fashion, three broadly-available educational psychology journals were selected to represent research in the field: Journal of Educational Psychology, Contemporary Educational Psychology, and the British Journal of Educational Psychology. A random sample of 50% of all empirical articles published in these journals during the 1998 and 1999 volumes were surveyed."" ... ""Articles containing no statistical analysis (e.g., qualitative, theory, or review articles) or containing statistical analyses for which there are no good methods or widely accepted methods of computing power or effect sizes (e.g., nonparametric tests, exploratory factor analysis) were excluded from this study (these were rare). All statistical tests within an article that related to central hypotheses being tested were recorded. Ancillary analyses, such as manipulation checks, and psychometric analyses of measures used, were not recorded""","Journal of Educational Psychology, Contemporary Educational Psychology, and the British Journal of Educational Psychology",1998-1999,unclear,Education,"""Using methods described by Cohen (1988; see also Thompson, 2002b), all statistics were converted to effect sizes (d), and observed power was computed (as few authors report effect sizes and power, effect sizes were usually calculated from reported results). For tests not usually associated with d for an effect size (e.g., correlation, multiple regression, chi-square), algebraic manipulations derived from Cohen (1988) were called upon to transform all other effect size indices to ds for comparability.4 For all tests, an a of .05 was assumed and, where appropriate, twotailed tests were assumed as well. The number of relevant effect sizes in individual articles ranged from 1 to 54. All effect sizes and power information were aggregated to the article level. Finally, to allow for comparison with other power surveys, power to detect small (d = .20), medium (d = .50), and large (d = .80) effects was calculated. Other information gathered about the articles includes: whether the research was an experimental or correlational design; whether it used a college sample or not; whether the authors reported effect sizes, observed or a priori power, or any information about measurement quality (e.g., Cronbach’s a, reliability of any type, or other indicators of measurement quality); and whether the authors gave any indication that the assumptions of the analyses used were checked.""","Mean power of statistical tests in each article, parametric alternatives used for non-parametric tests",NA,96,NA,cohen 1988,cohen 1988,cohen 1988,cohen 1988,NA,NA,NA,NA,NA,NA,NA,NA,NA,0.2,0.6,0.84,0.27,0.27,0.16,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
45,49,"Ward, Rose Marie",Highly significant findings in psychology: A power and effect size survey,Dissertation Abstracts International: Section B: The Sciences and Engineering,2002,Ward (2002),NA,"""All articles published in the Journal of Consulting and Clinical Psychology. 2000, vol. 68, the Journal of Personality and Social Psychology, 2000, vol. 78, and the Journal of Abnormal Psychology. 2000, vol. 109 were eligible for inclusion.""","Journal of Consulting and Clinical Psychology, 2000, vol. 68, the Journal of Personality and Social Psychology, 2000, vol. 78, and the Journal of Abnormal Psychology, 200, vol. 109",2000,"Preason correlation, ANOVA, t-test, ANCOVA, Chi-square, Multiple regression, MANOVA",General Psychology,"""The PASS software (NCSS: 2001) will be used to determine power level in conjunction with power programs developed by Rossi (1990). The following decision rules were employed: 1. Where a between-subjects design was utilized and subsamples sizes were not available, the subsamples were treated as equal. By assuming equal subsamples, maximum power was given for the research design. 2. Two-tailed tests with an alpha level of .05 were assumed for all tests. 3. Where a within-subjects design was employed in t-tests and ANOVA, the correlation was assumed to be .5 as recommended by Cohen (1988) and Lipsey (1990). 4. When ANCOVA was used, the correlation was assumed to be .5as recommended by Cohen (1988) and Lipsey (1990). 5. Statistical tests in which many assumptions were to be made and little research evidence supporting these assumptions were excluded (i.e. survival analysis, path analysis, logistic regression, hierarchical linear modeling, etc.)."" ""
Because the number of statistical tests included in an article varied greatly (from zero to over 100), the article was used as the unit of analysis. This equalized all articles and allowed them all to contribute equally to the power survey results.""",Mean power of statistical tests in each article,yes,157,2747,Cohen 1977,Cohen 1977,Cohen 1977,Cohen 1977,0.139,NA,NA,0.663,NA,NA,0.962,NA,NA,0.206,0.646,0.884,0.194,0.253,0.152,146,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
46,107,"Woods, S. P., Rippeth, J. D., Conover, E., Carey, C. L., Parsons, T. D. and Troster, A. I.",Statistical power of studies examining the cognitive effects of subthalamic nucleus deep brain stimulation in Parkinson's disease,Clinical Neuropsychologist,2006,Woods et al. (2006),NA,"""To identify the relevant published articles, key search terms (e.g., subthalamic nucleus, deep brain stimulation, cognitive, etc.) were entered into the PsychINFO, PubMed, and ISI Web of Science electronic databases for the years 1997 to 2004. In addition, references from articles reporting cognitive outcomes of STN DBS were reviewed to identify other papers of interest that may not have been indexed in the aforementioned databases. To be included in the current power review, an article must have used a repeated-measures design and at least one paired-samples group- level statistical analysis (e.g., a paired-samples t-test) to examine the cognitive seque- lae of STN DBS in a sample of persons with PD. Studies that used single- and=or mixed comparison-group designs were included. We excluded review articles, single case studies, statement papers, investigations that used only animal subjects, and studies not published in English.""This study was conducted using papers published during 2011 in the 10 nursing journals with the highest 5-year impact factors according to the Journal Citation Reports1 (Science and Social Sciences 2010 editions).""""",Studies Examining the Cognitive Effects of Subthalamic Nucleus Deep Brain Stimulation in Parkinson's Disease. Diverse journals.,1997-2004,article sample size,Neuropsychology,"""the G Power statistical package (Buchner, Faul, &Erdfelder, 1997; Erdfelder, Faul, & Buchner, 1996) was then used to calculate the statistical power of each study. Specifically, post hoc power calculations for paired-samples t-tests were generated considering each individual study’s sample size, associated degrees of freedom, and a critical alpha level of 0.05. The effect size index f [(rm)=r] is recommended for study designs in which k   2 (Cohen, 1988), as with the STN DBS literature where multiple repeated measures designs are common- place. Accordingly, power estimates were conducted using a priori defined Cohen’s f values for small ( f 1/4 0.10), medium ( f 1/4 0.25), and large ( f 1/4 0.40) effect sizes. Cohen’s f values—which are always positive and range from zero to an indefinite upper limit—are interpreted as the standard deviation of the standardized means in a given set of populations (Cohen, 1988). Following recommendations from Zak- zanis (2001) and Rossi (1990), we also calculated power estimates for very large ( f = 1.5)""",Mean power of statistical tests in each article,no,30,30,f (cohen 1988),0.1,0.25,0.4,0.06,0.06,0.07,0.13,0.09,0.2,0.25,0.17,0.43,0.07,0.18,0.34,0.02,0.13,0.23,14,8,22,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
47,121,"Overland, C. T.",Statistical Power in the Journal of Research in Music Education (2000-2010): A Retrospective Power Analysis,Bulletin of the Council for Research in Music Education,2014,Overland (2014),NA,articles reporting NHST in JRME,Journal of Research in Music Education,2000-2010,"""(a) t tests; (b) Pearson r tests; (c) sign tests; (d) z tests for the di<U+FB00>erence between two independent correlation coefcients; (d) z tests for the di<U+FB00>erence between two independent proportions; (e) chi-square tests; and (f) F tests in the analyses of variance, multivariance, covariance, and linear multiple regression.""",Education,"""Calculations for Cohen’s three levels of e<U+FB00>ect (i.e., small, medium, large) were performed using G*Power 3.1 (Faul, Erdfelder, Lang, & Buchner, 2007)""  ""Calculations for Cohen’s three levels of e<U+FB00>ect (i.e., small, medium, large) were performed using G*Power 3.1 (Faul, Erdfelder, Lang, & Buchner, 2007). Only tests of a study’s core hypotheses were considered; tests of assumptions, reliabilities, manipulation. checks, factor analyses, or other nonintegral tests within articles were ignored. Unless otherwise specifed, each test was considered two-tailed. When sample sizes were not provided, it was assumed all groups were of equal size. All tests of main and interaction e<U+FB00>ects were considered unless planned comparisons were explicitly indicated.""",Power for the most commonly used statistical test in each article (non-parametric alternatives used when non-parametric tests used) (ignoring,no,125,NA,Cohen 1988 (says 62 but appears to be wrong),Cohen 1988,Cohen 1988,Cohen 1988,NA,NA,NA,NA,NA,NA,NA,NA,NA,0.24,0.64,0.84,NA,NA,NA,NA,NA,NA,NA,NA,0.218271,0.2890624,0.2384077,"Says ""Cohen 1962"" but uses effect sizes from 1988","""Therefore, as new innovations in empirical research surface, the profession should remain mindful of the most relevant reporting practices, present both signifcant and nonsignifcant fndings throughout studies, and strongly encourage replication studies so that future research can continue to be fully informed.""",NA,NA,NA,NA,NA,NA,NA,NA
48,77,"Schweizer, G. and Furley, P.",Reproducible research in sport and exercise psychology: The role of sample sizes,Psychology of Sport and Exercise,2016,Schweizer & Furley (2016),NA,"""We examined all empirical articles published in all four journals starting in 2009 and ending in 2013. We did not examine non-empirical articles such as review articles and comments. After distinguishing between empirical and non-empirical articles, we coded samples sizes of the remaining articles. When an article contained multiple studies that were analysed separately, we coded each study's sample size separately. For every study, we coded in which journal it was published, publication year, sample size, and whether it was an experimental, a quasi-experimental, or a corre- lational study. We coded a study as experimental, when there was a randomized assignment of participants to factors (between-par- ticipants design) or a repeated manipulation (within-participants design). Furthermore, we distinguished between within- participants, mixed (a combination of a within-participants and a between-participants factor), and between-participants designs.""","""International Journal of Sport Psychology (IJSP), Journal of Applied Sport Psychology (JASP), Journal of Sport and Exercise Psychology (JSEP), and Psychology of Sport and Exercise (PSE)""",2009-2013,Correlational research design,Sport and exercise psychology,"""Correlational studies: We calculated the power for detecting an existing correlation between two continuous variables.
• Test family (Exact); statistical test: Correlation: Bivariate normal model; Post-hoc power analyses (Compute achieved power) –> two-tailed test
Experimental studies – between-participants designs: For all studies in this category, we calculated the power for a 2 groups between-participants design.
• Test family (t-test); statistical test: means – differences between two independent means (two groups); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to an equivalent outcome to using “F-Test Family: ANOVA, Fixed effects, omnibus, one-way (two-groups)”
Experimental studies - within-participants designs: For all studies in this category, we calculated the power for a 2 measurements within-participants design. For within-participants designs, power also depends on the correlation between the repeated measurements. The higher this correlation, the higher the power. We set this correlation to .5.
• Test family (t-test); statistical test: means – differences between two dependent means (matched pairs); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to a similar outcome to using “F-Test Family: ANOVA, repeated measures, within factors (2 measurements, .5 correlation between measurements; nonsphericity correction = 1).
• The t-test analysis is slightly more favorable in terms of power.
Experimental studies - mixed designs: When a study employed a mixed design (i.e., there were within-participants and between-participants factors) we calculated power separately for the within-participants and the between-participants factor and the interaction.
• Within-factor
o Test family F-Test; repeated measures, within factors (2 measurements, number of groups = 2; .5 correlation between measurements; nonsphericity correction =1)
• Between-factor
o Test family F-Test; repeated measures, between factors (2 measurements, number of groups = 2; .5 correlation between measurements)
• Interaction factor
o Test family F-Test; repeated measures, within factors (2 measurements, number of groups = 2; .5 correlation between measurements; nonsphericity correction =1)
Quasi-experimental studies: For all studies in this category, we calculated the power for a 2 groups between-participants design.
• Test family (t-test); statistical test: means – differences between two independent means (two groups); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to an equivalent outcome to using “F-Test Family: ANOVA, Fixed effects, omnibus, one-way (two-groups)”
""","Power of type of design, by article",generalised by type of research design,337,not assessed,r,0.1,0.3,0.5,0.31,0.2,0.5,0.99,0.93,0.99,0.99,0.99,0.99,NA,NA,NA,NA,NA,NA,221,124,386,NA,NA,NA,NA,NA,This one looked at the power to detect an effect at the median sample size in each type of design makes quite a few mistakes in the text. Mixed effects - I have only included the within factor analysis in order to maintain independence,NA,NA,NA,NA,NA,NA,NA,NA,NA
49,77,"Schweizer, G. and Furley, P.",NA,NA,2016,Schweizer & Furley (2016) ,NA,NA,NA,2009-2013,Quasi-experimental research design,Sport and exercise psychology,"""Correlational studies: We calculated the power for detecting an existing correlation between two continuous variables.
• Test family (Exact); statistical test: Correlation: Bivariate normal model; Post-hoc power analyses (Compute achieved power) –> two-tailed test
Experimental studies – between-participants designs: For all studies in this category, we calculated the power for a 2 groups between-participants design.
• Test family (t-test); statistical test: means – differences between two independent means (two groups); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to an equivalent outcome to using “F-Test Family: ANOVA, Fixed effects, omnibus, one-way (two-groups)”
Experimental studies - within-participants designs: For all studies in this category, we calculated the power for a 2 measurements within-participants design. For within-participants designs, power also depends on the correlation between the repeated measurements. The higher this correlation, the higher the power. We set this correlation to .5.
• Test family (t-test); statistical test: means – differences between two dependent means (matched pairs); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to a similar outcome to using “F-Test Family: ANOVA, repeated measures, within factors (2 measurements, .5 correlation between measurements; nonsphericity correction = 1).
• The t-test analysis is slightly more favorable in terms of power.
Experimental studies - mixed designs: When a study employed a mixed design (i.e., there were within-participants and between-participants factors) we calculated power separately for the within-participants and the between-participants factor and the interaction.
• Within-factor
o Test family F-Test; repeated measures, within factors (2 measurements, number of groups = 2; .5 correlation between measurements; nonsphericity correction =1)
• Between-factor
o Test family F-Test; repeated measures, between factors (2 measurements, number of groups = 2; .5 correlation between measurements)
• Interaction factor
o Test family F-Test; repeated measures, within factors (2 measurements, number of groups = 2; .5 correlation between measurements; nonsphericity correction =1)
Quasi-experimental studies: For all studies in this category, we calculated the power for a 2 groups between-participants design.
• Test family (t-test); statistical test: means – differences between two independent means (two groups); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to an equivalent outcome to using “F-Test Family: ANOVA, Fixed effects, omnibus, one-way (two-groups)”
""","Power of type of design, by article",generalised by type of research design,117,not assessed,r,0.1,0.3,0.5,0.16,0.1,0.3,0.84,0.54,0.99,0.99,0.96,0.99,NA,NA,NA,NA,NA,NA,91,45,206,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
50,77,"Schweizer, G. and Furley, P.",NA,NA,2016,Schweizer & Furley (2016)  ,NA,NA,NA,2009-2013,Experimental within subjects design,Sport and exercise psychology,"""Correlational studies: We calculated the power for detecting an existing correlation between two continuous variables.
• Test family (Exact); statistical test: Correlation: Bivariate normal model; Post-hoc power analyses (Compute achieved power) –> two-tailed test
Experimental studies – between-participants designs: For all studies in this category, we calculated the power for a 2 groups between-participants design.
• Test family (t-test); statistical test: means – differences between two independent means (two groups); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to an equivalent outcome to using “F-Test Family: ANOVA, Fixed effects, omnibus, one-way (two-groups)”
Experimental studies - within-participants designs: For all studies in this category, we calculated the power for a 2 measurements within-participants design. For within-participants designs, power also depends on the correlation between the repeated measurements. The higher this correlation, the higher the power. We set this correlation to .5.
• Test family (t-test); statistical test: means – differences between two dependent means (matched pairs); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to a similar outcome to using “F-Test Family: ANOVA, repeated measures, within factors (2 measurements, .5 correlation between measurements; nonsphericity correction = 1).
• The t-test analysis is slightly more favorable in terms of power.
Experimental studies - mixed designs: When a study employed a mixed design (i.e., there were within-participants and between-participants factors) we calculated power separately for the within-participants and the between-participants factor and the interaction.
• Within-factor
o Test family F-Test; repeated measures, within factors (2 measurements, number of groups = 2; .5 correlation between measurements; nonsphericity correction =1)
• Between-factor
o Test family F-Test; repeated measures, between factors (2 measurements, number of groups = 2; .5 correlation between measurements)
• Interaction factor
o Test family F-Test; repeated measures, within factors (2 measurements, number of groups = 2; .5 correlation between measurements; nonsphericity correction =1)
Quasi-experimental studies: For all studies in this category, we calculated the power for a 2 groups between-participants design.
• Test family (t-test); statistical test: means – differences between two independent means (two groups); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to an equivalent outcome to using “F-Test Family: ANOVA, Fixed effects, omnibus, one-way (two-groups)”
""","Power of type of design, by article",generalised by type of research design,109,not assessed,r,0.1,0.3,0.5,0.15,0.11,0.23,0.84,0.62,0.97,0.99,0.98,0.99,NA,NA,NA,NA,NA,NA,24,15,39,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
51,77,"Schweizer, G. and Furley, P.",NA,NA,2016,Schweizer & Furley (2016)    ,NA,NA,NA,2009-2013,Experimental between subjects design,Sport and exercise psychology,"""Correlational studies: We calculated the power for detecting an existing correlation between two continuous variables.
• Test family (Exact); statistical test: Correlation: Bivariate normal model; Post-hoc power analyses (Compute achieved power) –> two-tailed test
Experimental studies – between-participants designs: For all studies in this category, we calculated the power for a 2 groups between-participants design.
• Test family (t-test); statistical test: means – differences between two independent means (two groups); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to an equivalent outcome to using “F-Test Family: ANOVA, Fixed effects, omnibus, one-way (two-groups)”
Experimental studies - within-participants designs: For all studies in this category, we calculated the power for a 2 measurements within-participants design. For within-participants designs, power also depends on the correlation between the repeated measurements. The higher this correlation, the higher the power. We set this correlation to .5.
• Test family (t-test); statistical test: means – differences between two dependent means (matched pairs); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to a similar outcome to using “F-Test Family: ANOVA, repeated measures, within factors (2 measurements, .5 correlation between measurements; nonsphericity correction = 1).
• The t-test analysis is slightly more favorable in terms of power.
Experimental studies - mixed designs: When a study employed a mixed design (i.e., there were within-participants and between-participants factors) we calculated power separately for the within-participants and the between-participants factor and the interaction.
• Within-factor
o Test family F-Test; repeated measures, within factors (2 measurements, number of groups = 2; .5 correlation between measurements; nonsphericity correction =1)
• Between-factor
o Test family F-Test; repeated measures, between factors (2 measurements, number of groups = 2; .5 correlation between measurements)
• Interaction factor
o Test family F-Test; repeated measures, within factors (2 measurements, number of groups = 2; .5 correlation between measurements; nonsphericity correction =1)
Quasi-experimental studies: For all studies in this category, we calculated the power for a 2 groups between-participants design.
• Test family (t-test); statistical test: means – differences between two independent means (two groups); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to an equivalent outcome to using “F-Test Family: ANOVA, Fixed effects, omnibus, one-way (two-groups)”
""","Power of type of design, by article",generalised by type of research design,84,not assessed,r,0.1,0.3,0.5,0.14,0.1,0.2,0.77,0.53,0.93,0.99,0.96,0.99,NA,NA,NA,NA,NA,NA,75,44,124,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
52,77,"Schweizer, G. and Furley, P.",NA,NA,2016,Schweizer & Furley (2016)     ,NA,NA,NA,2009-2013,"Experimental mixed design, within factor test",Sport and exercise psychology,"""Correlational studies: We calculated the power for detecting an existing correlation between two continuous variables.
• Test family (Exact); statistical test: Correlation: Bivariate normal model; Post-hoc power analyses (Compute achieved power) –> two-tailed test
Experimental studies – between-participants designs: For all studies in this category, we calculated the power for a 2 groups between-participants design.
• Test family (t-test); statistical test: means – differences between two independent means (two groups); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to an equivalent outcome to using “F-Test Family: ANOVA, Fixed effects, omnibus, one-way (two-groups)”
Experimental studies - within-participants designs: For all studies in this category, we calculated the power for a 2 measurements within-participants design. For within-participants designs, power also depends on the correlation between the repeated measurements. The higher this correlation, the higher the power. We set this correlation to .5.
• Test family (t-test); statistical test: means – differences between two dependent means (matched pairs); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to a similar outcome to using “F-Test Family: ANOVA, repeated measures, within factors (2 measurements, .5 correlation between measurements; nonsphericity correction = 1).
• The t-test analysis is slightly more favorable in terms of power.
Experimental studies - mixed designs: When a study employed a mixed design (i.e., there were within-participants and between-participants factors) we calculated power separately for the within-participants and the between-participants factor and the interaction.
• Within-factor
o Test family F-Test; repeated measures, within factors (2 measurements, number of groups = 2; .5 correlation between measurements; nonsphericity correction =1)
• Between-factor
o Test family F-Test; repeated measures, between factors (2 measurements, number of groups = 2; .5 correlation between measurements)
• Interaction factor
o Test family F-Test; repeated measures, within factors (2 measurements, number of groups = 2; .5 correlation between measurements; nonsphericity correction =1)
Quasi-experimental studies: For all studies in this category, we calculated the power for a 2 groups between-participants design.
• Test family (t-test); statistical test: means – differences between two independent means (two groups); Post-hoc power analyses (Compute achieved power) –> two-tailed test
• This leads to an equivalent outcome to using “F-Test Family: ANOVA, Fixed effects, omnibus, one-way (two-groups)”
""","Power of type of design, by article",generalised by type of research design,45,not assessed,r,0.1,0.3,0.5,0.27,0.18,0.45,0.99,0.91,0.99,0.99,0.99,0.99,NA,NA,NA,NA,NA,NA,47,30,85,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
53,123,"Szucs, D. and Ioannidis, J. P. A.",Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature.,PLOS Biology,2017,Szucs & Ioannidis (2017),NA,"""We extracted statistical information from cognitive neuroscience and psychology papers published as PDF files. We sampled 18 journals frequently cited in cognitive neuroscience and psychology. Our aim was to collect data on the latest publication practices. To this end, we analyzed 4 y of regular issues for all journals published between Jan 2011 to Aug 2014. The time period was chosen to represent recent publication practices (during the closest possible period before the start of data analysis). Particular journals were chosen so as to select frequently cited journals with a range of impact factors from our disciplines of interest"" ... ""When there were fewer than 20 empirical papers in a journal issue, all empirical research reports with any reported t statistics were analyzed. When there were more than 20 papers in an issue, a random sample of 20 papers were analyzed merely because this was the upper limit of papers accessible in one query. This procedure sampled most papers in most issues and journals. All algorithms and computations were coded in Matlab 2015b (www.mathworks. com). Initial PDF file text extraction relied on the PdfToolbox Matlab package""","Nature Neuroscience, Neuron, Brain, The Journal of Neuroscience, Cerebral Cortex, NeuroImage, Cortex, Biological Psychology, Neuropsychologia, Neuroscience, Psychological Science, Cognitive Science, Cognition, Acta Psychologica, Journal of Experimental Child Psychology, Biological Psychiatry, Journal of Psychiatric Research, Neurobiology of Ageing",2011-2014,t tests,General Psychology,"""The power of t-tests was computed from the noncentral t distribution [22] assuming the above mixture of one-sample, matched-, and independent-sample t-tests.""",Assumes that the mix of t tests was the same as that of a sample of 100,Assumes that the mix of t tests was the same as that of a sample of 100,3801,26841,Cohen 1988 (d),0.2,0.5,0.8,0.11,NA,NA,0.44,NA,NA,0.73,NA,NA,0.17,0.49,0.71,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"""Some promising avenues to resolve the current replication crisis could include the preregistration of study objectives, compulsory prestudy power calculations, enforcing minimally required power levels, raising the statistical significance threshold to p < 0.001 if NHST is used, publishing negative findings once study design and power levels justify this, and using Bayesian analysis to provide probabilities for both the null and alternative hypotheses""",NA,NA,NA,NA,NA,NA,NA,NA
