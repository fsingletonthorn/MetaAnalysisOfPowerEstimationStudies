---
title             : "The Statistical Power of Psychology Research: A Systematic Review and Meta-analysis"
shorttitle        : "The Statistical Power of Psychology Research"

author: 
  - name          : "Felix Singleton Thorn"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Faculty of Medicine, Dentistry and Health Sciences, School of Psychological Sciences, University of Melbourne, Parkville, Victoria, 3010, Australia"
    email         : "fsingletonthorn@gmail.com"
  - name          : "Fiona Fidler"
    affiliation   : "2,3"
  - name          : "Paul Dudgeon"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Melbourne, School of Psychological Sciences"
  - id          : "3"
    institution   : "University of Melbourne, School of Historical and Philosophical Studies"
  - id          : "2"
    institution   : "University of Melbourne, School of Biosciences"

authornote: |

  F. Singleton Thorn conceptualized the research question, designed and performed the statistical analysis, collected the data and drafted this manuscript. F. Fidler and P. Dudgeon provided critical revisions to this manuscript. P. Dudgeon gave essential advice on the statistical analyses reported in this manuscript. 

abstract: |
  Statistical power describes the probability of a statistical test reaching statistical significance given a specific alternative hypothesis. Over the last half century, a number of "Power Surveys" have been published in psychology, papers which examine the statistical power of a set of studies in the published literature. This study uses mixed effects meta-analyses to analyze 46 power surveys, including over 8,000 individual studies published from 1932-2014, to estimate the average statistical power of psychology research at Cohen’s standardized effect size benchmarks and to show how this value has changed over time. The average statistical power of psychology research over this time period at Cohen’s (1988) benchmarks is extremely low for ‘small’ effects, .23 (95% CIs [.17, .29]), somewhat low for ‘medium’ effects, .62 (95% CI [.54, .70]), and only acceptably high for ‘large’ effects, .80 (95% CI [.68, .92]). The average statistical power of published psychology research has seen little to no change over time. A secondary analysis of surveys which assessed how often power analyses are reported in psychology research suggests that power analysis reporting rates have increased slightly over time but remain low. Finally, methods for avoiding the negative impacts of low powered research are outlined.


keywords          : "Publication bias, effect size, QRPs, statistical power, metascience, metaresearch"

bibliography      : ["references.bib"]

floatsintext      : yes
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : yes
always_allow_html : yes

documentclass     : "apa6"
classoption       : "man"
output:
  papaja::apa6_pdf
#:
#    includes:
#      after_body:
#      "Supplementary_materials.tex"

header-include:
    - \DeclareDelayedFloatFlavor{foo}{table}
    - \raggedbottom
---

```{r setup, include = FALSE}
library(papaja)
library(knitr)
library(kableExtra)
library(readxl)
library(metafor)
library(tidyverse)
library(stringr)
library(ggplot2)
library(mice)
library(Amelia)
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

```

```{r custom functions}
########### Setting up functions for data imputation ########
# the following estimators are from Wan, X., Wang, W., Liu, J., & Tong, T. (2014). Estimating the sample mean and standard deviation from the sample size, median, range and/or interquartile range. BMC Medical Research Methodology, 14(1), 135. doi:10.1186/1471-2288-14-135
# But the functions are from the package veramata, See Charles Grey's Dissertation for the completed package, or https://github.com/softloud/varameta before she finishes it. 
# parameters in the following functions: 
#   a Minimum value of sample.
##  m Median of sample.
##  b Maximum value of sample.
##  n Sample size.
##  q1 first quartile
##  q3 third quartile

eff_est_wan_c2 <- function(a, q.1, m, q.3, b, n) {
  x.bar <- (a + 2 * q.1 + 2 * m + 2 * q.3 + b) / 8
  s <- 
    (b - a) / (4 * qnorm(
      (n - 0.375) / (n + 0.25)
    )) +
    (q.3 - q.1) / (4 * qnorm(
      (0.75 * n - 0.125) / (n + 0.25)
    ))
  return(list(
    centre = x.bar,
    se = s / sqrt(n)
  ))
}


eff_est_wan_c3 <- function(q.1, m, q.3, n) {
  x.bar <- (q.1 + m + q.3) / 3
  s <- (q.3 - q.1) / (2 * qnorm(
    (0.75 * n - 0.125) / (n + 0.25)
  ))
  return(list(
    centre = x.bar,
    se = s / sqrt(n)
  ))
}

effect_se <- function(centre,
                      spread,
                      n,
                      centre_type = "mean",
                      spread_type = "sd") {
  
  # Estimate the standard error of the effect, depending on how that effect is
  # reported (median or mean).
  #
  # @param centre A sample mean or a median.
  # @param spread The associated measure of spread for the sample mean: either
  # a sample sd, sample interquartile range, or sample range.
  # @param n The sample size.
  # @param centre_type Specify if the center is "mean" or "median".
  # @param spread_type Specify if the spread is reported as "sd", "var", "iqr", or "range".
  #
  # @export
  
  if (centre_type == "mean" & spread_type == "sd") {
    return(se = spread / sqrt(n))
  } else if (centre_type == "median") {
    if (spread_type == "iqr") {
      sn_arg <- 3 / 4
    } else if (spread_type == "range") {
      sn_arg <- (n - 1 / 2) / n
    } else if (spread_type == "var") {
      return(se = sqrt(spread /  n))
    } else {
      stop("Check that your spread_type is either \"var\",  \"iqr\", or \"range\".")
    }
    
    # Estimate mu.
    mu <- log(centre)
    
    # Estimate sigma.
    sigma <-
      1 / qnorm(sn_arg) *
      log(1 / 2 *
            (spread * exp(-mu) + sqrt(spread ^ 2 * exp(-2 * mu) + 4)))
    
    return(1 / (2 * sqrt(n) * dlnorm(
      centre, meanlog = mu, sdlog = sigma
    )))
  } else {
    stop("Check that your centre_type is of the form \"mean\" or \"median\".")
  }
}


brown_case_3 <- function(theta, n, output = "all") {

# This function is equivalent to doing the following
#
#  NB: se_phi = 1 / (n*se)
#
# where se = sqrt( theta*(1-theta) / n)

# Transform to estimate and SE to an unbounded metric
phi    <- -log( (1/theta) - 1)
se_phi <- sqrt( theta*(1-theta) / n) / (theta*(1-theta))

if(output == "mean") {return(phi)}
if(output == "se") {return(se_phi)}
else {return(list(phi, se_phi))}
}

niceMLMESum <- function(REMod) {
  data_frame(Variable = c("Intercept", "Year" , rep(NA, 4)), 
    Estimate = c(REMod$b, rep(NA, 4)), 
             "95% CI LB" = c(REMod$ci.lb, rep(NA, 4)), 
             "95% CI UB" = c(REMod$ci.ub, rep(NA, 4)), 
             SE = c(REMod$se,  rep(NA, 4)), 
             p = c( printp(REMod$pval),  rep(NA, 4)),
             "Random effects" = c(NA, NA, 
                                  paste0("Project variance = ", 
                                         printnum(REMod$sigma2[1], digits = 3), ", n = ", 
                                         REMod$s.nlevels[1]),
                                  paste0("Article variance = ", 
                                         printnum(REMod$sigma2[2], digits = 3), ", n = ", 
                                         REMod$s.nlevels[2]), 
                                  paste0("Effect variance = ", 
                                         printnum(REMod$sigma2[3], digits = 3), 
                                         ", n = ", REMod$s.nlevels[3]),
                                  paste0("QE(",REMod$k-1, ") = ", 
                                         round(REMod$QE, 2),  ", p ", 
                                         ifelse(REMod$QEp <.001, "< .001", 
                                                paste("=" , round(REMod$QEp, 2))))))
}

```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Statistical power describes the probability of a statistical test finding statistically significant results given that a specific alternative hypothesis holds true. Cohen’s first power survey (1962) showed that articles published in a 1960 issue of the Journal of Abnormal and Social Psychology had a mean power of .48 to detect a ‘medium’ effect size equivalent to 0.5 Cohen’s d. This suggests that the average psychology study should fail to reach statistical significance even when studying a true “medium” effect on more than 50% of occasions. Since the publication of Cohen’s 1962 article over 40 power surveys have been performed, studies which systematically assess the statistical power of bodies of psychology research. The current study brings those papers together to estimate the statistical power of psychology research at Cohen’s effect size benchmarks and show how the statistical power of psychology research has changed over time. 

If studies in a body of literature have a low average statistical power, several major negative outcomes follow. Firstly, low power studies often produce estimates of effects that are so imprecise as to not allow researchers to make meaningful inferences [@cohenStatisticalPowerAbnormalsocial1962; @cohenStatisticalPowerAnalysis1988], wasting research funds, participant, and researcher time. Secondly, in the presence of publication and reporting biases toward statistically significant results, low average power leads to effect size exaggeration among published studies, and an increased false positive error rates among significant reported results [@bakkerRulesGameCalled2012; @decosterOpportunisticBiasesTheir2015; @ioannidisWhyMostDiscovered2008]. In recent years the low average power of psychology research has been pointed to as one of the driving factors of the “replication crisis” in psychology [@maxwellPsychologySufferingReplication2015]. Finally, low power research can be self-reinforcing. If researchers base the sample sizes they use in their studies on previous low power research, or if they base sample size decisions on published (and, on average, exaggerated) effect sizes, their own research will often have a lower than desirable level of statistical power to detect likely effect sizes [cite pub bias paper].

Hundreds of articles have been published since the 1960s discussing the issue of low statistical power in psychology [history chapter], and numerous tools have been developed to make power analysis an easy and routine part of research planning, from Cohen’s own textbooks and publications [e.g., -@cohenPowerPrimer1992; -@cohenStatisticalPowerAnalysis1988] to statistical power analysis computer programs (e.g., @faulPowerFlexibleStatistical2007). However, it is unclear whether the relative ease of use of these tools, as well as other changes in the way that research is planned and performed, have led to any change in the average power of psychology research over time. Given this large and growing body of work and the importance of avoiding the negative impacts of low statistical power on research literatures, it seems essential to begin to assess whether these efforts have had any impact on the statistical power of psychology research.  

In order to address this question, we use a systematic review and meta-analysis of power surveys in psychology to estimate the average power of this literature and to show whether this value has changed over time. Power surveys are articles which have examine a set of published studies and calculate their power to detect Cohen’s “small”, “medium” and “large” effect size benchmarks (see Table 2 for a list of Cohen’s effect size benchmarks for different analyses) given the particular statistical analyses that are used and the sample size included in each statistical analysis. 

Given that many of the included power surveys suggest that power analysis should be performed as part of research planning – along with the American Psychological Association and CONSORT reporting guidelines [@apapublicationscommunicationsboardworkinggrouponjournalarticlereportingstandardsReportingStandardsResearch2008; @schulzCONSORT2010Statement2010; @wilkinsonStatisticalMethodsPsychology1999] – a related and crucial question is whether researchers are following these instructions and performing and reporting power analyses more often. In order to address this question, Supplementary Materials 5 reports a meta-analysis of examinations of the proportion of articles which report a power analysis in order to assess whether there has been any change in how often power analyses are reported over time.

# Methods
## Research design
The design and hypotheses, along with a detailed analysis plan for the secondary analysis, were preregistered after an initial pilot sample of 17 articles had been collected, but before any analysis or summary statistics had been calculated. The pre-registration and pilot data are available from https://osf.io/n6jfd/, see table 1 for a list of deviations from the pre-registered protocol. 

\noindent Table 1.\newline
\noindent *Deviations from preregistered protocol.*
```{r table_deviations, results='asis', warning=F}
deviations <- data.frame(
Deviation = c("Missing mean were estimated or imputed when missing",
"Meta-analysis estimated means not medians",
"A variance stabilizing transformation was used",
"Restricted maximum likelihood estimation was used",
"Random effects were included for area of research and original study in both primary and secondary analyses",
"No analysis was performed examining sample size as an outcome",
'“Sport and exercise psychology” and “communication research” were included as fields of research'),
Explanation = c('Means and variances were imputed as large numbers of studies had some missing data. Analyses were also run without data imputation as was preregistered (see supplementary material 4).',
'Mean levels of power were reported more often than medians, in 45 compared to 47 articles, and as the standard error of means is smaller than that of medians all else being equal.',
'Restricted maximum likelihood estimation was used, no estimation method was preregistered.',
"A variance stabilizing transformation from Brown (1982) to account for the fact that power is bounded between .05 and 1",
'No method of accounting for non-independence between articles was preregistered. The mixed effects meta-analyses reported here include random effects for study, area of research as well as each study’s estimate. The preregistered models were also performed and are reported as sensitivity analyses, see supplementary materials 4 for model output for the primary analysis and supplementary materials 5 for model output for the secondary analysis.',
'No analysis was performed with sample size as an outcome as few articles (7) reported the average sample sizes of the investigated areas of research.',
'“Sport and exercise psychology” and “communication research” are distinct areas of research not listed as subfields in the preregistration') )

kable(deviations, booktabs = T, longtable = T) %>% 
  kable_styling() %>%
  column_spec(c(1,2), width = c("6cm", "10cm"))

```

## Record identification
See figure [PRISMA] for a PRISMA flow diagram of article identification, screening, eligibility analysis and inclusion. The sampling strategy was designed to return all reviews of the statistical power of bodies of research in psychological research (broadly defined, including educational, occupational, management, clinical, psychiatry, and neuroscience research). Power surveys were included if they systematically calculated the statistical power of statistical tests in a body of published research articles using effect sizes equivalent to Cohen’s (1988) benchmarks estimates for “small”, “medium” and “large” effects (see table 2). Articles which analysed the power of fewer than six articles were excluded to exclude articles which were not literature surveys but rather criticisms of a small body of “underpowered” research. Only articles with full texts available in English were included. 

\noindent Table 2. *Effect size benchmarks following Cohen (1977, 1988, 1992)*
```{r es_benchmarks, fig.pos='H', out.width="\\textwidth"}
  
ESBenchmarks <- tibble(`Type of test` = c("t test on means (d)",
"t test on correlations (r)",
"F test ANOVA (f)",
"F test for multiple correlation or regression (f2)",
"Chi-square test (w)"),  Small = c(.2, .1, .1, .02, .1), Medium = c(.5, .3, .25, .15, .3), Large = c(.5, .3, .25, .15, .3))

kable(ESBenchmarks, booktabs = T) %>% 
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Effect size benchmark" = 3)) 

```
\noindent Note. Cohen (1962) used slightly different estimates for small and large benchmarks (e.g., for t tests for mean differences d = .25 and 1 respectively) although the medium benchmarks has remained the same.

On the 11th of September 2017 the PsycInfo and Medline databases for all records including the words “power\*” “sampl\*” in their title and "power analysis", "statistical Power" or "sample size" in the main text, identifying an initial 1988 articles. After de-duplication, 1526 articles remained in the database. This database is available from https://osf.io/t6jf8/. Hand searches of all identified applicable articles’ reference lists were performed to attempt to identify any papers detailing power surveys that may have been missed by these search criteria, identifying an additional 18 articles. One additional article [@szucsEmpiricalAssessmentPublished2017] was identified through a Google Scholar search of “power survey psychology”. See supplementary materials 1 for a list of articles included, and supplementary materials 2 for search criteria. All of the articles identified in this literature search which reported the proportion of examined articles which reported a power analysis were included in the secondary analysis, along with two additional articles identified through reference list searches of the articles included in the secondary analysis. 

## Abstract and full text screening

1432 articles were excluded during abstract screening as they did not report examinations of the power of a body of psychology research (e.g., they discussed social power dynamics not statistical power, provided power analysis advice but did not examine a body of literature, etc). After screening of abstracts, 92 records remained and were subjected to full text screening. During full text screening, 46 articles were excluded leaving a total of 46 articles which gave mean or median power estimates for at least one of the small, medium or large effect size benchmarks. See Figure 1 for exclusion reasons at the full-text eligibility assessment phase.
\raggedbottom

```{r, out.width = "\\textwidth", fig.pos = "H", fig.cap="Prisma flow diagram of article identification, screening and selection. Note that this diagram does note show the selection process for the secondary analysis. The secondary analysis included 17 studies, 15 of which were identified during data eligibility screening for the primary analysis, and two which were identified through reference list searches conducted during data extraction for the secondary analysis."}
knitr::include_graphics("prisma_diagram.pdf")
```

## Data extraction

The articles included in the primary analysis were examined in randomized order to avoid systematic order effects. When additional power surveys were identified during data extraction by reference list searches, they were put aside until the current round of data extraction was complete, at which time all newly identified articles were assessed in random order. See https://osf.io/7ncke/ for data, and Supplementary Materials 3 for the codebook as well as a full list of datapoints extracted from articles for both the primary and secondary analyses. See Supplementary Materials 1 for a list of all included studies along with the sample size included in each study.

### Missing data handling and imputation

```{r data cleaning, message=FALSE, warning=FALSE, echo = FALSE}

######### data importing and wrangling ###############
# importing data
data <- read_excel("PowerEstimationReviewDataCollection2018.03.25.xlsx")

# Counting
# number of articles included (i.e., not discounting for missing data) 
N <- length(unique(subset(data$id, is.na(data$exclude) == TRUE))) 
# number of datapoints (i.e., non exlcuded papers)
n <- sum(is.na(data$exclude))

# removing all of the exlusions
dat <- subset(data, is.na(data$exclude))
# Adding effect ID
dat$eid <-  1:nrow(dat)
  
# converting to numerics
dat$NumberOfArticles<-as.numeric(dat$NumberOfArticles)

# figuring mean year of coverage for use in model later
singleYears <- as.numeric(dat$YearsStudied)
# splitting multiple years at "-"
years <- str_split(dat$YearsStudied, "-", simplify = T)
# converting to numerics
years <- apply(years,  2, as.numeric)
# calculating mean year of coverage 
dat$YearsStudiedMean <- rowMeans(years, na.rm = T)

# Ordering by year of coverage, 
dat<-dat[order(dat$YearsStudiedMean),]

# Adding whitespace at the end of columns to avoid issues that having identical study names causes later
for(i in unique(dat$StudyName)) {
  dat$StudyName[dat$StudyName == i] <- paste0(dat$StudyName[dat$StudyName == i], c("", " ", "  ", "    ", "     ", "       ")[1:length(dat$StudyName[dat$StudyName == i])])
}

## Recoding areas of reserach as per the preregistration 
# “clinical psychology/psychiatry”, “social/personality”, “education”, “general psychology” (i.e., those studies which look across fields of psychology research), “management/IO psychology”, “cognitive psychology” “neuropsychology”, “meta-analysis”
dat$SubfieldClassification[dat$SubfieldClassification == "Cognitive neuroscience, psychology, psychiatry"] <- "General Psychology"
dat$SubfieldClassification[dat$SubfieldClassification == "Clinical"] <- "Clinical Psychology/Psychiatry"
dat$SubfieldClassification[dat$SubfieldClassification == "Neuroscience"] <- "Neuropsychology"
dat$SubfieldClassification[dat$SubfieldClassification == "Medical Education"] <- "Education"

# removing cohen 1962's at large and small effect sizes (benchmark values were different)
dat$PowerAtSmallEffectMedian[dat$id==103] <- NA
dat$PowerAtLargeEffectMedian[dat$id==103] <- NA
dat$PowerAtSmallEffectMean[dat$id==103] <- NA
dat$PowerAtLargeEffectMean[dat$id==103] <- NA


#### Estimating missing parameters ####
###### Estimating Small var and sd
## setting up col for storring means + estimated variance 
dat$estSmallMean <- NA
dat$varSmall <- NA

# calculating wan c1 estimated mean and SEs following wan et al
dat[c('wanc2EstMean', 'wanc2EstSE')] <-
  eff_est_wan_c2(
    a = dat$PowerSmallMin,
    b = dat$PowerSmallMax,
    q.1 =  dat$FirstQuartilePowerAtSmall,
    m = dat$PowerAtSmallEffectMedian,
    q.3 =  dat$ThirdQuartilePowerAtSmall,
    n = dat$NumberOfArticles
  )

# calculating wan c3 estimated mean and SEs following wan et al
dat[c('wanc3EstMean', 'wanc3EstSE')] <-
  eff_est_wan_c3(
    q.1 =  dat$FirstQuartilePowerAtSmall,
    m = dat$PowerAtSmallEffectMedian,
    q.3 =  dat$ThirdQuartilePowerAtSmall,
    n = dat$NumberOfArticles
  )

# Counting the number of articles for which the wan estimators are used 
nSmallC1 <- sum(!is.na(dat$wanc2EstMean) & is.na(dat$PowerAtSmallEffectMean))
nSmallC3 <- sum(!is.na(dat$wanc3EstMean) & is.na(dat$PowerAtSmallEffectMean))

# Building mean column, in order of preferences for the method that used the most inforamtion 
# I.e., authors reported, Wan et al method c2, Wan et al, method c3
dat$estSmallMean <- dat$wanc2EstMean 
dat$estSmallMean[is.na(dat$estSmallMean)] <- dat$wanc3EstMean[is.na(dat$estSmallMean)]

# calculating the mean absolute error for Wan's methods the ? articles for which this is possible (i.e., where these values can be calculated and the authors reported the mean)
diffC3Small <- dat$estSmallMean - dat$PowerAtSmallEffectMean
# finishing off by using all of the reported means where possible
dat$estSmallMean[!is.na(dat$PowerAtSmallEffectMean)] <- dat$PowerAtSmallEffectMean[!is.na(dat$PowerAtSmallEffectMean)]


###### Estimating medium var and means
## setting up col for storring means + estimated variance 
dat$estMedMean <- NA
dat$varMed <- NA

# calculating wan c1 estimated mean and SEs following wan et al 
dat[c('wanc2EstMean', 'wanc2EstSE')] <-
  eff_est_wan_c2(
    a = dat$PowerMedMin,
    b = dat$PowerMedMax,
    q.1 =  dat$FirstQuartilePowerAtMedium,
    m = dat$PowerAtMediumEffectMedian,
    q.3 =  dat$ThirdQuartilePowerAtMedium,
    n = dat$NumberOfArticles
  )

# calculating wan c3 estimated mean and SEs following wan et al
dat[c('wanc3EstMean', 'wanc3EstSE')] <- 
  eff_est_wan_c3(
    q.1 =  dat$FirstQuartilePowerAtMedium,
    m = dat$PowerAtMediumEffectMedian,
    q.3 =  dat$ThirdQuartilePowerAtMedium,
    n = dat$NumberOfArticles
  )

nMediumC1 <- sum(!is.na(dat$wanc2EstMean) & is.na(dat$PowerAtMediumEffectMean))
nMediumC3 <- sum(!is.na(dat$wanc3EstMean) & is.na(dat$PowerAtMediumEffectMean))

# Building mean column, in order of preferences for the method that used the most inforamtion 
# I.e., authors reported, Wan et al method c2, Wan et al, method c3
dat$estMedMean <- dat$wanc2EstMean 
dat$estMedMean[is.na(dat$estMedMean)] <- dat$wanc3EstMean[is.na(dat$estMedMean)]
# calculating the mean absolute error for Wan's methods the ? articles for which this is possible (i.e., where these values can be calculated and the authors reported the mean)
diffC3Med <- dat$estMedMean - dat$PowerAtMediumEffectMean
# finishing off by using all of the reported means where possible
dat$estMedMean[!is.na(dat$PowerAtMediumEffectMean)] <- dat$PowerAtMediumEffectMean[!is.na(dat$PowerAtMediumEffectMean)]

###### Estimating Large var and means
## setting up col for storring means + estimated variance 
dat$estLargeMean <- NA
dat$varLarge <- NA

# calculating wan c1 estimated mean and SEs following wan et al
dat[c('wanc2EstMean', 'wanc2EstSE')] <-
  eff_est_wan_c2(
    a = dat$PowerLargeMin,
    b = dat$PowerLargeMax,
    q.1 =  dat$FirstQuartilePowerAtLarge,
    m = dat$PowerAtLargeEffectMedian,
    q.3 =  dat$ThirdQuartilePowerAtLarge,
    n = dat$NumberOfArticles
  )

# calculating wan c3 estimated mean and SEs following wan et al
dat[c('wanc3EstMean', 'wanc3EstSE')] <-
  eff_est_wan_c3(
    q.1 =  dat$FirstQuartilePowerAtLarge,
    m = dat$PowerAtLargeEffectMedian,
    q.3 =  dat$ThirdQuartilePowerAtLarge,
    n = dat$NumberOfArticles
  )

# Counting how many articles use the WAN estimators
nLargeC1 <- sum(!is.na(dat$wanc2EstMean) & is.na(dat$PowerAtLargeEffectMean))
nLargeC3 <- sum(!is.na(dat$wanc3EstMean) & is.na(dat$PowerAtLargeEffectMean))

# Building mean column, in order of preferences for the method that used the most inforamtion 
# I.e., authors reported, Wan et al method c2, Wan et al, method c3
dat$estLargeMean <- dat$wanc2EstMean 
dat$estLargeMean[is.na(dat$estLargeMean)] <- dat$wanc3EstMean[is.na(dat$estLargeMean)]
# calculating the error for Wan's methods the ? articles for which this is possible (i.e., where these values can be calculated and the authors reported the mean)
diffC3Large <- dat$estLargeMean - dat$PowerAtLargeEffectMean
# finishing off by using all of the reported means where possible
dat$estLargeMean[!is.na(dat$PowerAtLargeEffectMean)] <- dat$PowerAtLargeEffectMean[!is.na(dat$PowerAtLargeEffectMean)]

## putting Means estimated from frequency plots into the appropraite places 
dat[c("estSmallMean", "estMedMean", "estLargeMean")][dat$id == 62,] <- c(0.2456618, 0.5399306, 0.67625)

```

There were a total of `r nrow(dat)` year ranges (henceforth “cases”) for which mean or median power estimates were given for at least one of the small, medium or large effect size benchmarks in the `r length(unique(dat$id))` included articles. In `r sum(!complete.cases( data.frame(dat$PowerAtLargeEffectMean, dat$PowerAtMediumEffectMean, dat$PowerAtSmallEffectMean)))` of these cases, no means were reported for at least one of Cohen’s 1988 benchmark effect sizes (including Cohen, 1962, which used different “small” and “medium” benchmark effect sizes). although medians and interquartile ranges were provided. 

```{r}
source("Analysis/Estimating means and SDs from freq tables.R")
```

For two power surveys (Haase, 1974; Woolley, 1983) mean power levels were not reported, but frequency tables showing the number of articles achieving different levels of power at each effect size benchmark were presented. For those articles we estimated the mean power as the weighted average of the mid-interval values.
$$\bar{x} = \frac{\Sigma(f\hat{x})}{\Sigma (f)}$$
$f$ being the frequency within a particular bin and $\hat{x}$ being the mid-interval value (e.g., for the bin .1 - .19, the mid interval value would be .145). In order to validate this mean estimation method, the difference between the estimated means and the reported means was calculated for `r sum(!is.na(meanDiffs))` frequency tables from `r nrow(meanDiffs)` which reported mean power and frequency tables, giving a mean absolute error of just `r meanAbsDiff` (mean error = `r mean(unlist(meanDiffs), na.rm = T)` standard deviation of `r sd(unlist(meanDiffs), na.rm = T)`. An r script with the data extracted from the frequency tables and the working for these estimates can be found at https://osf.io/tdj6b/.

For five cases `r # i.e., (nMediumC3==5 & nMediumC3 == 5 & nLargeC3)`at the small, medium, and large effect size benchmarks missing means were estimated using reported the medians and interquartile power estimates following @wanEstimatingSampleMean2014’s method (equation C3), using functions from the R package varameta [@greyVerametaPackageVersion2019]. In order to validate this approach, the means for all articles which reported medians, quartiles as well as means were calculated (`r length(unique ( dat$id[(!is.na(diffC3Large) | !is.na(diffC3Med) | !is.na(diffC3Large))]))` articles reporting `r sum(!is.na(c(diffC3Large, diffC3Med, diffC3Large)))` estimated means), which lead to a mean absolute error of `r mean(abs(c(diffC3Large, diffC3Med, diffC3Large)), na.rm=T)` (mean error = `r mean(c(diffC3Large, diffC3Med, diffC3Large), na.rm=T)`, sd = `r sd(c(diffC3Large, diffC3Med, diffC3Large), na.rm=T)`).

## Analysis

All data-analysis was conducted using R 3.5.1 [@rdevelopmentcoreteamLanguageEnvironmentStatistical2017], and meta-analyses were performed using the metafor package [version 2.0.0; @R-metafor]. The R Markdown document including all code and the data required to reproduce this paper document are available from https://osf.io/as7md/.

At each benchmark level of power (small, medium, and large) a mixed effects meta-regression was performed. 
$$power_j=\gamma_0+\gamma_1Year_j+u_{area}+u_{survey}+u_{id}+e_j$$
This analysis predicts estimated power at each benchmark ($power_j$) with an overall intercept ($\gamma_0$), and a fixed effect for year $\gamma_1$. Random effects were included for the estimate ($u_{id}$), survey ($u_{survey}$), and area of psychology research ($u_{area}$). Random effects were included to account for non-independence between studies sampled from the same fields of research (e.g., clinical psychology or IO psychology), for non-independence in cases where surveys reported multiple estimates (e.g., when a power survey reported multiple power estimates for different year ranges), and at the effect level to make this a random effects meta-analysis. The variable year was mean centered, making the overall intercept interpretable as the estimated mean power at the mean examined year included in this study (1985). When a study covered a range of years, the mean year of the range of studies included in each set was entered as a predictor in the meta-regression. All analyses used restricted maximum likelihood estimation. 

Because statistical power is bounded between .05 and 1, we used a variance stabilizing transformation analogous to the fisher r-to-z transformation to convert the mean level of power at each benchmark effect size into an unbounded quantity ($\phi$) following @brownCovarianceStructures1982.
$$\phi = -\log{\bigg( \frac{1}{\hat{\theta}} - 1\bigg)}$$
In the above, $\hat\theta$ is the estimated mean statistical power at a given effect size benchmark. The standard error of each estimate was calculating using the following. 
$$ \sigma^2 = \sqrt{ \frac{ \hat{\theta}(1-\hat{\theta}) / n}{ \hat{\theta}(1-\hat{\theta})}} $$

Where $\hat\theta$ is again the estimated mean statistical power from each survey and $n$ is the number of studies examined in each power survey. All estimates are back-transformed to raw estimated statistical power unless otherwise stated. 

# Results

```{r}
## Setting up the back transformation function
backtransform <- function(theta) {1/(1 + exp(-theta))}

convertModelOutput <- function(brownModel) {
be <- backtransform( brownModel$b[1])
ll <- backtransform(brownModel$ci.lb[1])
ul <- backtransform(brownModel$ci.ub[1])

# This calculates the best estimate difference CIs differences at one year from the mean included year
differenceBE <- be - (1/(1 + exp(- brownModel$b[1] + 10*brownModel$b[2])))
# Estimated differnece between the best estimate and one year, lower bound 
differenceLL <- be  - (1/(1 + exp(-  brownModel$b[1] + 10*brownModel$ci.lb[2])))
# estimated difference beween the best estimate and one year later, upper bound
differenceUL <- be - (1/(1 + exp(- brownModel$b[1] + 10*brownModel$ci.ub[2])))

return(tibble(be = be, ll =  ll, ul = ul, differenceBE = differenceBE, differenceLL = differenceLL, differenceUL = differenceUL))
}

### Using Brown's formula
### Small
estimatesSmall <- brown_case_3(theta = dat$PowerAtSmallEffectMean,
                               n = dat$NumberOfArticles)
dat$brownMeanSmall <- estimatesSmall[[1]]
dat$brownMeanSE <- estimatesSmall[[2]]

# the optimiser is set here because sometimes the default model estimator can fail 
brownSmall <-
  rma.mv(
    yi = brownMeanSmall,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ as.numeric(YearsStudiedMean - mean(YearsStudiedMean)),
    slab = StudyName,
    data = dat, control=list(optimizer="optim", optmethod="Nelder-Mead"))

brownoutSmall <- convertModelOutput(brownSmall)

### Medium
estimatesMed <- brown_case_3(theta = dat$PowerAtMediumEffectMean,
                             n = dat$NumberOfArticles)

dat$brownMeanMed <- estimatesMed[[1]]
dat$brownMeanSE <- estimatesMed[[2]]

brownMed <-
  rma.mv(
    yi = brownMeanMed,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ as.numeric(YearsStudiedMean - mean(YearsStudiedMean)),
    slab = StudyName,
    data = dat
  )
brownoutMed <- convertModelOutput(brownMed)

### Large
estimatesLarge <- brown_case_3(theta = dat$PowerAtLargeEffectMean,
                               n = dat$NumberOfArticles)
dat$brownMeanLarge <- estimatesLarge[[1]]
dat$brownMeanSE <- estimatesLarge[[2]]

brownLarge <-
  rma.mv(
    yi = brownMeanLarge,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ as.numeric(YearsStudiedMean - mean(YearsStudiedMean)),
    slab = StudyName,
    data = dat
  )

brownoutLarge <- convertModelOutput(brownLarge)
```


The mixed effects meta-regression intercept parameter suggests the mean power of psychology across this time period is `r brownoutSmall$be` (95% CI [`r brownoutSmall$ll`, `r brownoutSmall$ul`]) for ‘small’ effects, `r brownoutMed$be` (95% CI [`r brownoutMed$ll`, `r brownoutMed$ul`])  for ‘medium’ effects, and `r brownoutLarge$be` (95% CI [`r brownoutLarge$ll`, `r brownoutLarge$ul`]) for 'large' effects following Cohen’s effect size benchmarks.

The estimated effect of time is negligible at all three benchmarks, in transformed units, `r   printnum(brownSmall$b[2], digits = 3)` (95% CI [`r printnum(brownSmall$ci.lb[2], digits = 3)`, `r printnum(brownSmall$ci.ub[2], digits = 3)`]) for ‘small’ effects,  `r printnum(brownSmall$b[2], digits = 3)` (95% CI [`r printnum( brownSmall$ci.lb[2], digits = 3)`, `r printnum(brownSmall$ci.ub[2], digits = 3)`])  for ‘medium’ effects, and  `r printnum(brownSmall$b[2], digits = 3)` (95% CI [`r  printnum(brownSmall$ci.lb[2], digits = 3)`, `r printnum(brownSmall$ci.ub[2], digits = 3)`])  for 'large' effects following Cohen’s effect size benchmarks. Although this change is non-linear when back transformed to Statistical Power, it represents an extremely small estimated change across the range of years covered in this study. 

For example, we can look at the predicted change in statistical power per decade from the overall intercept (a value which represents an estimate of the mean level of statistical power in psychology at the mean year included in the power surveys, 1985). This gives us an estimated change per decade of just `r printnum(brownoutSmall$differenceBE, digits = 3)` (95% CI [`r printnum(brownoutSmall$differenceLL, digits = 3)`, `r printnum(brownoutSmall$differenceUL, digits = 3)`]) at the ‘small’ benchmark, `r printnum(brownoutMed$differenceBE, digits = 3)` (95% CI [`r printnum(brownoutMed$differenceLL, digits = 3)`, `r printnum(brownoutMed$differenceUL, digits = 3)`]) at the ‘medium’ benchmark, and `r printnum(brownoutLarge$differenceBE, digits = 3)` (95% CI [`r printnum(brownoutLarge$differenceLL, digits = 3)`, `r printnum(brownoutLarge$differenceUL, digits = 3)`]) for 'large' effects following Cohen’s effect size benchmarks.

\newpage

```{r, dpi=300, fig.height= 15, fig.width= 13, out.width = "\\textwidth", fig.pos = "H", fig.cap="Forest plot of studies of the power of the power of psychology research literatures at Cohen’s (1988) small effect sizes. The polygon depicts reports the model intercept."}
res <- brownSmall
par(mar=c(4,4,1,2), font = 1)
forest(res, xlim=c(-1.5, 1.35), at = c(0,.25, .5, .75, 1), 
       transf = backtransform,
       ilab = data.frame(dat$YearsStudied, dat$SubfieldClassification),
       ilab.xpos=c(-.16, -.58), cex=1.1, ylim=c(-1, res$k+3),
       xlab="Estimated power", mlab="", addfit = F, showweights = F)


addpoly(x =  backtransform(res$b[1]), 
        ci.lb = backtransform(res$ci.lb[1]),
        ci.ub = backtransform(res$ci.ub[1]), cex = 1.1)

# Bold font 
par(font=2)
### add column headings to the plot
text(-1.3, res$k+2, c("Author(s) (year)"), cex = 1.25)
text(-.58, res$k+2, c("Subfield"), cex = 1.25)
text(-.18, res$k+2, c("Years sampled"), cex = 1.25)
# normal font 
par(font=1)
```


\newpage

```{r, dpi=300, fig.height= 15, fig.width= 13, out.width = "\\textwidth", fig.pos = "H", fig.cap="Forest plot of studies of the power of the power of psychology research literatures at Cohen’s (1988) medium effect sizes. The polygon depicts reports the model intercept."}
res <- brownMed
par(mar=c(4,4,1,2), font = 1)
forest(res, xlim=c(-1.5, 1.35), at = c(0,.25, .5, .75, 1), 
       transf = backtransform,
       ilab = data.frame(dat$YearsStudied, dat$SubfieldClassification),
       ilab.xpos=c(-.16, -.58), cex=1.1, ylim=c(-1, res$k+3),
       xlab="Estimated power", mlab="", addfit = F, showweights = F)


addpoly(x =  backtransform(res$b[1]), 
        ci.lb = backtransform(res$ci.lb[1]),
        ci.ub = backtransform(res$ci.ub[1]), cex = 1.1)

# Bold font 
par(font=2)
### add column headings to the plot
text(-1.3, res$k+2, c("Author(s) (year)"), cex = 1.25)
text(-.58, res$k+2, c("Subfield"), cex = 1.25)
text(-.18, res$k+2, c("Years sampled"), cex = 1.25)
# normal font 
par(font=1)
```

\newpage

```{r, dpi=300, fig.height= 15, fig.width= 13, out.width = "\\textwidth", fig.pos = "H", fig.cap="Forest plot of studies of the power of the power of psychology research literatures at Cohen’s (1988) large effect sizes. The polygon depicts reports the model intercept."}
res <- brownSmall
par(mar=c(4,4,1,2), font = 1)
forest(res, xlim=c(-1.5, 1.35), at = c(0,.25, .5, .75, 1), 
       transf = backtransform,
       ilab = data.frame(dat$YearsStudied, dat$SubfieldClassification),
       ilab.xpos=c(-.16, -.58), cex=1.1, ylim=c(-1, res$k+3),
       xlab="Estimated power", mlab="", addfit = F, showweights = F)


addpoly(x =  backtransform(res$b[1]), 
        ci.lb = backtransform(res$ci.lb[1]),
        ci.ub = backtransform(res$ci.ub[1]), cex = 1.1)

# Bold font 
par(font=2)
### add column headings to the plot
text(-1.3, res$k+2, c("Author(s) (year)"), cex = 1.25)
text(-.58, res$k+2, c("Subfield"), cex = 1.25)
text(-.18, res$k+2, c("Years sampled"), cex = 1.25)
# normal font 
par(font=1)
```


\newpage

```{r, dpi=300, fig.height= 4.5, fig.width= 6.5, out.width = "\\textwidth", fig.pos = "H", fig.cap="Scatter plot of mean statistical power to detect a small effect over time. Dotted lines are 95\\% confidence intervals, and the solid line is the estimated proportion power of psychology by year, point sizes reflect the relative weighting of articles."}

# small  
samplingVar <- dat$brownMeanSE
values <- dat$estSmallMean
## Model with unstandardised years for plotting, the optimiser is set here because sometimes the default model estimator can fail 
res <- 
  rma.mv(
    yi = brownMeanSmall,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ YearsStudiedMean,
    slab = StudyName,
    data = dat, control=list(optimizer="optim", optmethod="Nelder-Mead")
  )
 
### calculate predicted 
preds <- predict(res, transf = backtransform, newmods =(min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))))

### calculate point sizes by rescaling the standard errors
wi    <- 1/sqrt(samplingVar)
size  <- 0.5 + 3.0 * (wi - min(wi, na.rm = T))/(max(wi, na.rm = T) - min(wi, na.rm = T))

### plot the risk ratios against absolute latitude
plot(dat$YearsStudiedMean[!is.na(samplingVar)], values[!is.na(samplingVar)], pch=19, cex=size, 
     xlab="Year", ylab="Power at Small Benchmark", ylim = c(0,1),
     las=1, bty="l")

### add predicted values (and corresponding CI bounds)
lines((min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))), preds$pred)
lines((min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))), preds$ci.lb, lty="dashed")
lines((min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))), preds$ci.ub, lty="dashed")

```



```{r, dpi=300, fig.height= 4.5, fig.width= 6.5, warning=FALSE, out.width = "\\textwidth", fig.pos = "H", fig.cap="Scatter plot of mean statistical power to detect a medium effect over time. Dotted lines are 95\\% confidence intervals, and the solid line is the estimated proportion power of psychology by year, point sizes reflect the relative weighting of articles."}

# PLOT medium  
samplingVar <- dat$brownMeanSE
values <- dat$estMedMean
## Model with unstandardised years for plotting
res <- 
  rma.mv(
    yi = brownMeanMed,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ YearsStudiedMean,
    slab = StudyName,
    data = dat, control=list(optimizer="optim", optmethod="Nelder-Mead")
  )
### calculate predicted 
preds <- predict(res, transf = backtransform, newmods =(min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))))


### calculate point sizes by rescaling the standard errors
wi    <- 1/sqrt(samplingVar)
size  <- 0.5 + 3.0 * (wi - min(wi, na.rm = T))/(max(wi, na.rm = T) - min(wi, na.rm = T))

### plot the risk ratios against absolute latitude
plot(dat$YearsStudiedMean[!is.na(samplingVar)], values[!is.na(samplingVar)], pch=19, cex=size, 
     xlab="Year", ylab="Power at Medium Benchmark", ylim = c(0,1),
     las=1, bty="l")

### add predicted values (and corresponding CI bounds)
lines((min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))), preds$pred)
lines((min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))), preds$ci.lb, lty="dashed")
lines((min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))), preds$ci.ub, lty="dashed")

```

 
\newpage

```{r, dpi=300, fig.height= 4.5, fig.width= 6.5, warning=FALSE, out.width = "\\textwidth", fig.pos = "H", fig.cap="Scatter plot of mean statistical power to detect a large effect over time. Dotted lines are 95\\% confidence intervals, and the solid line is the estimated proportion power of psychology by year, point sizes reflect the relative weighting of articles."}

# PLOT large
samplingVar <- dat$brownMeanSE
values <- dat$estLargeMean
## Model with unstandardised years for plotting
res <- 
  rma.mv(
    yi = brownMeanLarge,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ YearsStudiedMean,
    slab = StudyName,
    data = dat
  )
### calculate predicted 
preds <- predict(res, transf = backtransform, newmods =(min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))))


### calculate point sizes by rescaling the standard errors
wi    <- 1/sqrt(samplingVar)
size  <- 0.5 + 3.0 * (wi - min(wi, na.rm = T))/(max(wi, na.rm = T) - min(wi, na.rm = T))

### plot the risk ratios against absolute latitude
plot(dat$YearsStudiedMean[!is.na(samplingVar)], values[!is.na(samplingVar)], pch=19, cex=size, 
     xlab="Year", ylab="Power at Large Benchmark", ylim = c(0,1),
     las=1, bty="l")

### add predicted values (and corresponding CI bounds)
lines((min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))), preds$pred)
lines((min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))), preds$ci.lb, lty="dashed")
lines((min(dat$YearsStudiedMean):ceiling(max(dat$YearsStudiedMean))), preds$ci.ub, lty="dashed")

```


\newpage 

\begin{noindent}
Table 2. 
\textit{Meta-regression of the power of psychology studies at a small effect size, including the year studied in each power survey as a moderator.}
\end{noindent}
```{r}
kable(niceMLMESum(brownSmall), booktabs = T, digits = 3) %>% 
  kable_styling() 
```

\begin{noindent}
Table 3.
\textit{Meta-regression of the power of psychology studies at a medium effect size, including the year studied in each power survey as a moderator.}
\end{noindent}
```{r}
kable(niceMLMESum(brownMed), booktabs = T, digits = 3) %>% 
  kable_styling() 
```

\newpage
\begin{noindent}
Table 4. 
\textit{Meta-regression of the power of psychology studies at a large effect size, including the year studied in each power survey as a moderator.}
\end{noindent}
```{r}
kable(niceMLMESum(brownLarge), booktabs = T, digits = 3) %>% 
  kable_styling() 
```


### Bias assessment

```{r}

brownEggerSmall <-
  rma.mv(
    yi = brownMeanMed,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ as.numeric(YearsStudiedMean - mean(YearsStudiedMean)) + NumberOfArticles,
    slab = StudyName,
    data = dat
  )

brownEggerMed <-
  rma.mv(
    yi = brownMeanMed,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ as.numeric(YearsStudiedMean - mean(YearsStudiedMean)) + NumberOfArticles,
    slab = StudyName,
    data = dat
  )

brownEggerLarge <-
  rma.mv(
    yi = brownMeanLarge,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ as.numeric(YearsStudiedMean - mean(YearsStudiedMean)) + NumberOfArticles,
    slab = StudyName,
    data = dat
  )

# funnel(brownSmall)
# funnel(brownMed)
# funnel(brownLarge)

# plot(dat$brownMeanMed, dat$brownMeanSE)
# param <- "NumberOfArticles"
printCoef <- function(mod, param = NA, digits = options("digits")) {
  if(is.na(param)) {param <- names(coef(mod))[1]}
  estimates <- coef(summary(mod))[names(coef(mod)) == param, c("estimate", "ci.lb", "ci.ub", "pval")]
  estimates <- c(printnum(estimates[1:3], digits = as.numeric(digits)), printp(as.numeric(estimates[4])))
  print(paste0(estimates[1], ", 95% CI [", estimates[2], ", ", estimates[3], "], *p* = ", estimates[4]))
}

# Double checking that this method induces an association between se and the mean outcome
sim <- function(loops) {
  outs <- list()
  for(i in 1:loops) {
    n <-  abs(round(rnorm(47, 50, 100))) + 1
    theta <- rbinom(47, size = n, .5) / n
    theta[theta == 1] <- .95
    theta[theta == 0] <- .05
    # plot(n, theta)
    # plot(brown_case_3(theta , n, "mean"), brown_case_3(theta, n, "se"))
    nPhiCor <- cor.test(n, theta)
    SEPhiCor <-
      cor.test(brown_case_3(theta , n, "mean"),
               brown_case_3(theta, n, "se"))
    outs[[i]] <- tibble(nPhiR = nPhiCor$estimate, nPhiP = nPhiCor$p.value, sePhiR = SEPhiCor$estimate, sePhiP =  SEPhiCor$p.value)
  }
  return(do.call(rbind, outs))
}

# cooks.distance(brownMed)
```

Although the included articles do not use traditional significance testing to assess their primary outcome, it is still possible that smaller articles which find more “alarming” results are more likely to be published. In order to assess for signs of publication bias we performed an analogue to Egger’s Test (Egger, Smith, Schneider, & Minder, 1997) in the multilevel context by including the number of articles surveyed in each study as a moderator. This analysis uses the number of articles included in each study as opposed to the standard error of the estimate as the standard error of the estimate is a function of the observed estimate. This test suggests that there was no association between sample size and statistical power at any of the benchmarks with coefficient estimates for sample size at the small, medium, and large effects of `r printnum(brownEggerSmall$b[3], digits = 4)` (95% CI [`r printnum(brownEggerSmall$ci.lb[3], digits = 4)`, `r printnum(brownEggerSmall$ci.ub[3], digits = 4)`]), `r printnum(brownEggerMed$b[3], digits = 4)` 95% CI [`r printnum(brownEggerMed$ci.lb[3], digits = 4)`, `r printnum(brownEggerMed$ci.ub[3], digits = 4)`], and `r printnum(brownEggerLarge$b[3], digits = 4)`, 95% CI [`r printnum(brownEggerLarge$ci.lb[3], digits = 4)`, `r printnum(brownEggerLarge$ci.ub[3], digits = 4)`], giving no obvious indication of publication bias in this sample. 


```{r}
### Sensitivity analysis

### Using Brown's formula
### Small
brownSmallNE <-
  rma.mv(
    yi = brownMeanSmall,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ as.numeric(YearsStudiedMean - mean(YearsStudiedMean)),
    slab = StudyName,
    data = filter(dat, !is.na(dat$PowerAtSmallEffectMean)),
    control=list(optimizer="optim", optmethod="Nelder-Mead"))

brownoutSmallNE <- convertModelOutput(brownSmallNE)

### Medium
brownMedNE <-
  rma.mv(
    yi = brownMeanMed,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ as.numeric(YearsStudiedMean - mean(YearsStudiedMean)),
    slab = StudyName,
    data = filter(dat, !is.na(dat$PowerAtMediumEffectMean)),
    control=list(optimizer="optim", optmethod="Nelder-Mead")
  )
brownoutMedNE <- convertModelOutput(brownMedNE)
 # brownoutMed
### Large
brownLargeNE <-
  rma.mv(
    yi = brownMeanLarge,
    V = brownMeanSE ^ 2,
    random = ~ 1 |
      SubfieldClassification / id / eid,
    mods = ~ as.numeric(YearsStudiedMean - mean(YearsStudiedMean)),
    slab = StudyName,
    data = filter(dat, !is.na(dat$PowerAtLargeEffectMean))
  )

brownoutLargeNE <- convertModelOutput(brownLargeNE)

```

# Discussion
This analysis suggests that there has been little to no change in the statistical power of psychology research over the previous half century. The reporting of statistical power analysis appears to have increased slightly over time, but is still uncommon. These results are unexpected given the large number of papers that have been published arguing for power analysis to be performed as a part of research planning over the last 50 years [e.g., @cohenStatisticalPowerAbnormalsocial1962; @bezeauStatisticalPowerEffect2001; @rossiStatisticalPowerPsychological1990a], the increasing availability of user friendly power analysis tools [e.g., @cohenPowerPrimer1992; @faulPowerFlexibleStatistical2007], as well as technological innovations (e.g., Amazon Turk studies) and larger undergraduate cohorts that could make larger sample research more tractable at least in many areas of psychological research.

Given that the average effect size seen in the psychology literature has been estimated to be around or even slightly below Cohen’s ‘medium’ effect size [e.g., @boscoCorrelationalEffectSize2015a; @gignacEffectSizeGuidelines2016; @quintanaStatisticalConsiderationsReporting2017] and have remained quite stable or even decreased slightly over time [see Chapter 7], this suggests that the average psychological study should fail to find significant results as much as 40% of occasions assuming that the effect under study is in fact present. Despite this, over 90% of psychology papers report statistically significant findings [@fanelliPositiveResultsIncrease2010]. This suggests that either a large proportion of performed research is going unreported, or that a large amount of research is presented as having found statistically significant findings achieved in some part through p-hacking, HARKing or through the exploitation of researcher degrees of freedom [@bakkerRestrictionOpportunisticUse2017; @lebelUnifiedFrameworkQuantify2018; @sijtsmaImprovingConductReporting2016].

Given the evidence regarding how poor our intuitions about the likely power and precision of research [@bakkerResearchersIntuitionsPower2016; @obrechtIntuitiveTestsLay2007a; @tverskyBeliefLawSmall1971], formal sample size planning should play a major role in helping researchers plan their studies. Formal sample size planning (e.g., planning for adequate levels of power, narrow confidence or credible intervals, convincing evidence via Bayes factors, etc.) is an important tool for researchers who wish to ensure that they are not wasting their participant’s time, their own time and limited research funding on experiments which are unlikely to allow them to draw accurate inferences. A variety of research planning packages and programs are freely available and should enable researchers to plan for most statistical analyses [e.g., the R package "SIMSEM" for structural equation modeling; @beaujeanSampleSizeDetermination2014; G*Power for the most common analyses such as ANOVA, regression or chi-square analysis; @faulPowerFlexibleStatistical2007a; PINT 2.2 for two level hierarchical modeling; @snijdersStandardErrorsSample1993; for advice on planning for sufficiently convincing Bayes factors see [@schonbrodtBayesFactorDesign2017]; and "PANGEA" for more complex ANOVA designs;  @westfallPANGEAPowerANalysis2015]. More complex analyses may require consultation with a statistician [@vanmeterStrengtheningInteractionsStatisticians2014].

Editors and reviewers can play a role in supporting the routine performance and reporting of a priori power analysis by requiring a statement of justification for the included sample size following the formal reporting guidelines that have already been established [@apapublicationscommunicationsboardworkinggrouponjournalarticlereportingstandardsReportingStandardsResearch2008; @schulzCONSORT2010Statement2010; @wilkinsonStatisticalMethodsPsychology1999]. Requiring the accurate justification of sample sizes as a routine part of research reporting (e.g., stating that the sample size was chosen due to practical constraints such as in the current study, identified through formal sample size planning such as AIPE or power analysis, or even stating that no sample size planning occurred when this is the case) could help establish a norm for these issues to be considered during research planning. 

This advice – that researchers should consider the statistical power of their analyses during research planning and that editors should request or even require the reporting of power analyses – is the suggested remedy in almost all of the statistical power reviews included in the current analysis. It has apparently failed to influence the practices of working scientists. It is hard to imagine that saying it again here will result in anything different. 

The recent development and rapid uptake of new research, publication and reporting initiatives give some reason for optimism. Preregistration of confirmatory analysis plans can help allow researchers to avoid unwittingly altering their analysis plans and increasing the probability of obtaining a false positive finding [@simmonsFalsePositivePsychology2011]. The use of preprint servers allows researchers to disseminate findings outside of the traditional publication system, subverting publication bias and allowing small scale studies or research to be available for future meta-analysts, helping to avoid effect size inflation. Large-scale multi-lab collaborative efforts like the Psychological Science Accelerator [@moshontzPsychologicalScienceAccelerator2018] and the Many Labs projects [@kleinManyLabsInvestigating2018] facilitate extremely large scale research, allowing for extremely high powered research even when effect sizes may be small. However, these initiatives still make up an extremely small part of the scientific literature. For research consumers this means we must accept that the published research literature likely provides exaggerated effect size estimates on average and has a higher false positive error rate than it otherwise would [@stanleyWhatMetaAnalysesReveal2018].

#### Limitations

In interpreting these findings, it’s important to keep in mind that the individual articles in these power surveys are not a random sample from the psychological research literature, and it is difficult to predict whether the sampling choices will tend to underestimate or overestimate the average power of psychological research. It is possible that power surveys are more likely to be performed when a particular area of research is underpowered, which could lead to this analysis underestimating the average statistical power of psychology. This issue only holds for a subset of the included studies, with the other included studies either using convenience samples (e.g., Szucs & Ioannidis, 2017), samples chosen to be broadly representative of a subfield (e.g., Orme & Combs-Orme, 1986), or samples selected to represent high-impact journals in a subfield (e.g., Cashen & Geiger, 2004; Rossi, 1990, a strategy which could upwardly bias estimates). Secondly, the included power surveys assume that alpha is set at .05, meaning that these results may overestimate power as they ignore alpha corrections for multiple comparisons which lead to lower power. Power surveys also almost uniformly target tests for which power can be easily estimated or defined, ignoring more sophisticated analyses (e.g., SEM, factor analysis, or multilevel models). This means that this study may underestimate the average power of psychological research if larger studies tend to use these more sophisticated techniques. However, given that simple significance testing is rarely a primary concern for these statistical techniques their exclusion from this analysis may not be unreasonable.

#### Conclusion

Statistical power to detect small to medium effects appears to be substantially lower than recommended standards and power analysis is rarely reported in psychology research. Statistical power does not appear to have increased over the last 60 years, despite continued criticism of this fact, the advocacy for the use of formal sample size planning techniques the increasing ease of use of. Research consumers should be aware that the average power of psychological science is lower than would be ideal for ‘small’ or ‘medium’ effects, and only acceptably high for ‘large’ effects. Research consumers should make sure they read and interpret the published literature with these facts in mind, take steps to avoid performing underpowered research, and ensure that the results of their analyses are available to future meta-analysts regardless of the statistical significance of their results. 

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")

```


```{r}
#\begingroup
#\setlength{\parindent}{-0.5in}
#\setlength{\leftskip}{0.5in}

#<div id = "refs"></div>
#\endgroup
```



```{r}
# render_appendix("Supplementary_materials.RMD")
```
